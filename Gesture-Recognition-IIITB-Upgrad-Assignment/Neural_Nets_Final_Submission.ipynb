{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural_Nets_Project_Starter_Code_v15.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "uDHcOjNt7CZK",
        "NaMnQLWyN2gh",
        "uFRfGlRzN2hU",
        "ZmWU4wPnhqxE"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1MEguMB57CYF"
      },
      "source": [
        "# **Submitted by:**\n",
        "#### Kiran Gupta\n",
        "#### Parama Bhattacharya"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JL3L4lxs0ve",
        "colab_type": "text"
      },
      "source": [
        "### Problem Statement:\n",
        "Our model should be able to identify gestures which correspond to specific commands in a smart TV. There are mentioned below:\n",
        "#####\t***Thumbs up:  Increase the volume***\n",
        "#####\t***Thumbs down: Decrease the volume***\n",
        "#####\t***Left swipe: 'Jump' backwards 10 seconds***\n",
        "#####\t***Right swipe: 'Jump' forward 10 seconds*** \n",
        "#####\t***Stop: Pause the movie*** \n",
        "Each video (typically 2-3 seconds long) is divided into a sequence of 30 frames(images). These videos have been recorded by various people performing one of the five gestures in front of a webcam - similar to what the smart TV will use.  \n",
        "Each video is a sequence of 30 frames (or images). All images in a particular video subfolder have the same dimensions but different videos may have different dimensions. Specifically, videos have two types of dimensions - either 360x360 or 120x160 (depending on the webcam used to record the videos). \n",
        "Hence, we need to do some pre-processing to standardise the videos. We have done resizing of the frames to a shape of 120 X 120.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjB0GbNJsD2O",
        "colab_type": "text"
      },
      "source": [
        "# Gesture Recognition\n",
        "In this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C1dX6o9T7CYI",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from scipy.misc import imread, imresize\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import datetime\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MCcto1mUm1nc",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kOdR4yRw7CYS"
      },
      "source": [
        "We set the random seed so that the results don't vary drastically."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2oDGnueu7CYZ",
        "colab": {}
      },
      "source": [
        "np.random.seed(30)\n",
        "import random as rn\n",
        "rn.seed(30)\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "tf.set_random_seed(30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zmfdLLJu7CYj"
      },
      "source": [
        "In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "D58_XLJvGWG9",
        "colab": {}
      },
      "source": [
        "DATASET_PATH = 'Project_data'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MuCwUpGz7CYm",
        "colab": {}
      },
      "source": [
        "train_doc = np.random.permutation(open(DATASET_PATH + '/' + 'train.csv').readlines())\n",
        "val_doc = np.random.permutation(open(DATASET_PATH + '/' + 'val.csv').readlines())\n",
        "batch_size = 24 #experiment with the batch size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dpab6S8dN_Jq"
      },
      "source": [
        "Parameter Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hV7mo12jEikj",
        "colab": {}
      },
      "source": [
        "# input image dimensions\n",
        "img_height = 160  # X dimension of the image\n",
        "img_width = 160   # Y dimesnion of the image\n",
        "\n",
        "img_frames = 30  # Number of image frames per video\n",
        "\n",
        "# number of channels\n",
        "# For grayscale use 1 value and for color images use 3 (R,G,B channels)\n",
        "img_channel = 3\n",
        "\n",
        "## Number of output classes\n",
        "## eg: In my case I wanted to predict 5 types of gestures (Thumbs up, Thumbs down, Left swipe, Right Swipe, Stop)\n",
        "nb_classes = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w1md1mNON2gE",
        "colab": {}
      },
      "source": [
        "# define the function to normalize the image\n",
        "def normalize_img(image):\n",
        "    norm_image = image - np.min(image)/np.max(image) - np.min(image)\n",
        "    return norm_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqnEkCRtr_tM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For transfer Learning with mobilenet\n",
        "\n",
        "img_frames = 16  # Number of image frames per video\n",
        "batch_size = 8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Tn_HDLYk7CYv"
      },
      "source": [
        "## Generator\n",
        "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L0E0Gtwc7CYx",
        "colab": {}
      },
      "source": [
        "def generator(source_path, folder_list, batch_size):\n",
        "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
        "    #create a list of image numbers you want to use for a particular video\n",
        "    img_idx = [x for x in range(0, img_frames)]\n",
        "    while True:\n",
        "        t = np.random.permutation(folder_list)\n",
        "        num_batches = len(t)//batch_size # calculate the number of batches\n",
        "        for batch in range(num_batches): # we iterate over the number of batches\n",
        "            #batch_data = np.zeros((batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
        "            batch_data = np.zeros((batch_size, img_frames, img_height, img_width, img_channel)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
        "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
        "            for folder in range(batch_size): # iterate over the batch_size\n",
        "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
        "                for idx,item in enumerate(img_idx): #  Iterate over the frames/images of a folder to read them in\n",
        "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
        "\n",
        "                    # Need to resize the data so that they all are of same size.\n",
        "                    resized_image = imresize(image, (img_height, img_width, img_channel))\n",
        "                    # Normalization of the images is required\n",
        "                    normalized_image = normalize_img(resized_image)\n",
        "                    batch_data[folder,idx,:,:,0] = normalized_image[:,:,0] #normalise and feed in the image\n",
        "                    batch_data[folder,idx,:,:,1] = normalized_image[:,:,1] #normalise and feed in the image\n",
        "                    batch_data[folder,idx,:,:,2] = normalized_image[:,:,2] #normalise and feed in the image\n",
        "                    \n",
        "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
        "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
        "\n",
        "        \n",
        "        # write the code for the remaining data points which are left after full batches\n",
        "        batch_remaining = len(t)%batch_size\n",
        "        if(batch_remaining != 0):\n",
        "            batch_data = np.zeros((len(t)%batch_size, img_frames, img_height, img_width, img_channel)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
        "            batch_labels = np.zeros((len(t)%batch_size,5)) # batch_labels is the one hot representation of the output\n",
        "            for folder in range(len(t)%batch_size): # iterate over the batch_size\n",
        "                imgs = os.listdir(source_path+'/'+ t[folder + (num_batches*batch_size)].split(';')[0]) # read all the images in the folder\n",
        "                for idx,item in enumerate(img_idx): #  Iterate over the frames/images of a folder to read them in\n",
        "                    image = imread(source_path+'/'+ t[folder + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
        "                    \n",
        "                    # Need to resize the data so that they all are of same size.\n",
        "                    resized_image = imresize(image, (img_height, img_width, img_channel))\n",
        "                    # Normalization of the images is required                 \n",
        "                    normalized_image = normalize_img(resized_image)\n",
        "                    batch_data[folder,idx,:,:,0] = normalized_image[:,:,0] #normalise and feed in the image\n",
        "                    batch_data[folder,idx,:,:,1] = normalized_image[:,:,1] #normalise and feed in the image\n",
        "                    batch_data[folder,idx,:,:,2] = normalized_image[:,:,2] #normalise and feed in the image\n",
        "                    \n",
        "                batch_labels[folder, int(t[folder + (num_batches*batch_size)].strip().split(';')[2])] = 1\n",
        "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\t\t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Hh9AYWA49NhE"
      },
      "source": [
        "## Augmented Data Generator\n",
        "In this generator, unlike the previous one where we are performing only\n",
        "1. Resizing of the Image\n",
        "2. Normalization of the resized image\n",
        "\n",
        "Additionally, the following transformations are carried out for the purpose of augmentation:\n",
        "1. Accepting a batch of images used for training.\n",
        "2. Taking the batch and applying a series of random transformations to each image in the batch including random rotation\n",
        "3. Replacing the original batch with the new, randomly transformed batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o5pfqzzJWe1Z",
        "colab": {}
      },
      "source": [
        "def augmented_generator(source_path, folder_list, batch_size):\n",
        "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
        "    #create a list of image numbers you want to use for a particular video\n",
        "    img_idx = [x for x in range(0, img_frames)]\n",
        "    while True:\n",
        "        t = np.random.permutation(folder_list)\n",
        "        num_batches = len(t)//batch_size # calculate the number of batches\n",
        "        for batch in range(num_batches): # we iterate over the number of batches\n",
        "            batch_data = np.zeros((batch_size, img_frames, img_height, img_width, img_channel)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
        "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
        "            for folder in range(batch_size): # iterate over the batch_size\n",
        "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
        "                for idx,item in enumerate(img_idx): #  Iterate over the frames/images of a folder to read them in\n",
        "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
        "                    \n",
        "                    # Need to perform affine transformation for understanding the relation between 2 images\n",
        "                    # warpAffine( src, warp_dst, warp_mat, warp_dst.size() ), here src is the input image, warp_dst is the output image, warp_mat is the transformation and warp_dst.size() is the desired size of the image\n",
        "                    M = cv2.getRotationMatrix2D((img_width//2, img_height//2), np.random.randint(-30, 30), 1.0)\n",
        "                    transformed_image = cv2.warpAffine(image, M, (image.shape[0], image.shape[1]))\n",
        "                    \n",
        "                    # Need to resize the data so that they all are of same size.\n",
        "                    resized_image = imresize(transformed_image, (img_height, img_width, img_channel))\n",
        "                    # Normalization of the images is required\n",
        "                    normalized_image = normalize_img(resized_image)                    \n",
        "                    \n",
        "                    batch_data[folder,idx,:,:,0] = normalized_image[:,:,0] #normalise and feed in the image\n",
        "                    batch_data[folder,idx,:,:,1] = normalized_image[:,:,1] #normalise and feed in the image\n",
        "                    batch_data[folder,idx,:,:,2] = normalized_image[:,:,2] #normalise and feed in the image\n",
        "     \n",
        "                   \n",
        "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
        "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
        "\n",
        "        \n",
        "        # write the code for the remaining data points which are left after full batches\n",
        "        batch_remaining = len(t)%batch_size\n",
        "        if(batch_remaining != 0):\n",
        "            batch_data = np.zeros((len(t)%batch_size, img_frames, img_height, img_width, img_channel)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
        "            batch_labels = np.zeros((len(t)%batch_size,5)) # batch_labels is the one hot representation of the output\n",
        "            \n",
        "            for folder in range(len(t)%batch_size): # iterate over the batch_size\n",
        "                imgs = os.listdir(source_path+'/'+ t[folder + (num_batches*batch_size)].split(';')[0]) # read all the images in the folder\n",
        "                for idx,item in enumerate(img_idx): #  Iterate over the frames/images of a folder to read them in\n",
        "                    image = imread(source_path+'/'+ t[folder + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
        "                    \n",
        "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
        "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
        "                    # Need to perform affine transformation for understanding the relation between 2 images\n",
        "                    # warpAffine( src, warp_dst, warp_mat, warp_dst.size() ), here src is the input image, warp_dst is the output image, warp_mat is the transformation and warp_dst.size() is the desired size of the image\n",
        "                    M = cv2.getRotationMatrix2D((img_width//2, img_height//2), np.random.randint(-30, 30), 1.0)\n",
        "                    transformed_image = cv2.warpAffine(image, M, (image.shape[0], image.shape[1]))\n",
        "\n",
        "                    # Need to resize the data so that they all are of same size.\n",
        "                    resized_image = imresize(transformed_image, (img_height, img_width, img_channel))\n",
        "                    # Normalization of the images is required\n",
        "                    normalized_image = normalize_img(resized_image)                    \n",
        "                    \n",
        "                    batch_data[folder,idx,:,:,0] = normalized_image[:,:,0] #normalise and feed in the image\n",
        "                    batch_data[folder,idx,:,:,1] = normalized_image[:,:,1] #normalise and feed in the image\n",
        "                    batch_data[folder,idx,:,:,2] = normalized_image[:,:,2] #normalise and feed in the image\n",
        "                    \n",
        "                batch_labels[folder, int(t[folder + (num_batches*batch_size)].strip().split(';')[2])] = 1\n",
        "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PhXgU-1o7CY8"
      },
      "source": [
        "Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_jZCV9Jx7CY_",
        "colab": {},
        "outputId": "953864de-cbfa-4a9f-e66b-48d41dd99b87"
      },
      "source": [
        "curr_dt_time = datetime.datetime.now()\n",
        "train_path = 'Project_data/train'\n",
        "val_path = 'Project_data/val'\n",
        "num_train_sequences = len(train_doc)\n",
        "print('# training sequences =', num_train_sequences)\n",
        "num_val_sequences = len(val_doc)\n",
        "print('# validation sequences =', num_val_sequences)\n",
        "num_epochs = 20   # choose the number of epochs\n",
        "print ('# epochs =', num_epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# training sequences = 663\n",
            "# validation sequences = 100\n",
            "# epochs = 20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uDHcOjNt7CZK"
      },
      "source": [
        "## Model\n",
        "Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nGJy9tFTN2gb",
        "colab": {}
      },
      "source": [
        "#Define all the library\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Dropout, BatchNormalization, Activation\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D, Conv3D, MaxPooling3D\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras import optimizers\n",
        "from keras.optimizers import Adam\n",
        "from keras.applications import mobilenet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NaMnQLWyN2gh"
      },
      "source": [
        "# Conv2D + RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bZHks6dqN2gj",
        "colab": {}
      },
      "source": [
        "#write your model here\n",
        "img_filter = [8,16,32,64]\n",
        "img_dense = [128,64]\n",
        "dropout = [0.5, 0.25]\n",
        "\n",
        "# Input\n",
        "input_shape=(img_frames,img_height,img_width,img_channel)\n",
        "\n",
        "# Input Layer\n",
        "model = Sequential()\n",
        "model.add(TimeDistributed(Conv2D(img_filter[0], kernel_size=(3,3), padding='same', activation='relu'), input_shape=input_shape))\n",
        "model.add(TimeDistributed(BatchNormalization()))\n",
        "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2), strides=(2, 2))))\n",
        "\n",
        "# Layer 2\n",
        "model.add(TimeDistributed(Conv2D(img_filter[1], (3, 3), padding='same', activation='relu')))\n",
        "model.add(TimeDistributed(BatchNormalization()))\n",
        "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2), strides=(2, 2))))\n",
        "\n",
        "# Layer 3\n",
        "model.add(TimeDistributed(Conv2D(img_filter[1], (2, 2), padding='same', activation='relu')))\n",
        "model.add(TimeDistributed(BatchNormalization()))\n",
        "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2), strides=(2, 2))))\n",
        "\n",
        "# Layer 4\n",
        "model.add(TimeDistributed(Conv2D(img_filter[1], (2, 2), padding='same', activation='relu')))\n",
        "model.add(TimeDistributed(BatchNormalization()))\n",
        "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2), strides=(2, 2))))\n",
        "\n",
        "#Flatten the layers\n",
        "model.add(Flatten())\n",
        "\n",
        "#Fully Connected Layer\n",
        "model.add(Dense(img_dense[0],activation='relu'))\n",
        "model.add(Dropout(dropout[0]))\n",
        "\n",
        "model.add(Dense(img_dense[1],activation='relu'))\n",
        "model.add(Dropout(dropout[1]))\n",
        "\n",
        "#softmax layer\n",
        "model.add(Dense(nb_classes,activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RmabSkg-N2gp"
      },
      "source": [
        "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tXWnAL0xN2gq",
        "colab": {},
        "outputId": "ecf9ce73-dd2d-4e50-8eeb-8dd9fe7972f2"
      },
      "source": [
        "learning_rate = 0.001\n",
        "optimiser = Adam(learning_rate)\n",
        "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "print (model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "time_distributed_1 (TimeDist (None, 30, 100, 100, 8)   224       \n",
            "_________________________________________________________________\n",
            "time_distributed_2 (TimeDist (None, 30, 100, 100, 8)   32        \n",
            "_________________________________________________________________\n",
            "time_distributed_3 (TimeDist (None, 30, 50, 50, 8)     0         \n",
            "_________________________________________________________________\n",
            "time_distributed_4 (TimeDist (None, 30, 50, 50, 16)    1168      \n",
            "_________________________________________________________________\n",
            "time_distributed_5 (TimeDist (None, 30, 50, 50, 16)    64        \n",
            "_________________________________________________________________\n",
            "time_distributed_6 (TimeDist (None, 30, 25, 25, 16)    0         \n",
            "_________________________________________________________________\n",
            "time_distributed_7 (TimeDist (None, 30, 25, 25, 16)    1040      \n",
            "_________________________________________________________________\n",
            "time_distributed_8 (TimeDist (None, 30, 25, 25, 16)    64        \n",
            "_________________________________________________________________\n",
            "time_distributed_9 (TimeDist (None, 30, 12, 12, 16)    0         \n",
            "_________________________________________________________________\n",
            "time_distributed_10 (TimeDis (None, 30, 12, 12, 16)    1040      \n",
            "_________________________________________________________________\n",
            "time_distributed_11 (TimeDis (None, 30, 12, 12, 16)    64        \n",
            "_________________________________________________________________\n",
            "time_distributed_12 (TimeDis (None, 30, 6, 6, 16)      0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 17280)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               2211968   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 5)                 325       \n",
            "=================================================================\n",
            "Total params: 2,224,245\n",
            "Trainable params: 2,224,133\n",
            "Non-trainable params: 112\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "f14lh0BfN2gz"
      },
      "source": [
        "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J11Od8EdN2g0",
        "colab": {}
      },
      "source": [
        "train_generator = generator(train_path, train_doc, batch_size)\n",
        "val_generator = generator(val_path, val_doc, batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1jpQJCmRN2g5",
        "colab": {}
      },
      "source": [
        "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
        "    \n",
        "if not os.path.exists(model_name):\n",
        "    os.mkdir(model_name)\n",
        "        \n",
        "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
        "\n",
        "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1) # write the REducelronplateau code here\n",
        "callbacks_list = [checkpoint, LR]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "o6PRq4R1N2g_"
      },
      "source": [
        "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O1CnISeFN2hB",
        "colab": {}
      },
      "source": [
        "if (num_train_sequences%batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "if (num_val_sequences%batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences/batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences//batch_size) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "40rKDDUUN2hG"
      },
      "source": [
        "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Yuf5aGe6N2hH",
        "colab": {},
        "outputId": "65eab260-089d-4fe6-f4d5-f51d26f78721"
      },
      "source": [
        "batch_size = 10\n",
        "num_epochs = 10\n",
        "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
        "                    callbacks=callbacks_list, validation_data=val_generator, \n",
        "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source path =  Project_data/val ; batch size = 10\n",
            "Source path =  Project_data/train ; batch size = 10\n",
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:15: DeprecationWarning: `imread` is deprecated!\n",
            "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imread`` instead.\n",
            "  from ipykernel import kernelapp as app\n",
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "66/67 [============================>.] - ETA: 2s - loss: 6.6969 - categorical_accuracy: 0.2788"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:38: DeprecationWarning: `imread` is deprecated!\n",
            "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imread`` instead.\n",
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:43: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "67/67 [==============================] - 192s 3s/step - loss: 6.6849 - categorical_accuracy: 0.2796 - val_loss: 3.4033 - val_categorical_accuracy: 0.3000\n",
            "\n",
            "Epoch 00001: saving model to model_init_2020-08-1411_39_44.901874/model-00001-6.69321-0.27903-3.40325-0.30000.h5\n",
            "Epoch 2/10\n",
            "67/67 [==============================] - 76s 1s/step - loss: 4.2941 - categorical_accuracy: 0.3805 - val_loss: 1.4490 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00002: saving model to model_init_2020-08-1411_39_44.901874/model-00002-4.33782-0.37406-1.44904-0.50000.h5\n",
            "Epoch 3/10\n",
            "67/67 [==============================] - 77s 1s/step - loss: 2.9259 - categorical_accuracy: 0.3956 - val_loss: 2.2104 - val_categorical_accuracy: 0.5200\n",
            "\n",
            "Epoch 00003: saving model to model_init_2020-08-1411_39_44.901874/model-00003-2.86626-0.39970-2.21038-0.52000.h5\n",
            "Epoch 4/10\n",
            "67/67 [==============================] - 77s 1s/step - loss: 2.9940 - categorical_accuracy: 0.4139 - val_loss: 1.5903 - val_categorical_accuracy: 0.4900\n",
            "\n",
            "Epoch 00004: saving model to model_init_2020-08-1411_39_44.901874/model-00004-2.99807-0.41478-1.59028-0.49000.h5\n",
            "\n",
            "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "Epoch 5/10\n",
            "67/67 [==============================] - 76s 1s/step - loss: 2.2484 - categorical_accuracy: 0.3960 - val_loss: 1.4397 - val_categorical_accuracy: 0.4500\n",
            "\n",
            "Epoch 00005: saving model to model_init_2020-08-1411_39_44.901874/model-00005-2.23083-0.39668-1.43970-0.45000.h5\n",
            "Epoch 6/10\n",
            "67/67 [==============================] - 76s 1s/step - loss: 1.9903 - categorical_accuracy: 0.4099 - val_loss: 1.3288 - val_categorical_accuracy: 0.4500\n",
            "\n",
            "Epoch 00006: saving model to model_init_2020-08-1411_39_44.901874/model-00006-1.99462-0.40724-1.32885-0.45000.h5\n",
            "Epoch 7/10\n",
            "67/67 [==============================] - 76s 1s/step - loss: 1.8640 - categorical_accuracy: 0.4244 - val_loss: 1.3255 - val_categorical_accuracy: 0.4800\n",
            "\n",
            "Epoch 00007: saving model to model_init_2020-08-1411_39_44.901874/model-00007-1.87248-0.42534-1.32547-0.48000.h5\n",
            "Epoch 8/10\n",
            "67/67 [==============================] - 76s 1s/step - loss: 1.4162 - categorical_accuracy: 0.4662 - val_loss: 1.3379 - val_categorical_accuracy: 0.5100\n",
            "\n",
            "Epoch 00008: saving model to model_init_2020-08-1411_39_44.901874/model-00008-1.41042-0.46757-1.33786-0.51000.h5\n",
            "Epoch 9/10\n",
            "67/67 [==============================] - 76s 1s/step - loss: 1.4001 - categorical_accuracy: 0.4592 - val_loss: 1.2796 - val_categorical_accuracy: 0.4500\n",
            "\n",
            "Epoch 00009: saving model to model_init_2020-08-1411_39_44.901874/model-00009-1.40803-0.45701-1.27955-0.45000.h5\n",
            "Epoch 10/10\n",
            "67/67 [==============================] - 76s 1s/step - loss: 1.4319 - categorical_accuracy: 0.4771 - val_loss: 1.3345 - val_categorical_accuracy: 0.4500\n",
            "\n",
            "Epoch 00010: saving model to model_init_2020-08-1411_39_44.901874/model-00010-1.43609-0.47511-1.33448-0.45000.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0dff073d68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qUxjB8bo_EBM"
      },
      "source": [
        "**Observation**\n",
        "\n",
        "*   Training Accuracy: 47.7%\n",
        "*   Validation Accuracy: 45%\n",
        "*   Learning Rate: 0.0005000000237487257\n",
        "*   Filter Size: 3 X 3\n",
        "*   Batch Size: 10\n",
        "*   Epoch: 10\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "It5bCIboN2hN",
        "colab": {},
        "outputId": "d40b7980-3940-4a68-dcf7-d2cfd7269809"
      },
      "source": [
        "batch_size = 10\n",
        "num_epochs = 30\n",
        "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
        "                    callbacks=callbacks_list, validation_data=val_generator, \n",
        "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:15: DeprecationWarning: `imread` is deprecated!\n",
            "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imread`` instead.\n",
            "  from ipykernel import kernelapp as app\n",
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "65/67 [============================>.] - ETA: 2s - loss: 1.3634 - categorical_accuracy: 0.4615"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:38: DeprecationWarning: `imread` is deprecated!\n",
            "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imread`` instead.\n",
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:43: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "67/67 [==============================] - 76s 1s/step - loss: 1.3631 - categorical_accuracy: 0.4572 - val_loss: 1.1615 - val_categorical_accuracy: 0.5200\n",
            "\n",
            "Epoch 00001: saving model to model_init_2020-08-1411_39_44.901874/model-00001-1.36109-0.45852-1.16153-0.52000.h5\n",
            "Epoch 2/30\n",
            "67/67 [==============================] - 76s 1s/step - loss: 1.3411 - categorical_accuracy: 0.4984 - val_loss: 1.6216 - val_categorical_accuracy: 0.4300\n",
            "\n",
            "Epoch 00002: saving model to model_init_2020-08-1411_39_44.901874/model-00002-1.34807-0.49321-1.62161-0.43000.h5\n",
            "Epoch 3/30\n",
            "67/67 [==============================] - 75s 1s/step - loss: 1.2677 - categorical_accuracy: 0.5035 - val_loss: 1.1324 - val_categorical_accuracy: 0.5800\n",
            "\n",
            "Epoch 00003: saving model to model_init_2020-08-1411_39_44.901874/model-00003-1.26956-0.50528-1.13241-0.58000.h5\n",
            "Epoch 4/30\n",
            "67/67 [==============================] - 75s 1s/step - loss: 1.2470 - categorical_accuracy: 0.5404 - val_loss: 1.2066 - val_categorical_accuracy: 0.5700\n",
            "\n",
            "Epoch 00004: saving model to model_init_2020-08-1411_39_44.901874/model-00004-1.23727-0.54600-1.20660-0.57000.h5\n",
            "Epoch 5/30\n",
            "67/67 [==============================] - 76s 1s/step - loss: 1.1087 - categorical_accuracy: 0.5926 - val_loss: 1.1760 - val_categorical_accuracy: 0.6400\n",
            "\n",
            "Epoch 00005: saving model to model_init_2020-08-1411_39_44.901874/model-00005-1.10306-0.59879-1.17596-0.64000.h5\n",
            "\n",
            "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "Epoch 6/30\n",
            "67/67 [==============================] - 76s 1s/step - loss: 1.0960 - categorical_accuracy: 0.5816 - val_loss: 1.2554 - val_categorical_accuracy: 0.4800\n",
            "\n",
            "Epoch 00006: saving model to model_init_2020-08-1411_39_44.901874/model-00006-1.09678-0.58069-1.25544-0.48000.h5\n",
            "Epoch 7/30\n",
            "67/67 [==============================] - 76s 1s/step - loss: 1.0312 - categorical_accuracy: 0.6020 - val_loss: 1.1748 - val_categorical_accuracy: 0.6200\n",
            "\n",
            "Epoch 00007: saving model to model_init_2020-08-1411_39_44.901874/model-00007-1.02958-0.60483-1.17485-0.62000.h5\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "Epoch 8/30\n",
            "67/67 [==============================] - 75s 1s/step - loss: 0.9187 - categorical_accuracy: 0.6279 - val_loss: 1.3202 - val_categorical_accuracy: 0.5800\n",
            "\n",
            "Epoch 00008: saving model to model_init_2020-08-1411_39_44.901874/model-00008-0.92085-0.62745-1.32025-0.58000.h5\n",
            "Epoch 9/30\n",
            "67/67 [==============================] - 75s 1s/step - loss: 0.9122 - categorical_accuracy: 0.6323 - val_loss: 1.1289 - val_categorical_accuracy: 0.6800\n",
            "\n",
            "Epoch 00009: saving model to model_init_2020-08-1411_39_44.901874/model-00009-0.91498-0.63198-1.12894-0.68000.h5\n",
            "Epoch 10/30\n",
            "67/67 [==============================] - 78s 1s/step - loss: 0.8783 - categorical_accuracy: 0.6552 - val_loss: 1.0917 - val_categorical_accuracy: 0.6600\n",
            "\n",
            "Epoch 00010: saving model to model_init_2020-08-1411_39_44.901874/model-00010-0.88259-0.65158-1.09167-0.66000.h5\n",
            "Epoch 11/30\n",
            "67/67 [==============================] - 77s 1s/step - loss: 0.8920 - categorical_accuracy: 0.6319 - val_loss: 1.1760 - val_categorical_accuracy: 0.6400\n",
            "\n",
            "Epoch 00011: saving model to model_init_2020-08-1411_39_44.901874/model-00011-0.88962-0.63499-1.17603-0.64000.h5\n",
            "Epoch 12/30\n",
            "67/67 [==============================] - 77s 1s/step - loss: 0.8393 - categorical_accuracy: 0.6697 - val_loss: 1.1459 - val_categorical_accuracy: 0.6800\n",
            "\n",
            "Epoch 00012: saving model to model_init_2020-08-1411_39_44.901874/model-00012-0.83720-0.66968-1.14594-0.68000.h5\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "Epoch 13/30\n",
            "67/67 [==============================] - 77s 1s/step - loss: 0.8472 - categorical_accuracy: 0.6473 - val_loss: 1.0627 - val_categorical_accuracy: 0.6800\n",
            "\n",
            "Epoch 00013: saving model to model_init_2020-08-1411_39_44.901874/model-00013-0.84494-0.64706-1.06274-0.68000.h5\n",
            "Epoch 14/30\n",
            "67/67 [==============================] - 78s 1s/step - loss: 0.8130 - categorical_accuracy: 0.6816 - val_loss: 1.1277 - val_categorical_accuracy: 0.6800\n",
            "\n",
            "Epoch 00014: saving model to model_init_2020-08-1411_39_44.901874/model-00014-0.81082-0.68175-1.12774-0.68000.h5\n",
            "Epoch 15/30\n",
            "67/67 [==============================] - 78s 1s/step - loss: 0.7395 - categorical_accuracy: 0.6726 - val_loss: 0.9911 - val_categorical_accuracy: 0.7000\n",
            "\n",
            "Epoch 00015: saving model to model_init_2020-08-1411_39_44.901874/model-00015-0.74190-0.67270-0.99108-0.70000.h5\n",
            "Epoch 16/30\n",
            "67/67 [==============================] - 76s 1s/step - loss: 0.7578 - categorical_accuracy: 0.6797 - val_loss: 1.1164 - val_categorical_accuracy: 0.7000\n",
            "\n",
            "Epoch 00016: saving model to model_init_2020-08-1411_39_44.901874/model-00016-0.75451-0.68326-1.11636-0.70000.h5\n",
            "Epoch 17/30\n",
            "67/67 [==============================] - 77s 1s/step - loss: 0.7046 - categorical_accuracy: 0.6940 - val_loss: 1.0046 - val_categorical_accuracy: 0.6800\n",
            "\n",
            "Epoch 00017: saving model to model_init_2020-08-1411_39_44.901874/model-00017-0.71089-0.69080-1.00461-0.68000.h5\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "Epoch 18/30\n",
            "67/67 [==============================] - 78s 1s/step - loss: 0.7568 - categorical_accuracy: 0.6985 - val_loss: 1.0799 - val_categorical_accuracy: 0.6800\n",
            "\n",
            "Epoch 00018: saving model to model_init_2020-08-1411_39_44.901874/model-00018-0.75823-0.69532-1.07986-0.68000.h5\n",
            "Epoch 19/30\n",
            "67/67 [==============================] - 76s 1s/step - loss: 0.7575 - categorical_accuracy: 0.6976 - val_loss: 1.1487 - val_categorical_accuracy: 0.6500\n",
            "\n",
            "Epoch 00019: saving model to model_init_2020-08-1411_39_44.901874/model-00019-0.75419-0.70136-1.14869-0.65000.h5\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "Epoch 20/30\n",
            "67/67 [==============================] - 76s 1s/step - loss: 0.7393 - categorical_accuracy: 0.6980 - val_loss: 1.0104 - val_categorical_accuracy: 0.7000\n",
            "\n",
            "Epoch 00020: saving model to model_init_2020-08-1411_39_44.901874/model-00020-0.74024-0.69834-1.01037-0.70000.h5\n",
            "Epoch 21/30\n",
            "67/67 [==============================] - 76s 1s/step - loss: 0.7468 - categorical_accuracy: 0.6955 - val_loss: 1.0701 - val_categorical_accuracy: 0.6800\n",
            "\n",
            "Epoch 00021: saving model to model_init_2020-08-1411_39_44.901874/model-00021-0.75060-0.69231-1.07009-0.68000.h5\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "Epoch 22/30\n",
            "67/67 [==============================] - 76s 1s/step - loss: 0.6909 - categorical_accuracy: 0.6965 - val_loss: 1.0488 - val_categorical_accuracy: 0.6800\n",
            "\n",
            "Epoch 00022: saving model to model_init_2020-08-1411_39_44.901874/model-00022-0.69152-0.69683-1.04884-0.68000.h5\n",
            "Epoch 23/30\n",
            "67/67 [==============================] - 76s 1s/step - loss: 0.6532 - categorical_accuracy: 0.7164 - val_loss: 1.0932 - val_categorical_accuracy: 0.6700\n",
            "\n",
            "Epoch 00023: saving model to model_init_2020-08-1411_39_44.901874/model-00023-0.65679-0.71342-1.09315-0.67000.h5\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "Epoch 24/30\n",
            "67/67 [==============================] - 78s 1s/step - loss: 0.7011 - categorical_accuracy: 0.6991 - val_loss: 1.0170 - val_categorical_accuracy: 0.7300\n",
            "\n",
            "Epoch 00024: saving model to model_init_2020-08-1411_39_44.901874/model-00024-0.69723-0.70287-1.01700-0.73000.h5\n",
            "Epoch 25/30\n",
            "67/67 [==============================] - 76s 1s/step - loss: 0.7915 - categorical_accuracy: 0.6797 - val_loss: 1.1544 - val_categorical_accuracy: 0.6800\n",
            "\n",
            "Epoch 00025: saving model to model_init_2020-08-1411_39_44.901874/model-00025-0.78147-0.68326-1.15442-0.68000.h5\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "Epoch 26/30\n",
            "67/67 [==============================] - 77s 1s/step - loss: 0.7298 - categorical_accuracy: 0.7059 - val_loss: 1.0344 - val_categorical_accuracy: 0.7000\n",
            "\n",
            "Epoch 00026: saving model to model_init_2020-08-1411_39_44.901874/model-00026-0.73208-0.70287-1.03440-0.70000.h5\n",
            "Epoch 27/30\n",
            "67/67 [==============================] - 78s 1s/step - loss: 0.6605 - categorical_accuracy: 0.6970 - val_loss: 1.0350 - val_categorical_accuracy: 0.7000\n",
            "\n",
            "Epoch 00027: saving model to model_init_2020-08-1411_39_44.901874/model-00027-0.66604-0.69382-1.03497-0.70000.h5\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "Epoch 28/30\n",
            "67/67 [==============================] - 76s 1s/step - loss: 0.7258 - categorical_accuracy: 0.7055 - val_loss: 1.1441 - val_categorical_accuracy: 0.6800\n",
            "\n",
            "Epoch 00028: saving model to model_init_2020-08-1411_39_44.901874/model-00028-0.72914-0.70588-1.14411-0.68000.h5\n",
            "Epoch 29/30\n",
            "67/67 [==============================] - 76s 1s/step - loss: 0.7308 - categorical_accuracy: 0.6980 - val_loss: 1.0603 - val_categorical_accuracy: 0.7100\n",
            "\n",
            "Epoch 00029: saving model to model_init_2020-08-1411_39_44.901874/model-00029-0.73078-0.69834-1.06035-0.71000.h5\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "Epoch 30/30\n",
            "67/67 [==============================] - 77s 1s/step - loss: 0.6700 - categorical_accuracy: 0.7234 - val_loss: 1.0281 - val_categorical_accuracy: 0.6900\n",
            "\n",
            "Epoch 00030: saving model to model_init_2020-08-1411_39_44.901874/model-00030-0.66586-0.72398-1.02814-0.69000.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0dff073e80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "A3NqWbFFADEW"
      },
      "source": [
        "**Observation**\n",
        "\n",
        "*   Training Accuracy: 72.3%\n",
        "*   Validation Accuracy: 69%\n",
        "*   Learning Rate: 4.882812731921149e-07\n",
        "*   Filter Size: 3 X 3\n",
        "*   Batch Size: 10\n",
        "*   Epoch: 30"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vnmr5SAQCr7U"
      },
      "source": [
        "**Conclusion**\n",
        "For the Conv2D we can see that as and when the number of epochs increases, the accuracy increases. But we had to restrict to a epoch of size 30 due to memory contraints."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uFRfGlRzN2hU"
      },
      "source": [
        "# Conv3D + RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eru9y0EfLmNC"
      },
      "source": [
        "**Model 1 - No Data Augmentation and Adam optimizer with\n",
        "            Image size 160 X 160, Batch Size 24 and Epoch 10**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8bGlP_8T7CZM",
        "colab": {}
      },
      "source": [
        "#write your model here\n",
        "img_filter = [8,16,32,64]\n",
        "img_dense = [256,128]\n",
        "dropout = [0.5, 0.5]\n",
        "\n",
        "# Input\n",
        "input_shape=(img_frames,img_height,img_width,img_channel)\n",
        "\n",
        "# Input Layer\n",
        "model = Sequential()\n",
        "model.add(Conv3D(img_filter[0], kernel_size=(5, 5, 5), padding='same', input_shape=input_shape))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "# Layer 2\n",
        "model.add(Conv3D(img_filter[1], (3, 3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "# Layer 3\n",
        "model.add(Conv3D(img_filter[2], (1, 3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "# Layer 4\n",
        "model.add(Conv3D(img_filter[3], (1, 3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "#Flatten the layers\n",
        "model.add(Flatten())\n",
        "\n",
        "#Fully Connected Layer\n",
        "model.add(Dense(img_dense[0],activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(dropout[0]))\n",
        "\n",
        "model.add(Dense(img_dense[1],activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(dropout[1]))\n",
        "\n",
        "#softmax layer\n",
        "model.add(Dense(nb_classes,activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zFSr13XP7CZR"
      },
      "source": [
        "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "anmImaDi7CZS",
        "scrolled": true,
        "colab": {},
        "outputId": "e7c78207-a39b-43a4-ca6a-a32a27fe4790"
      },
      "source": [
        "learning_rate = 0.001\n",
        "optimiser = Adam(learning_rate)\n",
        "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "print (model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv3d_1 (Conv3D)            (None, 30, 160, 160, 8)   3008      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 30, 160, 160, 8)   0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 30, 160, 160, 8)   32        \n",
            "_________________________________________________________________\n",
            "max_pooling3d_1 (MaxPooling3 (None, 15, 80, 80, 8)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_2 (Conv3D)            (None, 15, 80, 80, 16)    3472      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 15, 80, 80, 16)    0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 15, 80, 80, 16)    64        \n",
            "_________________________________________________________________\n",
            "max_pooling3d_2 (MaxPooling3 (None, 7, 40, 40, 16)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_3 (Conv3D)            (None, 7, 40, 40, 32)     4640      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 7, 40, 40, 32)     0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 7, 40, 40, 32)     128       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_3 (MaxPooling3 (None, 3, 20, 20, 32)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_4 (Conv3D)            (None, 3, 20, 20, 64)     18496     \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 3, 20, 20, 64)     0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 3, 20, 20, 64)     256       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_4 (MaxPooling3 (None, 1, 10, 10, 64)     0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 6400)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               1638656   \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 5)                 645       \n",
            "=================================================================\n",
            "Total params: 1,703,829\n",
            "Trainable params: 1,702,821\n",
            "Non-trainable params: 1,008\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EE2VB2LB7CZY"
      },
      "source": [
        "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HFF_uGu17CZa",
        "colab": {}
      },
      "source": [
        "train_generator = generator(train_path, train_doc, batch_size)\n",
        "val_generator = generator(val_path, val_doc, batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HlImqCMv7CZg",
        "colab": {}
      },
      "source": [
        "model_name = 'model_conv3D' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
        "    \n",
        "if not os.path.exists(model_name):\n",
        "    os.mkdir(model_name)\n",
        "        \n",
        "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
        "\n",
        "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1) # write the REducelronplateau code here\n",
        "callbacks_list = [checkpoint, LR]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4eFgYlQb7CZo"
      },
      "source": [
        "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QLTlj-aq7CZp",
        "colab": {}
      },
      "source": [
        "if (num_train_sequences%batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "if (num_val_sequences%batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences/batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences//batch_size) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cx3AH-3x7CZv"
      },
      "source": [
        "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "caloIbiY7CZx",
        "colab": {},
        "outputId": "27628e6d-e671-41aa-9639-019a56ed9eaf"
      },
      "source": [
        "num_epochs = 10\n",
        "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
        "                    callbacks=callbacks_list, validation_data=val_generator, \n",
        "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source path =  Project_data/val ; batch size = 24\n",
            "Source path =  Project_data/train ; batch size = 24\n",
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:15: DeprecationWarning: `imread` is deprecated!\n",
            "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imread`` instead.\n",
            "  from ipykernel import kernelapp as app\n",
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n",
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:39: DeprecationWarning: `imread` is deprecated!\n",
            "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imread`` instead.\n",
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:44: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "28/28 [==============================] - 204s 7s/step - loss: 2.0363 - categorical_accuracy: 0.3561 - val_loss: 2.1340 - val_categorical_accuracy: 0.2800\n",
            "\n",
            "Epoch 00001: saving model to model_conv3D_2020-08-1608_38_46.350670/model-00001-2.03345-0.35897-2.13400-0.28000.h5\n",
            "Epoch 2/10\n",
            "28/28 [==============================] - 87s 3s/step - loss: 1.4625 - categorical_accuracy: 0.5060 - val_loss: 1.1877 - val_categorical_accuracy: 0.4900\n",
            "\n",
            "Epoch 00002: saving model to model_conv3D_2020-08-1608_38_46.350670/model-00002-1.45387-0.50830-1.18769-0.49000.h5\n",
            "Epoch 3/10\n",
            "28/28 [==============================] - 91s 3s/step - loss: 1.2393 - categorical_accuracy: 0.5307 - val_loss: 1.0729 - val_categorical_accuracy: 0.5800\n",
            "\n",
            "Epoch 00003: saving model to model_conv3D_2020-08-1608_38_46.350670/model-00003-1.23598-0.53243-1.07291-0.58000.h5\n",
            "Epoch 4/10\n",
            "28/28 [==============================] - 92s 3s/step - loss: 1.0435 - categorical_accuracy: 0.6107 - val_loss: 1.0124 - val_categorical_accuracy: 0.5700\n",
            "\n",
            "Epoch 00004: saving model to model_conv3D_2020-08-1608_38_46.350670/model-00004-1.04288-0.61086-1.01243-0.57000.h5\n",
            "Epoch 5/10\n",
            "28/28 [==============================] - 92s 3s/step - loss: 0.8482 - categorical_accuracy: 0.6637 - val_loss: 1.5455 - val_categorical_accuracy: 0.4700\n",
            "\n",
            "Epoch 00005: saving model to model_conv3D_2020-08-1608_38_46.350670/model-00005-0.84865-0.66365-1.54550-0.47000.h5\n",
            "Epoch 6/10\n",
            "28/28 [==============================] - 92s 3s/step - loss: 0.7380 - categorical_accuracy: 0.7462 - val_loss: 1.0931 - val_categorical_accuracy: 0.6300\n",
            "\n",
            "Epoch 00006: saving model to model_conv3D_2020-08-1608_38_46.350670/model-00006-0.73784-0.74811-1.09315-0.63000.h5\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "Epoch 7/10\n",
            "28/28 [==============================] - 92s 3s/step - loss: 0.6032 - categorical_accuracy: 0.7622 - val_loss: 2.1713 - val_categorical_accuracy: 0.3100\n",
            "\n",
            "Epoch 00007: saving model to model_conv3D_2020-08-1608_38_46.350670/model-00007-0.60297-0.76169-2.17129-0.31000.h5\n",
            "Epoch 8/10\n",
            "28/28 [==============================] - 92s 3s/step - loss: 0.4164 - categorical_accuracy: 0.8568 - val_loss: 1.2505 - val_categorical_accuracy: 0.5500\n",
            "\n",
            "Epoch 00008: saving model to model_conv3D_2020-08-1608_38_46.350670/model-00008-0.41784-0.85671-1.25054-0.55000.h5\n",
            "\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "Epoch 9/10\n",
            "28/28 [==============================] - 92s 3s/step - loss: 0.4245 - categorical_accuracy: 0.8370 - val_loss: 1.3760 - val_categorical_accuracy: 0.5300\n",
            "\n",
            "Epoch 00009: saving model to model_conv3D_2020-08-1608_38_46.350670/model-00009-0.41910-0.84012-1.37600-0.53000.h5\n",
            "Epoch 10/10\n",
            "28/28 [==============================] - 92s 3s/step - loss: 0.3647 - categorical_accuracy: 0.8577 - val_loss: 1.3971 - val_categorical_accuracy: 0.4700\n",
            "\n",
            "Epoch 00010: saving model to model_conv3D_2020-08-1608_38_46.350670/model-00010-0.36620-0.85671-1.39714-0.47000.h5\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fec8d585710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HYZuuzR2L5GV"
      },
      "source": [
        "**Observation**\n",
        "\n",
        "*   Training Accuracy: 74.6%\n",
        "*   Validation Accuracy: 63%\n",
        "*   Learning Rate: 0.001\n",
        "*   Filter Size: 5 X 5, 3 X 3\n",
        "*   Batch Size: 24\n",
        "*   Epoch: 10\n",
        "*   Optimizer: Adam\n",
        "*   Augmentation: None\n",
        "*   Trainable Parameters: 1,702,821"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sM4y0fdFMSzr"
      },
      "source": [
        "**Model 2 - No Data Augmentation and Adam optimizer with\n",
        "            Image size 120 X 120, Batch Size 24 and Epoch 10**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HpcHjQOHK2bn",
        "colab": {}
      },
      "source": [
        "#write your model here\n",
        "img_height = 120\n",
        "img_width = 120\n",
        "img_filter = [16,32,64,128]\n",
        "img_dense = [256,128]\n",
        "dropout = [0.5,0.25]\n",
        "\n",
        "# Input\n",
        "input_shape=(img_frames,img_height,img_width,img_channel)\n",
        "\n",
        "# Input Layer\n",
        "model = Sequential()\n",
        "model.add(Conv3D(img_filter[0], kernel_size=(5, 5, 5), padding='same', input_shape=input_shape))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "# Layer 2\n",
        "model.add(Conv3D(img_filter[1], kernel_size=(3, 3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "# Layer 3\n",
        "model.add(Conv3D(img_filter[2], kernel_size=(1, 3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "# Layer 4\n",
        "model.add(Conv3D(img_filter[3], kernel_size=(1, 3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "#Flatten the layers\n",
        "model.add(Flatten())\n",
        "\n",
        "#Fully Connected Layer\n",
        "model.add(Dense(img_dense[0],activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(dropout[0]))\n",
        "\n",
        "model.add(Dense(img_dense[1],activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(dropout[1]))\n",
        "\n",
        "#softmax layer\n",
        "model.add(Dense(nb_classes,activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gjO3968DK2bx",
        "colab": {},
        "outputId": "bae4a928-c361-44b7-8217-f62b662b8229"
      },
      "source": [
        "learning_rate = 0.01\n",
        "optimiser = Adam(learning_rate)\n",
        "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "print (model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv3d_13 (Conv3D)           (None, 30, 120, 120, 16)  6016      \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 30, 120, 120, 16)  0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_19 (Batc (None, 30, 120, 120, 16)  64        \n",
            "_________________________________________________________________\n",
            "max_pooling3d_13 (MaxPooling (None, 15, 60, 60, 16)    0         \n",
            "_________________________________________________________________\n",
            "conv3d_14 (Conv3D)           (None, 15, 60, 60, 32)    13856     \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 15, 60, 60, 32)    0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_20 (Batc (None, 15, 60, 60, 32)    128       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_14 (MaxPooling (None, 7, 30, 30, 32)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_15 (Conv3D)           (None, 7, 30, 30, 64)     18496     \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 7, 30, 30, 64)     0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_21 (Batc (None, 7, 30, 30, 64)     256       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_15 (MaxPooling (None, 3, 15, 15, 64)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_16 (Conv3D)           (None, 3, 15, 15, 128)    73856     \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 3, 15, 15, 128)    0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_22 (Batc (None, 3, 15, 15, 128)    512       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_16 (MaxPooling (None, 1, 7, 7, 128)      0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 256)               1605888   \n",
            "_________________________________________________________________\n",
            "batch_normalization_23 (Batc (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "batch_normalization_24 (Batc (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 5)                 645       \n",
            "=================================================================\n",
            "Total params: 1,754,149\n",
            "Trainable params: 1,752,901\n",
            "Non-trainable params: 1,248\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AWdTm0i7K2b2",
        "colab": {}
      },
      "source": [
        "train_generator = generator(train_path, train_doc, batch_size)\n",
        "val_generator = generator(val_path, val_doc, batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Uw4yaaAEK2cK",
        "colab": {}
      },
      "source": [
        "model_name = 'model2_conv3D' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
        "    \n",
        "if not os.path.exists(model_name):\n",
        "    os.mkdir(model_name)\n",
        "        \n",
        "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
        "\n",
        "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1) # write the REducelronplateau code here\n",
        "callbacks_list = [checkpoint, LR]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "APEjM_qJK2cR",
        "colab": {}
      },
      "source": [
        "if (num_train_sequences%batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "if (num_val_sequences%batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences/batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences//batch_size) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uF8grWAEK2cW",
        "colab": {},
        "outputId": "cae5074a-f64d-49dd-c5f8-1ca5cdd409a6"
      },
      "source": [
        "num_epochs = 10\n",
        "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
        "                    callbacks=callbacks_list, validation_data=val_generator, \n",
        "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source path =  Project_data/val ; batch size = 24\n",
            "Source path =  Project_data/train ; batch size = 24\n",
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:15: DeprecationWarning: `imread` is deprecated!\n",
            "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imread`` instead.\n",
            "  from ipykernel import kernelapp as app\n",
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " 3/28 [==>...........................] - ETA: 1:54 - loss: 2.0010 - categorical_accuracy: 0.2778"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:39: DeprecationWarning: `imread` is deprecated!\n",
            "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imread`` instead.\n",
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:44: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "28/28 [==============================] - 88s 3s/step - loss: 1.7393 - categorical_accuracy: 0.3923 - val_loss: 13.1037 - val_categorical_accuracy: 0.1800\n",
            "\n",
            "Epoch 00001: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00001-1.74317-0.39216-13.10369-0.18000.h5\n",
            "Epoch 2/10\n",
            "28/28 [==============================] - 78s 3s/step - loss: 1.3826 - categorical_accuracy: 0.3965 - val_loss: 7.8441 - val_categorical_accuracy: 0.2300\n",
            "\n",
            "Epoch 00002: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00002-1.38217-0.39819-7.84411-0.23000.h5\n",
            "Epoch 3/10\n",
            "28/28 [==============================] - 80s 3s/step - loss: 1.1805 - categorical_accuracy: 0.4824 - val_loss: 2.2178 - val_categorical_accuracy: 0.3800\n",
            "\n",
            "Epoch 00003: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00003-1.17894-0.48265-2.21778-0.38000.h5\n",
            "Epoch 4/10\n",
            "28/28 [==============================] - 80s 3s/step - loss: 1.1096 - categorical_accuracy: 0.5416 - val_loss: 2.0047 - val_categorical_accuracy: 0.3800\n",
            "\n",
            "Epoch 00004: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00004-1.11287-0.53997-2.00466-0.38000.h5\n",
            "Epoch 5/10\n",
            "28/28 [==============================] - 81s 3s/step - loss: 0.9377 - categorical_accuracy: 0.6220 - val_loss: 0.9537 - val_categorical_accuracy: 0.6100\n",
            "\n",
            "Epoch 00005: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00005-0.93980-0.62142-0.95370-0.61000.h5\n",
            "Epoch 6/10\n",
            "28/28 [==============================] - 81s 3s/step - loss: 0.9116 - categorical_accuracy: 0.6214 - val_loss: 1.6002 - val_categorical_accuracy: 0.4800\n",
            "\n",
            "Epoch 00006: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00006-0.91495-0.61991-1.60019-0.48000.h5\n",
            "Epoch 7/10\n",
            "28/28 [==============================] - 80s 3s/step - loss: 0.7833 - categorical_accuracy: 0.6899 - val_loss: 1.1930 - val_categorical_accuracy: 0.6100\n",
            "\n",
            "Epoch 00007: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00007-0.78352-0.68929-1.19305-0.61000.h5\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
            "Epoch 8/10\n",
            "28/28 [==============================] - 80s 3s/step - loss: 0.5961 - categorical_accuracy: 0.7396 - val_loss: 0.8234 - val_categorical_accuracy: 0.6900\n",
            "\n",
            "Epoch 00008: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00008-0.59339-0.74057-0.82345-0.69000.h5\n",
            "Epoch 9/10\n",
            "28/28 [==============================] - 80s 3s/step - loss: 0.5533 - categorical_accuracy: 0.7994 - val_loss: 1.6801 - val_categorical_accuracy: 0.4500\n",
            "\n",
            "Epoch 00009: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00009-0.55179-0.79940-1.68007-0.45000.h5\n",
            "Epoch 10/10\n",
            "28/28 [==============================] - 80s 3s/step - loss: 0.4274 - categorical_accuracy: 0.8202 - val_loss: 0.9148 - val_categorical_accuracy: 0.7000\n",
            "\n",
            "Epoch 00010: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00010-0.42706-0.82051-0.91476-0.70000.h5\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fec438098d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CsRiu0QzMx9q"
      },
      "source": [
        "**Observation**\n",
        "\n",
        "*   Training Accuracy: 74%\n",
        "*   Validation Accuracy: 69%\n",
        "*   Learning Rate: 0.005\n",
        "*   Filter Size: 5 X 5, 3 X 3\n",
        "*   Batch Size: 24\n",
        "*   Epoch: 10\n",
        "*   Optimizer: Adam\n",
        "*   Augmentation: None\n",
        "*   Trainable Parameters: 1,752,901"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "R7rCFUwVOZoL"
      },
      "source": [
        "**Model 3 - No Data Augmentation and Adam optimizer with\n",
        "            Image size 120 X 120, Batch Size 24 and Epoch 30**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RmG7otSe7CZ-",
        "colab": {},
        "outputId": "18331070-3a47-4e36-fed9-7e2f0441e76e"
      },
      "source": [
        "num_epochs = 30\n",
        "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
        "                    callbacks=callbacks_list, validation_data=val_generator, \n",
        "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:15: DeprecationWarning: `imread` is deprecated!\n",
            "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imread`` instead.\n",
            "  from ipykernel import kernelapp as app\n",
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " 3/28 [==>...........................] - ETA: 1:33 - loss: 0.3524 - categorical_accuracy: 0.8889"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:39: DeprecationWarning: `imread` is deprecated!\n",
            "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imread`` instead.\n",
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:44: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "28/28 [==============================] - 87s 3s/step - loss: 0.3746 - categorical_accuracy: 0.8634 - val_loss: 0.7911 - val_categorical_accuracy: 0.7200\n",
            "\n",
            "Epoch 00001: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00001-0.37419-0.86425-0.79107-0.72000.h5\n",
            "Epoch 2/30\n",
            "28/28 [==============================] - 80s 3s/step - loss: 0.3415 - categorical_accuracy: 0.8866 - val_loss: 0.7362 - val_categorical_accuracy: 0.7000\n",
            "\n",
            "Epoch 00002: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00002-0.34073-0.88688-0.73618-0.70000.h5\n",
            "Epoch 3/30\n",
            "28/28 [==============================] - 80s 3s/step - loss: 0.2515 - categorical_accuracy: 0.9134 - val_loss: 1.0023 - val_categorical_accuracy: 0.6500\n",
            "\n",
            "Epoch 00003: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00003-0.25061-0.91403-1.00226-0.65000.h5\n",
            "Epoch 4/30\n",
            "28/28 [==============================] - 80s 3s/step - loss: 0.2747 - categorical_accuracy: 0.9024 - val_loss: 0.8226 - val_categorical_accuracy: 0.7100\n",
            "\n",
            "Epoch 00004: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00004-0.27547-0.90196-0.82263-0.71000.h5\n",
            "\n",
            "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
            "Epoch 5/30\n",
            "28/28 [==============================] - 80s 3s/step - loss: 0.1991 - categorical_accuracy: 0.9277 - val_loss: 0.8544 - val_categorical_accuracy: 0.7000\n",
            "\n",
            "Epoch 00005: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00005-0.19954-0.92760-0.85436-0.70000.h5\n",
            "Epoch 6/30\n",
            "28/28 [==============================] - 80s 3s/step - loss: 0.1683 - categorical_accuracy: 0.9494 - val_loss: 0.7311 - val_categorical_accuracy: 0.7700\n",
            "\n",
            "Epoch 00006: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00006-0.16932-0.94872-0.73106-0.77000.h5\n",
            "Epoch 7/30\n",
            "28/28 [==============================] - 81s 3s/step - loss: 0.1718 - categorical_accuracy: 0.9434 - val_loss: 0.7726 - val_categorical_accuracy: 0.7500\n",
            "\n",
            "Epoch 00007: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00007-0.17318-0.94268-0.77256-0.75000.h5\n",
            "Epoch 8/30\n",
            "28/28 [==============================] - 80s 3s/step - loss: 0.2029 - categorical_accuracy: 0.9270 - val_loss: 0.6844 - val_categorical_accuracy: 0.7600\n",
            "\n",
            "Epoch 00008: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00008-0.19099-0.93213-0.68442-0.76000.h5\n",
            "Epoch 9/30\n",
            "28/28 [==============================] - 80s 3s/step - loss: 0.1241 - categorical_accuracy: 0.9747 - val_loss: 1.2284 - val_categorical_accuracy: 0.5600\n",
            "\n",
            "Epoch 00009: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00009-0.12470-0.97436-1.22838-0.56000.h5\n",
            "Epoch 10/30\n",
            "28/28 [==============================] - 80s 3s/step - loss: 0.1434 - categorical_accuracy: 0.9479 - val_loss: 0.6364 - val_categorical_accuracy: 0.8000\n",
            "\n",
            "Epoch 00010: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00010-0.14391-0.94721-0.63639-0.80000.h5\n",
            "Epoch 11/30\n",
            "28/28 [==============================] - 82s 3s/step - loss: 0.1194 - categorical_accuracy: 0.9604 - val_loss: 0.9300 - val_categorical_accuracy: 0.6700\n",
            "\n",
            "Epoch 00011: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00011-0.11875-0.96078-0.92998-0.67000.h5\n",
            "Epoch 12/30\n",
            "28/28 [==============================] - 80s 3s/step - loss: 0.0947 - categorical_accuracy: 0.9670 - val_loss: 0.6652 - val_categorical_accuracy: 0.7300\n",
            "\n",
            "Epoch 00012: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00012-0.09192-0.96833-0.66522-0.73000.h5\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
            "Epoch 13/30\n",
            "28/28 [==============================] - 80s 3s/step - loss: 0.0977 - categorical_accuracy: 0.9774 - val_loss: 0.8815 - val_categorical_accuracy: 0.7500\n",
            "\n",
            "Epoch 00013: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00013-0.09467-0.97888-0.88147-0.75000.h5\n",
            "Epoch 14/30\n",
            "28/28 [==============================] - 79s 3s/step - loss: 0.0959 - categorical_accuracy: 0.9679 - val_loss: 0.5084 - val_categorical_accuracy: 0.8200\n",
            "\n",
            "Epoch 00014: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00014-0.09434-0.96833-0.50842-0.82000.h5\n",
            "Epoch 15/30\n",
            "28/28 [==============================] - 80s 3s/step - loss: 0.0981 - categorical_accuracy: 0.9679 - val_loss: 0.5448 - val_categorical_accuracy: 0.8100\n",
            "\n",
            "Epoch 00015: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00015-0.09696-0.96833-0.54481-0.81000.h5\n",
            "Epoch 16/30\n",
            "28/28 [==============================] - 81s 3s/step - loss: 0.0885 - categorical_accuracy: 0.9717 - val_loss: 0.6225 - val_categorical_accuracy: 0.7500\n",
            "\n",
            "Epoch 00016: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00016-0.08922-0.97134-0.62246-0.75000.h5\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
            "Epoch 17/30\n",
            "28/28 [==============================] - 80s 3s/step - loss: 0.0654 - categorical_accuracy: 0.9851 - val_loss: 0.5032 - val_categorical_accuracy: 0.8000\n",
            "\n",
            "Epoch 00017: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00017-0.06540-0.98492-0.50322-0.80000.h5\n",
            "Epoch 18/30\n",
            "28/28 [==============================] - 79s 3s/step - loss: 0.0718 - categorical_accuracy: 0.9747 - val_loss: 0.6580 - val_categorical_accuracy: 0.7900\n",
            "\n",
            "Epoch 00018: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00018-0.07213-0.97436-0.65798-0.79000.h5\n",
            "Epoch 19/30\n",
            "28/28 [==============================] - 80s 3s/step - loss: 0.0873 - categorical_accuracy: 0.9806 - val_loss: 0.5618 - val_categorical_accuracy: 0.7900\n",
            "\n",
            "Epoch 00019: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00019-0.08833-0.98039-0.56181-0.79000.h5\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
            "Epoch 20/30\n",
            "28/28 [==============================] - 80s 3s/step - loss: 0.0738 - categorical_accuracy: 0.9843 - val_loss: 0.5706 - val_categorical_accuracy: 0.7900\n",
            "\n",
            "Epoch 00020: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00020-0.07337-0.98492-0.57058-0.79000.h5\n",
            "Epoch 21/30\n",
            "28/28 [==============================] - 81s 3s/step - loss: 0.0686 - categorical_accuracy: 0.9821 - val_loss: 0.5947 - val_categorical_accuracy: 0.8200\n",
            "\n",
            "Epoch 00021: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00021-0.06893-0.98190-0.59469-0.82000.h5\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
            "Epoch 22/30\n",
            "28/28 [==============================] - 80s 3s/step - loss: 0.0709 - categorical_accuracy: 0.9843 - val_loss: 0.6005 - val_categorical_accuracy: 0.7800\n",
            "\n",
            "Epoch 00022: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00022-0.06604-0.98492-0.60050-0.78000.h5\n",
            "Epoch 23/30\n",
            "28/28 [==============================] - 80s 3s/step - loss: 0.0567 - categorical_accuracy: 0.9836 - val_loss: 0.5865 - val_categorical_accuracy: 0.8100\n",
            "\n",
            "Epoch 00023: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00023-0.05722-0.98341-0.58654-0.81000.h5\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
            "Epoch 24/30\n",
            "28/28 [==============================] - 80s 3s/step - loss: 0.0473 - categorical_accuracy: 0.9911 - val_loss: 0.4431 - val_categorical_accuracy: 0.8300\n",
            "\n",
            "Epoch 00024: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00024-0.04747-0.99095-0.44312-0.83000.h5\n",
            "Epoch 25/30\n",
            "28/28 [==============================] - 81s 3s/step - loss: 0.0888 - categorical_accuracy: 0.9670 - val_loss: 0.6438 - val_categorical_accuracy: 0.7800\n",
            "\n",
            "Epoch 00025: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00025-0.08388-0.96833-0.64376-0.78000.h5\n",
            "Epoch 26/30\n",
            "28/28 [==============================] - 80s 3s/step - loss: 0.0472 - categorical_accuracy: 0.9911 - val_loss: 0.4901 - val_categorical_accuracy: 0.8300\n",
            "\n",
            "Epoch 00026: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00026-0.04706-0.99095-0.49008-0.83000.h5\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
            "Epoch 27/30\n",
            "28/28 [==============================] - 79s 3s/step - loss: 0.0641 - categorical_accuracy: 0.9777 - val_loss: 0.6212 - val_categorical_accuracy: 0.7700\n",
            "\n",
            "Epoch 00027: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00027-0.06438-0.97738-0.62122-0.77000.h5\n",
            "Epoch 28/30\n",
            "28/28 [==============================] - 80s 3s/step - loss: 0.0709 - categorical_accuracy: 0.9804 - val_loss: 0.5393 - val_categorical_accuracy: 0.8400\n",
            "\n",
            "Epoch 00028: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00028-0.06707-0.98190-0.53927-0.84000.h5\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
            "Epoch 29/30\n",
            "28/28 [==============================] - 80s 3s/step - loss: 0.0521 - categorical_accuracy: 0.9896 - val_loss: 0.5428 - val_categorical_accuracy: 0.7900\n",
            "\n",
            "Epoch 00029: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00029-0.05268-0.98944-0.54282-0.79000.h5\n",
            "Epoch 30/30\n",
            "28/28 [==============================] - 79s 3s/step - loss: 0.0671 - categorical_accuracy: 0.9821 - val_loss: 0.6600 - val_categorical_accuracy: 0.8000\n",
            "\n",
            "Epoch 00030: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00030-0.06740-0.98190-0.66000-0.80000.h5\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fec439af128>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nqihz2aGBnS7"
      },
      "source": [
        "**Observation**\n",
        "\n",
        "*   Training Accuracy: 96.8%\n",
        "*   Validation Accuracy: 82%\n",
        "*   Learning Rate: 0.0006\n",
        "*   Filter Size: 5 X 5, 3 X 3\n",
        "*   Batch Size: 24\n",
        "*   Epoch: 30\n",
        "*   Optimizer: Adam\n",
        "*   Augmentation: None\n",
        "*   Trainable Parameters: 1,752,901"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "U95XSmhFN1nz"
      },
      "source": [
        "**Model 4 - No Data Augmentation and Adam optimizer with\n",
        "            Image size 120 X 120, Batch Size 32 and Epoch 30**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AaZDhhacK2cp",
        "colab": {},
        "outputId": "44c3aca1-febb-4149-b0be-731c3fa26dc6"
      },
      "source": [
        "if (num_train_sequences%batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "if (num_val_sequences%batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences/batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences//batch_size) + 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:15: DeprecationWarning: `imread` is deprecated!\n",
            "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imread`` instead.\n",
            "  from ipykernel import kernelapp as app\n",
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n",
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:39: DeprecationWarning: `imread` is deprecated!\n",
            "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imread`` instead.\n",
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:44: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "28/28 [==============================] - 86s 3s/step - loss: 0.0540 - categorical_accuracy: 0.9828 - val_loss: 0.5753 - val_categorical_accuracy: 0.8100\n",
            "\n",
            "Epoch 00001: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00001-0.05310-0.98341-0.57532-0.81000.h5\n",
            "Epoch 2/30\n",
            "28/28 [==============================] - 81s 3s/step - loss: 0.0595 - categorical_accuracy: 0.9866 - val_loss: 0.5052 - val_categorical_accuracy: 0.8200\n",
            "\n",
            "Epoch 00002: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00002-0.05943-0.98643-0.50518-0.82000.h5\n",
            "Epoch 3/30\n",
            "28/28 [==============================] - 81s 3s/step - loss: 0.0513 - categorical_accuracy: 0.9857 - val_loss: 0.6851 - val_categorical_accuracy: 0.7800\n",
            "\n",
            "Epoch 00003: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00003-0.05063-0.98643-0.68508-0.78000.h5\n",
            "Epoch 4/30\n",
            "28/28 [==============================] - 80s 3s/step - loss: 0.0523 - categorical_accuracy: 0.9955 - val_loss: 0.4138 - val_categorical_accuracy: 0.8400\n",
            "\n",
            "Epoch 00004: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00004-0.05248-0.99548-0.41384-0.84000.h5\n",
            "Epoch 5/30\n",
            "28/28 [==============================] - 80s 3s/step - loss: 0.0522 - categorical_accuracy: 0.9851 - val_loss: 0.7025 - val_categorical_accuracy: 0.8000\n",
            "\n",
            "Epoch 00005: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00005-0.05134-0.98492-0.70252-0.80000.h5\n",
            "Epoch 6/30\n",
            "28/28 [==============================] - 80s 3s/step - loss: 0.0628 - categorical_accuracy: 0.9857 - val_loss: 0.5970 - val_categorical_accuracy: 0.8000\n",
            "\n",
            "Epoch 00006: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00006-0.05921-0.98643-0.59703-0.80000.h5\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
            "Epoch 7/30\n",
            "28/28 [==============================] - 80s 3s/step - loss: 0.0520 - categorical_accuracy: 0.9896 - val_loss: 0.5848 - val_categorical_accuracy: 0.8100\n",
            "\n",
            "Epoch 00007: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00007-0.05233-0.98944-0.58475-0.81000.h5\n",
            "Epoch 8/30\n",
            "28/28 [==============================] - 81s 3s/step - loss: 0.0777 - categorical_accuracy: 0.9762 - val_loss: 0.5451 - val_categorical_accuracy: 0.8200\n",
            "\n",
            "Epoch 00008: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00008-0.07714-0.97587-0.54505-0.82000.h5\n",
            "\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
            "Epoch 9/30\n",
            "28/28 [==============================] - 81s 3s/step - loss: 0.0530 - categorical_accuracy: 0.9917 - val_loss: 0.5666 - val_categorical_accuracy: 0.8200\n",
            "\n",
            "Epoch 00009: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00009-0.05276-0.99246-0.56663-0.82000.h5\n",
            "Epoch 10/30\n",
            "28/28 [==============================] - 80s 3s/step - loss: 0.0647 - categorical_accuracy: 0.9798 - val_loss: 0.3913 - val_categorical_accuracy: 0.8700\n",
            "\n",
            "Epoch 00010: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00010-0.06412-0.98039-0.39134-0.87000.h5\n",
            "Epoch 11/30\n",
            "28/28 [==============================] - 81s 3s/step - loss: 0.0608 - categorical_accuracy: 0.9836 - val_loss: 0.6405 - val_categorical_accuracy: 0.7900\n",
            "\n",
            "Epoch 00011: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00011-0.06121-0.98341-0.64046-0.79000.h5\n",
            "Epoch 12/30\n",
            "28/28 [==============================] - 81s 3s/step - loss: 0.0583 - categorical_accuracy: 0.9843 - val_loss: 0.6274 - val_categorical_accuracy: 0.8300\n",
            "\n",
            "Epoch 00012: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00012-0.05742-0.98492-0.62740-0.83000.h5\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
            "Epoch 13/30\n",
            "28/28 [==============================] - 79s 3s/step - loss: 0.0673 - categorical_accuracy: 0.9821 - val_loss: 0.5118 - val_categorical_accuracy: 0.8200\n",
            "\n",
            "Epoch 00013: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00013-0.06775-0.98190-0.51178-0.82000.h5\n",
            "Epoch 14/30\n",
            "28/28 [==============================] - 81s 3s/step - loss: 0.0679 - categorical_accuracy: 0.9851 - val_loss: 0.5873 - val_categorical_accuracy: 0.7900\n",
            "\n",
            "Epoch 00014: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00014-0.06781-0.98492-0.58733-0.79000.h5\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
            "Epoch 15/30\n",
            "28/28 [==============================] - 82s 3s/step - loss: 0.0536 - categorical_accuracy: 0.9881 - val_loss: 0.5257 - val_categorical_accuracy: 0.8500\n",
            "\n",
            "Epoch 00015: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00015-0.05365-0.98793-0.52570-0.85000.h5\n",
            "Epoch 16/30\n",
            "28/28 [==============================] - 80s 3s/step - loss: 0.0578 - categorical_accuracy: 0.9881 - val_loss: 0.6247 - val_categorical_accuracy: 0.8100\n",
            "\n",
            "Epoch 00016: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00016-0.05834-0.98793-0.62468-0.81000.h5\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
            "Epoch 17/30\n",
            "28/28 [==============================] - 80s 3s/step - loss: 0.0808 - categorical_accuracy: 0.9751 - val_loss: 0.5969 - val_categorical_accuracy: 0.7800\n",
            "\n",
            "Epoch 00017: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00017-0.07761-0.97738-0.59690-0.78000.h5\n",
            "Epoch 18/30\n",
            "28/28 [==============================] - 82s 3s/step - loss: 0.0567 - categorical_accuracy: 0.9896 - val_loss: 0.4439 - val_categorical_accuracy: 0.8600\n",
            "\n",
            "Epoch 00018: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00018-0.05727-0.98944-0.44391-0.86000.h5\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 7.629394360719743e-08.\n",
            "Epoch 19/30\n",
            "28/28 [==============================] - 81s 3s/step - loss: 0.0609 - categorical_accuracy: 0.9872 - val_loss: 0.4751 - val_categorical_accuracy: 0.8500\n",
            "\n",
            "Epoch 00019: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00019-0.06029-0.98793-0.47514-0.85000.h5\n",
            "Epoch 20/30\n",
            "28/28 [==============================] - 82s 3s/step - loss: 0.0552 - categorical_accuracy: 0.9851 - val_loss: 0.8240 - val_categorical_accuracy: 0.7300\n",
            "\n",
            "Epoch 00020: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00020-0.05577-0.98492-0.82398-0.73000.h5\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
            "Epoch 21/30\n",
            "28/28 [==============================] - 81s 3s/step - loss: 0.0782 - categorical_accuracy: 0.9783 - val_loss: 0.4760 - val_categorical_accuracy: 0.8600\n",
            "\n",
            "Epoch 00021: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00021-0.07488-0.97888-0.47602-0.86000.h5\n",
            "Epoch 22/30\n",
            "28/28 [==============================] - 78s 3s/step - loss: 0.0543 - categorical_accuracy: 0.9851 - val_loss: 0.5328 - val_categorical_accuracy: 0.8100\n",
            "\n",
            "Epoch 00022: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00022-0.05417-0.98492-0.53278-0.81000.h5\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.907348590179936e-08.\n",
            "Epoch 23/30\n",
            "28/28 [==============================] - 81s 3s/step - loss: 0.0643 - categorical_accuracy: 0.9836 - val_loss: 0.5629 - val_categorical_accuracy: 0.8300\n",
            "\n",
            "Epoch 00023: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00023-0.06498-0.98341-0.56290-0.83000.h5\n",
            "Epoch 24/30\n",
            "28/28 [==============================] - 81s 3s/step - loss: 0.0553 - categorical_accuracy: 0.9819 - val_loss: 0.5238 - val_categorical_accuracy: 0.8500\n",
            "\n",
            "Epoch 00024: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00024-0.05405-0.98341-0.52375-0.85000.h5\n",
            "\n",
            "Epoch 00024: ReduceLROnPlateau reducing learning rate to 9.53674295089968e-09.\n",
            "Epoch 25/30\n",
            "28/28 [==============================] - 81s 3s/step - loss: 0.0524 - categorical_accuracy: 0.9885 - val_loss: 0.6326 - val_categorical_accuracy: 0.7900\n",
            "\n",
            "Epoch 00025: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00025-0.04721-0.99095-0.63257-0.79000.h5\n",
            "Epoch 26/30\n",
            "28/28 [==============================] - 82s 3s/step - loss: 0.0492 - categorical_accuracy: 0.9896 - val_loss: 0.4851 - val_categorical_accuracy: 0.8100\n",
            "\n",
            "Epoch 00026: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00026-0.04859-0.98944-0.48505-0.81000.h5\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 4.76837147544984e-09.\n",
            "Epoch 27/30\n",
            "28/28 [==============================] - 81s 3s/step - loss: 0.0601 - categorical_accuracy: 0.9828 - val_loss: 0.6316 - val_categorical_accuracy: 0.8400\n",
            "\n",
            "Epoch 00027: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00027-0.05933-0.98341-0.63157-0.84000.h5\n",
            "Epoch 28/30\n",
            "28/28 [==============================] - 81s 3s/step - loss: 0.0440 - categorical_accuracy: 0.9896 - val_loss: 0.5828 - val_categorical_accuracy: 0.7700\n",
            "\n",
            "Epoch 00028: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00028-0.04428-0.98944-0.58284-0.77000.h5\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 2.38418573772492e-09.\n",
            "Epoch 29/30\n",
            "28/28 [==============================] - 81s 3s/step - loss: 0.0574 - categorical_accuracy: 0.9902 - val_loss: 0.5517 - val_categorical_accuracy: 0.8300\n",
            "\n",
            "Epoch 00029: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00029-0.05714-0.99095-0.55172-0.83000.h5\n",
            "Epoch 30/30\n",
            "28/28 [==============================] - 82s 3s/step - loss: 0.0582 - categorical_accuracy: 0.9821 - val_loss: 0.6274 - val_categorical_accuracy: 0.8200\n",
            "\n",
            "Epoch 00030: saving model to model2_conv3D_2020-08-1608_38_46.350670/model-00030-0.05804-0.98190-0.62743-0.82000.h5\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.19209286886246e-09.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fec361511d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "exvEkgdyOW4c"
      },
      "source": [
        "**Observation**\n",
        "\n",
        "*   Training Accuracy: 98%\n",
        "*   Validation Accuracy: 87%\n",
        "*   Learning Rate: 1.220703097715159e-06\n",
        "*   Filter Size: 5 X 5, 3 X 3\n",
        "*   Batch Size: 32\n",
        "*   Epoch: 10\n",
        "*   Optimizer: Adam\n",
        "*   Augmentation: None\n",
        "*   Trainable Parameters: 1,752,901"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Yyh1m2lbO4fL"
      },
      "source": [
        "**Model 5 - No Data Augmentation and Adam optimizer with\n",
        "            Filter Size of (3,3,3), Image size 120 X 120, Batch Size 32 and Epoch 30**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VAbQ8nCDK2cx",
        "colab": {}
      },
      "source": [
        "#write your model here\n",
        "img_height = 120\n",
        "img_width = 120\n",
        "img_filter = [16,32,64,128]\n",
        "img_dense = [256,128]\n",
        "dropout = [0.5,0.25]\n",
        "\n",
        "# Input\n",
        "input_shape=(img_frames,img_height,img_width,img_channel)\n",
        "\n",
        "# Input Layer\n",
        "model = Sequential()\n",
        "model.add(Conv3D(img_filter[0], kernel_size=(3, 3, 3), padding='same', input_shape=input_shape))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "# Layer 2\n",
        "model.add(Conv3D(img_filter[1], kernel_size=(3, 3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "# Layer 3\n",
        "model.add(Conv3D(img_filter[2], kernel_size=(3, 3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "# Layer 4\n",
        "model.add(Conv3D(img_filter[3], kernel_size=(3, 3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "#Flatten the layers\n",
        "model.add(Flatten())\n",
        "\n",
        "#Fully Connected Layer\n",
        "model.add(Dense(img_dense[0],activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(dropout[0]))\n",
        "\n",
        "model.add(Dense(img_dense[1],activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(dropout[1]))\n",
        "\n",
        "#softmax layer\n",
        "model.add(Dense(nb_classes,activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3mfnVMnSK2c1",
        "colab": {},
        "outputId": "3722b4c2-3548-42e0-9d25-ed6ed782e942"
      },
      "source": [
        "learning_rate = 0.01\n",
        "optimiser = Adam(learning_rate)\n",
        "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "print (model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv3d_17 (Conv3D)           (None, 30, 120, 120, 16)  1312      \n",
            "_________________________________________________________________\n",
            "activation_17 (Activation)   (None, 30, 120, 120, 16)  0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_25 (Batc (None, 30, 120, 120, 16)  64        \n",
            "_________________________________________________________________\n",
            "max_pooling3d_17 (MaxPooling (None, 15, 60, 60, 16)    0         \n",
            "_________________________________________________________________\n",
            "conv3d_18 (Conv3D)           (None, 15, 60, 60, 32)    13856     \n",
            "_________________________________________________________________\n",
            "activation_18 (Activation)   (None, 15, 60, 60, 32)    0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_26 (Batc (None, 15, 60, 60, 32)    128       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_18 (MaxPooling (None, 7, 30, 30, 32)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_19 (Conv3D)           (None, 7, 30, 30, 64)     55360     \n",
            "_________________________________________________________________\n",
            "activation_19 (Activation)   (None, 7, 30, 30, 64)     0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_27 (Batc (None, 7, 30, 30, 64)     256       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_19 (MaxPooling (None, 3, 15, 15, 64)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_20 (Conv3D)           (None, 3, 15, 15, 128)    221312    \n",
            "_________________________________________________________________\n",
            "activation_20 (Activation)   (None, 3, 15, 15, 128)    0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_28 (Batc (None, 3, 15, 15, 128)    512       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_20 (MaxPooling (None, 1, 7, 7, 128)      0         \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 256)               1605888   \n",
            "_________________________________________________________________\n",
            "batch_normalization_29 (Batc (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "batch_normalization_30 (Batc (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 5)                 645       \n",
            "=================================================================\n",
            "Total params: 1,933,765\n",
            "Trainable params: 1,932,517\n",
            "Non-trainable params: 1,248\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GCdX14n8K2c6",
        "colab": {}
      },
      "source": [
        "train_generator = generator(train_path, train_doc, batch_size)\n",
        "val_generator = generator(val_path, val_doc, batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mhOlTsosK2c9",
        "colab": {}
      },
      "source": [
        "model_name = 'model4_conv3D' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
        "    \n",
        "if not os.path.exists(model_name):\n",
        "    os.mkdir(model_name)\n",
        "        \n",
        "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
        "\n",
        "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1) # write the REducelronplateau code here\n",
        "callbacks_list = [checkpoint, LR]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KPPjtsclK2dC",
        "colab": {}
      },
      "source": [
        "if (num_train_sequences%batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "if (num_val_sequences%batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences/batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences//batch_size) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3GwS-cRbK2dH",
        "colab": {},
        "outputId": "7b250fc6-ecc1-4a9d-dade-4e6b8be431dc"
      },
      "source": [
        "num_epochs = 30\n",
        "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
        "                    callbacks=callbacks_list, validation_data=val_generator, \n",
        "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source path =  Project_data/val Source path =  Project_data/train ; batch size = 32\n",
            "; batch size = 32\n",
            "Epoch 1/30\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:15: DeprecationWarning: `imread` is deprecated!\n",
            "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imread`` instead.\n",
            "  from ipykernel import kernelapp as app\n",
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " 2/21 [=>............................] - ETA: 2:11 - loss: 2.3826 - categorical_accuracy: 0.2031"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:39: DeprecationWarning: `imread` is deprecated!\n",
            "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imread`` instead.\n",
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:44: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21/21 [==============================] - 92s 4s/step - loss: 1.7611 - categorical_accuracy: 0.3752 - val_loss: 12.4109 - val_categorical_accuracy: 0.2300\n",
            "\n",
            "Epoch 00001: saving model to model4_conv3D_2020-08-1608_38_46.350670/model-00001-1.76100-0.37557-12.41093-0.23000.h5\n",
            "Epoch 2/30\n",
            "21/21 [==============================] - 76s 4s/step - loss: 1.2326 - categorical_accuracy: 0.5014 - val_loss: 8.6929 - val_categorical_accuracy: 0.2300\n",
            "\n",
            "Epoch 00002: saving model to model4_conv3D_2020-08-1608_38_46.350670/model-00002-1.23323-0.50226-8.69289-0.23000.h5\n",
            "Epoch 3/30\n",
            "21/21 [==============================] - 80s 4s/step - loss: 1.0957 - categorical_accuracy: 0.5474 - val_loss: 5.5931 - val_categorical_accuracy: 0.2400\n",
            "\n",
            "Epoch 00003: saving model to model4_conv3D_2020-08-1608_38_46.350670/model-00003-1.09981-0.54600-5.59310-0.24000.h5\n",
            "Epoch 4/30\n",
            "21/21 [==============================] - 79s 4s/step - loss: 0.9683 - categorical_accuracy: 0.6095 - val_loss: 1.8976 - val_categorical_accuracy: 0.3900\n",
            "\n",
            "Epoch 00004: saving model to model4_conv3D_2020-08-1608_38_46.350670/model-00004-0.96582-0.61237-1.89756-0.39000.h5\n",
            "Epoch 5/30\n",
            "21/21 [==============================] - 79s 4s/step - loss: 0.7640 - categorical_accuracy: 0.6990 - val_loss: 0.9485 - val_categorical_accuracy: 0.6400\n",
            "\n",
            "Epoch 00005: saving model to model4_conv3D_2020-08-1608_38_46.350670/model-00005-0.76649-0.69683-0.94849-0.64000.h5\n",
            "Epoch 6/30\n",
            "21/21 [==============================] - 80s 4s/step - loss: 0.7827 - categorical_accuracy: 0.6773 - val_loss: 0.9115 - val_categorical_accuracy: 0.6000\n",
            "\n",
            "Epoch 00006: saving model to model4_conv3D_2020-08-1608_38_46.350670/model-00006-0.77733-0.67873-0.91146-0.60000.h5\n",
            "Epoch 7/30\n",
            "21/21 [==============================] - 80s 4s/step - loss: 0.7971 - categorical_accuracy: 0.6903 - val_loss: 0.9994 - val_categorical_accuracy: 0.6100\n",
            "\n",
            "Epoch 00007: saving model to model4_conv3D_2020-08-1608_38_46.350670/model-00007-0.79562-0.69080-0.99936-0.61000.h5\n",
            "Epoch 8/30\n",
            "21/21 [==============================] - 80s 4s/step - loss: 0.7091 - categorical_accuracy: 0.7028 - val_loss: 1.1643 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00008: saving model to model4_conv3D_2020-08-1608_38_46.350670/model-00008-0.70822-0.70287-1.16430-0.50000.h5\n",
            "\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
            "Epoch 9/30\n",
            "21/21 [==============================] - 80s 4s/step - loss: 0.5381 - categorical_accuracy: 0.7692 - val_loss: 1.2953 - val_categorical_accuracy: 0.5600\n",
            "\n",
            "Epoch 00009: saving model to model4_conv3D_2020-08-1608_38_46.350670/model-00009-0.53730-0.77074-1.29535-0.56000.h5\n",
            "Epoch 10/30\n",
            "21/21 [==============================] - 80s 4s/step - loss: 0.4175 - categorical_accuracy: 0.8515 - val_loss: 0.7637 - val_categorical_accuracy: 0.7400\n",
            "\n",
            "Epoch 00010: saving model to model4_conv3D_2020-08-1608_38_46.350670/model-00010-0.41599-0.85068-0.76368-0.74000.h5\n",
            "Epoch 11/30\n",
            "21/21 [==============================] - 80s 4s/step - loss: 0.3406 - categorical_accuracy: 0.8816 - val_loss: 0.9334 - val_categorical_accuracy: 0.6800\n",
            "\n",
            "Epoch 00011: saving model to model4_conv3D_2020-08-1608_38_46.350670/model-00011-0.33802-0.88235-0.93341-0.68000.h5\n",
            "Epoch 12/30\n",
            "21/21 [==============================] - 80s 4s/step - loss: 0.3367 - categorical_accuracy: 0.8653 - val_loss: 1.0814 - val_categorical_accuracy: 0.6500\n",
            "\n",
            "Epoch 00012: saving model to model4_conv3D_2020-08-1608_38_46.350670/model-00012-0.33512-0.86576-1.08135-0.65000.h5\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
            "Epoch 13/30\n",
            "21/21 [==============================] - 80s 4s/step - loss: 0.2679 - categorical_accuracy: 0.9027 - val_loss: 1.0314 - val_categorical_accuracy: 0.6300\n",
            "\n",
            "Epoch 00013: saving model to model4_conv3D_2020-08-1608_38_46.350670/model-00013-0.26876-0.90196-1.03142-0.63000.h5\n",
            "Epoch 14/30\n",
            "21/21 [==============================] - 80s 4s/step - loss: 0.2533 - categorical_accuracy: 0.9015 - val_loss: 0.9363 - val_categorical_accuracy: 0.7100\n",
            "\n",
            "Epoch 00014: saving model to model4_conv3D_2020-08-1608_38_46.350670/model-00014-0.25209-0.90196-0.93627-0.71000.h5\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
            "Epoch 15/30\n",
            "21/21 [==============================] - 80s 4s/step - loss: 0.1922 - categorical_accuracy: 0.9270 - val_loss: 0.8930 - val_categorical_accuracy: 0.7000\n",
            "\n",
            "Epoch 00015: saving model to model4_conv3D_2020-08-1608_38_46.350670/model-00015-0.19325-0.92609-0.89305-0.70000.h5\n",
            "Epoch 16/30\n",
            "21/21 [==============================] - 81s 4s/step - loss: 0.2193 - categorical_accuracy: 0.9185 - val_loss: 0.8051 - val_categorical_accuracy: 0.7200\n",
            "\n",
            "Epoch 00016: saving model to model4_conv3D_2020-08-1608_38_46.350670/model-00016-0.22027-0.91855-0.80508-0.72000.h5\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
            "Epoch 17/30\n",
            "21/21 [==============================] - 80s 4s/step - loss: 0.1843 - categorical_accuracy: 0.9349 - val_loss: 0.8319 - val_categorical_accuracy: 0.7200\n",
            "\n",
            "Epoch 00017: saving model to model4_conv3D_2020-08-1608_38_46.350670/model-00017-0.18479-0.93514-0.83191-0.72000.h5\n",
            "Epoch 18/30\n",
            "21/21 [==============================] - 80s 4s/step - loss: 0.1533 - categorical_accuracy: 0.9473 - val_loss: 0.8256 - val_categorical_accuracy: 0.7100\n",
            "\n",
            "Epoch 00018: saving model to model4_conv3D_2020-08-1608_38_46.350670/model-00018-0.15400-0.94721-0.82560-0.71000.h5\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
            "Epoch 19/30\n",
            "21/21 [==============================] - 80s 4s/step - loss: 0.1451 - categorical_accuracy: 0.9592 - val_loss: 0.8148 - val_categorical_accuracy: 0.7100\n",
            "\n",
            "Epoch 00019: saving model to model4_conv3D_2020-08-1608_38_46.350670/model-00019-0.14541-0.95928-0.81476-0.71000.h5\n",
            "Epoch 20/30\n",
            "21/21 [==============================] - 80s 4s/step - loss: 0.1484 - categorical_accuracy: 0.9512 - val_loss: 0.8220 - val_categorical_accuracy: 0.7300\n",
            "\n",
            "Epoch 00020: saving model to model4_conv3D_2020-08-1608_38_46.350670/model-00020-0.14778-0.95173-0.82197-0.73000.h5\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
            "Epoch 21/30\n",
            "21/21 [==============================] - 80s 4s/step - loss: 0.1682 - categorical_accuracy: 0.9462 - val_loss: 0.8214 - val_categorical_accuracy: 0.7200\n",
            "\n",
            "Epoch 00021: saving model to model4_conv3D_2020-08-1608_38_46.350670/model-00021-0.16626-0.94721-0.82143-0.72000.h5\n",
            "Epoch 22/30\n",
            "21/21 [==============================] - 80s 4s/step - loss: 0.1147 - categorical_accuracy: 0.9652 - val_loss: 0.8263 - val_categorical_accuracy: 0.7200\n",
            "\n",
            "Epoch 00022: saving model to model4_conv3D_2020-08-1608_38_46.350670/model-00022-0.11445-0.96531-0.82627-0.72000.h5\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
            "Epoch 23/30\n",
            "21/21 [==============================] - 80s 4s/step - loss: 0.1545 - categorical_accuracy: 0.9486 - val_loss: 0.8155 - val_categorical_accuracy: 0.7200\n",
            "\n",
            "Epoch 00023: saving model to model4_conv3D_2020-08-1608_38_46.350670/model-00023-0.15265-0.95023-0.81550-0.72000.h5\n",
            "Epoch 24/30\n",
            "21/21 [==============================] - 80s 4s/step - loss: 0.1225 - categorical_accuracy: 0.9667 - val_loss: 0.8024 - val_categorical_accuracy: 0.7300\n",
            "\n",
            "Epoch 00024: saving model to model4_conv3D_2020-08-1608_38_46.350670/model-00024-0.12074-0.96682-0.80243-0.73000.h5\n",
            "\n",
            "Epoch 00024: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
            "Epoch 25/30\n",
            "21/21 [==============================] - 79s 4s/step - loss: 0.1358 - categorical_accuracy: 0.9617 - val_loss: 0.7962 - val_categorical_accuracy: 0.7300\n",
            "\n",
            "Epoch 00025: saving model to model4_conv3D_2020-08-1608_38_46.350670/model-00025-0.13579-0.96229-0.79617-0.73000.h5\n",
            "Epoch 26/30\n",
            "21/21 [==============================] - 80s 4s/step - loss: 0.1376 - categorical_accuracy: 0.9503 - val_loss: 0.7926 - val_categorical_accuracy: 0.7400\n",
            "\n",
            "Epoch 00026: saving model to model4_conv3D_2020-08-1608_38_46.350670/model-00026-0.13792-0.95023-0.79256-0.74000.h5\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
            "Epoch 27/30\n",
            "21/21 [==============================] - 81s 4s/step - loss: 0.1140 - categorical_accuracy: 0.9756 - val_loss: 0.7867 - val_categorical_accuracy: 0.7400\n",
            "\n",
            "Epoch 00027: saving model to model4_conv3D_2020-08-1608_38_46.350670/model-00027-0.11418-0.97587-0.78673-0.74000.h5\n",
            "Epoch 28/30\n",
            "21/21 [==============================] - 80s 4s/step - loss: 0.1313 - categorical_accuracy: 0.9628 - val_loss: 0.7855 - val_categorical_accuracy: 0.7400\n",
            "\n",
            "Epoch 00028: saving model to model4_conv3D_2020-08-1608_38_46.350670/model-00028-0.13178-0.96229-0.78551-0.74000.h5\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
            "Epoch 29/30\n",
            "21/21 [==============================] - 80s 4s/step - loss: 0.1147 - categorical_accuracy: 0.9702 - val_loss: 0.7826 - val_categorical_accuracy: 0.7300\n",
            "\n",
            "Epoch 00029: saving model to model4_conv3D_2020-08-1608_38_46.350670/model-00029-0.11512-0.96983-0.78265-0.73000.h5\n",
            "Epoch 30/30\n",
            "21/21 [==============================] - 80s 4s/step - loss: 0.1385 - categorical_accuracy: 0.9587 - val_loss: 0.7807 - val_categorical_accuracy: 0.7300\n",
            "\n",
            "Epoch 00030: saving model to model4_conv3D_2020-08-1608_38_46.350670/model-00030-0.13831-0.95928-0.78074-0.73000.h5\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fec62fcda58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FXothMrzPQD0"
      },
      "source": [
        "**Observation**\n",
        "\n",
        "*   Training Accuracy: 85%\n",
        "*   Validation Accuracy: 74%\n",
        "*   Learning Rate: 0.005\n",
        "*   Filter Size: 3 X 3, 3 X 3\n",
        "*   Batch Size: 32\n",
        "*   Epoch: 30\n",
        "*   Optimizer: Adam\n",
        "*   Augmentation: None\n",
        "*   Trainable Parameters: 1,932,517"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x7msvPRTPnf0"
      },
      "source": [
        "**Inference**\n",
        "\n",
        "So changing filter size has reduced the accuracy. So will stick to the filter size of 5 X 5 and 3 X 3. Let me change the strides and see whether it makes any difference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zdK-99vEQBYV"
      },
      "source": [
        "**Model 6 - No Data Augmentation and Adam optimizer with\n",
        "            Filter Size of (5,5,5) followed by(3,3,3), Image size 120 X 120, Strides (2,2) and Batch Size 32 and Epoch 30**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vrB3r5zFP_5e",
        "colab": {}
      },
      "source": [
        "#write your model here\n",
        "img_height = 120\n",
        "img_width = 120\n",
        "img_filter = [16,32,64,128]\n",
        "img_dense = [256,128]\n",
        "dropout = [0.5,0.25]\n",
        "\n",
        "# Input\n",
        "input_shape=(img_frames,img_height,img_width,img_channel)\n",
        "\n",
        "# Input Layer\n",
        "model = Sequential()\n",
        "model.add(Conv3D(img_filter[0], kernel_size=(5, 5, 5), strides=(1,1,1), padding='same', input_shape=input_shape))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2,2,2)))\n",
        "\n",
        "# Layer 2\n",
        "model.add(Conv3D(img_filter[1], kernel_size=(3, 3, 3), strides=(1,1,1), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "# Layer 3\n",
        "model.add(Conv3D(img_filter[2], kernel_size=(1, 3, 3), strides=(1,1,1), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2,2,2)))\n",
        "\n",
        "# Layer 4\n",
        "model.add(Conv3D(img_filter[3], kernel_size=(1, 3, 3), strides=(1,1,1), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2,2,2)))\n",
        "\n",
        "#Flatten the layers\n",
        "model.add(Flatten())\n",
        "\n",
        "#Fully Connected Layer\n",
        "model.add(Dense(img_dense[0],activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(dropout[0]))\n",
        "\n",
        "model.add(Dense(img_dense[1],activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(dropout[1]))\n",
        "\n",
        "#softmax layer\n",
        "model.add(Dense(nb_classes,activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Xhp7kj0zQ7YA",
        "colab": {},
        "outputId": "0b2385b3-fb69-445e-ba17-857eac7a3696"
      },
      "source": [
        "learning_rate = 0.01\n",
        "optimiser = Adam(learning_rate)\n",
        "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "print (model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv3d_1 (Conv3D)            (None, 30, 120, 120, 16)  6016      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 30, 120, 120, 16)  0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 30, 120, 120, 16)  64        \n",
            "_________________________________________________________________\n",
            "max_pooling3d_1 (MaxPooling3 (None, 15, 60, 60, 16)    0         \n",
            "_________________________________________________________________\n",
            "conv3d_2 (Conv3D)            (None, 15, 60, 60, 32)    13856     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 15, 60, 60, 32)    0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 15, 60, 60, 32)    128       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_2 (MaxPooling3 (None, 7, 30, 30, 32)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_3 (Conv3D)            (None, 7, 30, 30, 64)     18496     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 7, 30, 30, 64)     0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 7, 30, 30, 64)     256       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_3 (MaxPooling3 (None, 3, 15, 15, 64)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_4 (Conv3D)            (None, 3, 15, 15, 128)    73856     \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 3, 15, 15, 128)    0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 3, 15, 15, 128)    512       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_4 (MaxPooling3 (None, 1, 7, 7, 128)      0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               1605888   \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 5)                 645       \n",
            "=================================================================\n",
            "Total params: 1,754,149\n",
            "Trainable params: 1,752,901\n",
            "Non-trainable params: 1,248\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6o7DXOGVRDfO",
        "colab": {}
      },
      "source": [
        "train_generator = generator(train_path, train_doc, batch_size)\n",
        "val_generator = generator(val_path, val_doc, batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hCtbq0BYREKj",
        "colab": {}
      },
      "source": [
        "model_name = 'model5_conv3D' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
        "    \n",
        "if not os.path.exists(model_name):\n",
        "    os.mkdir(model_name)\n",
        "        \n",
        "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
        "\n",
        "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1) # write the REducelronplateau code here\n",
        "callbacks_list = [checkpoint, LR]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GhRaCbLRREqF",
        "colab": {}
      },
      "source": [
        "if (num_train_sequences%batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "if (num_val_sequences%batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences/batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences//batch_size) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xSdhujdpRQrX",
        "colab": {},
        "outputId": "ae3b6f08-5070-469d-8bf6-b1a86f074843"
      },
      "source": [
        "batch_size = 32\n",
        "num_epochs = 10\n",
        "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
        "                    callbacks=callbacks_list, validation_data=val_generator, \n",
        "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source path =  Project_data/val ; batch size = 24\n",
            "Source path =  Project_data/train ; batch size = 24\n",
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:15: DeprecationWarning: `imread` is deprecated!\n",
            "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imread`` instead.\n",
            "  from ipykernel import kernelapp as app\n",
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n",
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:39: DeprecationWarning: `imread` is deprecated!\n",
            "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imread`` instead.\n",
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:44: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "28/28 [==============================] - 193s 7s/step - loss: 1.8853 - categorical_accuracy: 0.3695 - val_loss: 10.2888 - val_categorical_accuracy: 0.2500\n",
            "\n",
            "Epoch 00001: saving model to model5_conv3D_2020-08-1612_43_33.908554/model-00001-1.89107-0.36652-10.28880-0.25000.h5\n",
            "Epoch 2/10\n",
            "28/28 [==============================] - 75s 3s/step - loss: 1.1721 - categorical_accuracy: 0.5286 - val_loss: 4.4683 - val_categorical_accuracy: 0.2800\n",
            "\n",
            "Epoch 00002: saving model to model5_conv3D_2020-08-1612_43_33.908554/model-00002-1.16659-0.52941-4.46833-0.28000.h5\n",
            "Epoch 3/10\n",
            "28/28 [==============================] - 77s 3s/step - loss: 0.8941 - categorical_accuracy: 0.6631 - val_loss: 7.0511 - val_categorical_accuracy: 0.2500\n",
            "\n",
            "Epoch 00003: saving model to model5_conv3D_2020-08-1612_43_33.908554/model-00003-0.89776-0.66214-7.05108-0.25000.h5\n",
            "Epoch 4/10\n",
            "28/28 [==============================] - 79s 3s/step - loss: 0.8662 - categorical_accuracy: 0.6473 - val_loss: 4.1422 - val_categorical_accuracy: 0.2700\n",
            "\n",
            "Epoch 00004: saving model to model5_conv3D_2020-08-1612_43_33.908554/model-00004-0.86763-0.64706-4.14217-0.27000.h5\n",
            "Epoch 5/10\n",
            "28/28 [==============================] - 79s 3s/step - loss: 0.6848 - categorical_accuracy: 0.7098 - val_loss: 1.1375 - val_categorical_accuracy: 0.6100\n",
            "\n",
            "Epoch 00005: saving model to model5_conv3D_2020-08-1612_43_33.908554/model-00005-0.68373-0.71041-1.13750-0.61000.h5\n",
            "Epoch 6/10\n",
            "28/28 [==============================] - 79s 3s/step - loss: 0.6336 - categorical_accuracy: 0.7787 - val_loss: 1.1286 - val_categorical_accuracy: 0.6300\n",
            "\n",
            "Epoch 00006: saving model to model5_conv3D_2020-08-1612_43_33.908554/model-00006-0.62701-0.78281-1.12855-0.63000.h5\n",
            "Epoch 7/10\n",
            "28/28 [==============================] - 78s 3s/step - loss: 0.5608 - categorical_accuracy: 0.7952 - val_loss: 4.0980 - val_categorical_accuracy: 0.3200\n",
            "\n",
            "Epoch 00007: saving model to model5_conv3D_2020-08-1612_43_33.908554/model-00007-0.56328-0.79336-4.09801-0.32000.h5\n",
            "Epoch 8/10\n",
            "28/28 [==============================] - 78s 3s/step - loss: 0.5059 - categorical_accuracy: 0.8056 - val_loss: 1.5898 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00008: saving model to model5_conv3D_2020-08-1612_43_33.908554/model-00008-0.50828-0.80392-1.58984-0.50000.h5\n",
            "\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
            "Epoch 9/10\n",
            "28/28 [==============================] - 76s 3s/step - loss: 0.4203 - categorical_accuracy: 0.8373 - val_loss: 1.4774 - val_categorical_accuracy: 0.5600\n",
            "\n",
            "Epoch 00009: saving model to model5_conv3D_2020-08-1612_43_33.908554/model-00009-0.41162-0.83861-1.47737-0.56000.h5\n",
            "Epoch 10/10\n",
            "28/28 [==============================] - 76s 3s/step - loss: 0.3640 - categorical_accuracy: 0.8485 - val_loss: 1.1274 - val_categorical_accuracy: 0.5800\n",
            "\n",
            "Epoch 00010: saving model to model5_conv3D_2020-08-1612_43_33.908554/model-00010-0.36325-0.84917-1.12741-0.58000.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fcf8f152ef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VXnZ1XoiYFzt"
      },
      "source": [
        "**Model 7 - No Data Augmentation and Adadelta optimizer with\n",
        "            Filter Size of (5,5,5) followed by(3,3,3), Image size 120 X 120 and Batch Size 32 and Epoch 30**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pvlCZeh3N2iB",
        "colab": {},
        "outputId": "083b9194-0e2c-497d-a37b-692022749845"
      },
      "source": [
        "from keras import optimizers\n",
        "import keras\n",
        "\n",
        "learning_rate = 0.001\n",
        "model.compile(optimizer=keras.optimizers.Adadelta(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "print (model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv3d_4 (Conv3D)            (None, 30, 120, 120, 16)  6016      \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 30, 120, 120, 16)  0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 30, 120, 120, 16)  64        \n",
            "_________________________________________________________________\n",
            "max_pooling3d_4 (MaxPooling3 (None, 15, 60, 60, 16)    0         \n",
            "_________________________________________________________________\n",
            "conv3d_5 (Conv3D)            (None, 15, 60, 60, 32)    13856     \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 15, 60, 60, 32)    0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 15, 60, 60, 32)    128       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_5 (MaxPooling3 (None, 7, 30, 30, 32)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_6 (Conv3D)            (None, 7, 30, 30, 64)     18496     \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 7, 30, 30, 64)     0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 7, 30, 30, 64)     256       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_6 (MaxPooling3 (None, 3, 15, 15, 64)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_7 (Conv3D)            (None, 3, 15, 15, 128)    73856     \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 3, 15, 15, 128)    0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 3, 15, 15, 128)    512       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_7 (MaxPooling3 (None, 1, 7, 7, 128)      0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               1605888   \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 5)                 645       \n",
            "=================================================================\n",
            "Total params: 1,754,149\n",
            "Trainable params: 1,752,901\n",
            "Non-trainable params: 1,248\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zlajZTEhT3_r",
        "colab": {}
      },
      "source": [
        "train_generator = generator(train_path, train_doc, batch_size)\n",
        "val_generator = generator(val_path, val_doc, batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZZkN9JicT5U0",
        "colab": {}
      },
      "source": [
        "model_name = 'model6_conv3d' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
        "    \n",
        "if not os.path.exists(model_name):\n",
        "    os.mkdir(model_name)\n",
        "        \n",
        "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
        "\n",
        "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1) # write the REducelronplateau code here\n",
        "callbacks_list = [checkpoint, LR]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KE8xR5i0UFnQ",
        "colab": {}
      },
      "source": [
        "if (num_train_sequences%batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "if (num_val_sequences%batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences/batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences//batch_size) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZakV8XsUUOCV",
        "colab": {},
        "outputId": "a2ae4d80-26c0-45bc-d00e-29ffd0a368b9"
      },
      "source": [
        "batch_size = 32\n",
        "num_epochs = 30\n",
        "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
        "                    callbacks=callbacks_list, validation_data=val_generator, \n",
        "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source path =  Project_data/val ; batch size = 24\n",
            "Source path =  Project_data/trainEpoch 1/30\n",
            " ; batch size = 24\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:15: DeprecationWarning: `imread` is deprecated!\n",
            "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imread`` instead.\n",
            "  from ipykernel import kernelapp as app\n",
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n",
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:39: DeprecationWarning: `imread` is deprecated!\n",
            "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imread`` instead.\n",
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:44: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "28/28 [==============================] - 194s 7s/step - loss: 1.6129 - categorical_accuracy: 0.4065 - val_loss: 1.1122 - val_categorical_accuracy: 0.5900\n",
            "\n",
            "Epoch 00001: saving model to model6_conv3d_2020-08-1613_15_57.168437/model-00001-1.61545-0.40573-1.11225-0.59000.h5\n",
            "Epoch 2/30\n",
            "28/28 [==============================] - 76s 3s/step - loss: 1.1210 - categorical_accuracy: 0.5686 - val_loss: 1.2539 - val_categorical_accuracy: 0.5400\n",
            "\n",
            "Epoch 00002: saving model to model6_conv3d_2020-08-1613_15_57.168437/model-00002-1.12910-0.56561-1.25388-0.54000.h5\n",
            "Epoch 3/30\n",
            "28/28 [==============================] - 80s 3s/step - loss: 0.8876 - categorical_accuracy: 0.6367 - val_loss: 1.4679 - val_categorical_accuracy: 0.4200\n",
            "\n",
            "Epoch 00003: saving model to model6_conv3d_2020-08-1613_15_57.168437/model-00003-0.88892-0.63801-1.46790-0.42000.h5\n",
            "\n",
            "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.5.\n",
            "Epoch 4/30\n",
            "28/28 [==============================] - 80s 3s/step - loss: 0.7556 - categorical_accuracy: 0.7220 - val_loss: 1.4571 - val_categorical_accuracy: 0.5300\n",
            "\n",
            "Epoch 00004: saving model to model6_conv3d_2020-08-1613_15_57.168437/model-00004-0.75530-0.72097-1.45706-0.53000.h5\n",
            "Epoch 5/30\n",
            "28/28 [==============================] - 80s 3s/step - loss: 0.5304 - categorical_accuracy: 0.7983 - val_loss: 0.8766 - val_categorical_accuracy: 0.6900\n",
            "\n",
            "Epoch 00005: saving model to model6_conv3d_2020-08-1613_15_57.168437/model-00005-0.52820-0.80090-0.87658-0.69000.h5\n",
            "Epoch 6/30\n",
            "28/28 [==============================] - 81s 3s/step - loss: 0.4992 - categorical_accuracy: 0.8117 - val_loss: 1.0433 - val_categorical_accuracy: 0.6000\n",
            "\n",
            "Epoch 00006: saving model to model6_conv3d_2020-08-1613_15_57.168437/model-00006-0.49566-0.81448-1.04333-0.60000.h5\n",
            "Epoch 7/30\n",
            "28/28 [==============================] - 80s 3s/step - loss: 0.4009 - categorical_accuracy: 0.8630 - val_loss: 1.0876 - val_categorical_accuracy: 0.6100\n",
            "\n",
            "Epoch 00007: saving model to model6_conv3d_2020-08-1613_15_57.168437/model-00007-0.40362-0.86124-1.08756-0.61000.h5\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.25.\n",
            "Epoch 8/30\n",
            "28/28 [==============================] - 80s 3s/step - loss: 0.2917 - categorical_accuracy: 0.8979 - val_loss: 1.0532 - val_categorical_accuracy: 0.6100\n",
            "\n",
            "Epoch 00008: saving model to model6_conv3d_2020-08-1613_15_57.168437/model-00008-0.29317-0.89744-1.05322-0.61000.h5\n",
            "Epoch 9/30\n",
            "28/28 [==============================] - 81s 3s/step - loss: 0.2404 - categorical_accuracy: 0.9289 - val_loss: 1.6946 - val_categorical_accuracy: 0.4700\n",
            "\n",
            "Epoch 00009: saving model to model6_conv3d_2020-08-1613_15_57.168437/model-00009-0.23771-0.93062-1.69462-0.47000.h5\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.125.\n",
            "Epoch 10/30\n",
            "28/28 [==============================] - 80s 3s/step - loss: 0.2280 - categorical_accuracy: 0.9238 - val_loss: 0.8836 - val_categorical_accuracy: 0.7100\n",
            "\n",
            "Epoch 00010: saving model to model6_conv3d_2020-08-1613_15_57.168437/model-00010-0.22551-0.92459-0.88364-0.71000.h5\n",
            "Epoch 11/30\n",
            "28/28 [==============================] - 80s 3s/step - loss: 0.1641 - categorical_accuracy: 0.9515 - val_loss: 0.9047 - val_categorical_accuracy: 0.7200\n",
            "\n",
            "Epoch 00011: saving model to model6_conv3d_2020-08-1613_15_57.168437/model-00011-0.16257-0.95173-0.90472-0.72000.h5\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0625.\n",
            "Epoch 12/30\n",
            "28/28 [==============================] - 80s 3s/step - loss: 0.1674 - categorical_accuracy: 0.9553 - val_loss: 0.7881 - val_categorical_accuracy: 0.7300\n",
            "\n",
            "Epoch 00012: saving model to model6_conv3d_2020-08-1613_15_57.168437/model-00012-0.16805-0.95475-0.78808-0.73000.h5\n",
            "Epoch 13/30\n",
            "28/28 [==============================] - 80s 3s/step - loss: 0.1488 - categorical_accuracy: 0.9536 - val_loss: 0.8079 - val_categorical_accuracy: 0.7400\n",
            "\n",
            "Epoch 00013: saving model to model6_conv3d_2020-08-1613_15_57.168437/model-00013-0.14774-0.95475-0.80794-0.74000.h5\n",
            "Epoch 14/30\n",
            "28/28 [==============================] - 80s 3s/step - loss: 0.1436 - categorical_accuracy: 0.9560 - val_loss: 0.6894 - val_categorical_accuracy: 0.7600\n",
            "\n",
            "Epoch 00014: saving model to model6_conv3d_2020-08-1613_15_57.168437/model-00014-0.14263-0.95626-0.68943-0.76000.h5\n",
            "Epoch 15/30\n",
            "28/28 [==============================] - 80s 3s/step - loss: 0.1585 - categorical_accuracy: 0.9553 - val_loss: 0.7512 - val_categorical_accuracy: 0.7600\n",
            "\n",
            "Epoch 00015: saving model to model6_conv3d_2020-08-1613_15_57.168437/model-00015-0.15947-0.95475-0.75121-0.76000.h5\n",
            "Epoch 16/30\n",
            "28/28 [==============================] - 79s 3s/step - loss: 0.1357 - categorical_accuracy: 0.9611 - val_loss: 0.7152 - val_categorical_accuracy: 0.7900\n",
            "\n",
            "Epoch 00016: saving model to model6_conv3d_2020-08-1613_15_57.168437/model-00016-0.13338-0.96229-0.71516-0.79000.h5\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.03125.\n",
            "Epoch 17/30\n",
            "28/28 [==============================] - 79s 3s/step - loss: 0.1210 - categorical_accuracy: 0.9685 - val_loss: 0.6796 - val_categorical_accuracy: 0.7900\n",
            "\n",
            "Epoch 00017: saving model to model6_conv3d_2020-08-1613_15_57.168437/model-00017-0.12004-0.96983-0.67958-0.79000.h5\n",
            "Epoch 18/30\n",
            "28/28 [==============================] - 79s 3s/step - loss: 0.0875 - categorical_accuracy: 0.9806 - val_loss: 0.6489 - val_categorical_accuracy: 0.8000\n",
            "\n",
            "Epoch 00018: saving model to model6_conv3d_2020-08-1613_15_57.168437/model-00018-0.08797-0.98039-0.64890-0.80000.h5\n",
            "Epoch 19/30\n",
            "28/28 [==============================] - 80s 3s/step - loss: 0.1045 - categorical_accuracy: 0.9777 - val_loss: 0.6735 - val_categorical_accuracy: 0.7900\n",
            "\n",
            "Epoch 00019: saving model to model6_conv3d_2020-08-1613_15_57.168437/model-00019-0.10393-0.97738-0.67352-0.79000.h5\n",
            "Epoch 20/30\n",
            "28/28 [==============================] - 80s 3s/step - loss: 0.1127 - categorical_accuracy: 0.9717 - val_loss: 0.6638 - val_categorical_accuracy: 0.8200\n",
            "\n",
            "Epoch 00020: saving model to model6_conv3d_2020-08-1613_15_57.168437/model-00020-0.11269-0.97134-0.66385-0.82000.h5\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.015625.\n",
            "Epoch 21/30\n",
            "28/28 [==============================] - 80s 3s/step - loss: 0.1114 - categorical_accuracy: 0.9694 - val_loss: 0.6310 - val_categorical_accuracy: 0.7900\n",
            "\n",
            "Epoch 00021: saving model to model6_conv3d_2020-08-1613_15_57.168437/model-00021-0.11143-0.96983-0.63101-0.79000.h5\n",
            "Epoch 22/30\n",
            "28/28 [==============================] - 80s 3s/step - loss: 0.1140 - categorical_accuracy: 0.9687 - val_loss: 0.6705 - val_categorical_accuracy: 0.7900\n",
            "\n",
            "Epoch 00022: saving model to model6_conv3d_2020-08-1613_15_57.168437/model-00022-0.11465-0.96833-0.67050-0.79000.h5\n",
            "Epoch 23/30\n",
            "28/28 [==============================] - 80s 3s/step - loss: 0.1159 - categorical_accuracy: 0.9668 - val_loss: 0.6690 - val_categorical_accuracy: 0.8000\n",
            "\n",
            "Epoch 00023: saving model to model6_conv3d_2020-08-1613_15_57.168437/model-00023-0.11032-0.96983-0.66902-0.80000.h5\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0078125.\n",
            "Epoch 24/30\n",
            "28/28 [==============================] - 80s 3s/step - loss: 0.0963 - categorical_accuracy: 0.9806 - val_loss: 0.6298 - val_categorical_accuracy: 0.8100\n",
            "\n",
            "Epoch 00024: saving model to model6_conv3d_2020-08-1613_15_57.168437/model-00024-0.09707-0.98039-0.62975-0.81000.h5\n",
            "Epoch 25/30\n",
            "28/28 [==============================] - 80s 3s/step - loss: 0.0940 - categorical_accuracy: 0.9732 - val_loss: 0.6297 - val_categorical_accuracy: 0.8200\n",
            "\n",
            "Epoch 00025: saving model to model6_conv3d_2020-08-1613_15_57.168437/model-00025-0.09422-0.97285-0.62965-0.82000.h5\n",
            "Epoch 26/30\n",
            "28/28 [==============================] - 79s 3s/step - loss: 0.1108 - categorical_accuracy: 0.9670 - val_loss: 0.6375 - val_categorical_accuracy: 0.8200\n",
            "\n",
            "Epoch 00026: saving model to model6_conv3d_2020-08-1613_15_57.168437/model-00026-0.10957-0.96833-0.63754-0.82000.h5\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.00390625.\n",
            "Epoch 27/30\n",
            "28/28 [==============================] - 80s 3s/step - loss: 0.1552 - categorical_accuracy: 0.9525 - val_loss: 0.6177 - val_categorical_accuracy: 0.8200\n",
            "\n",
            "Epoch 00027: saving model to model6_conv3d_2020-08-1613_15_57.168437/model-00027-0.14647-0.95626-0.61767-0.82000.h5\n",
            "Epoch 28/30\n",
            "28/28 [==============================] - 80s 3s/step - loss: 0.0797 - categorical_accuracy: 0.9836 - val_loss: 0.6198 - val_categorical_accuracy: 0.8100\n",
            "\n",
            "Epoch 00028: saving model to model6_conv3d_2020-08-1613_15_57.168437/model-00028-0.07999-0.98341-0.61984-0.81000.h5\n",
            "Epoch 29/30\n",
            "28/28 [==============================] - 80s 3s/step - loss: 0.0956 - categorical_accuracy: 0.9738 - val_loss: 0.6114 - val_categorical_accuracy: 0.8100\n",
            "\n",
            "Epoch 00029: saving model to model6_conv3d_2020-08-1613_15_57.168437/model-00029-0.09565-0.97436-0.61135-0.81000.h5\n",
            "Epoch 30/30\n",
            "28/28 [==============================] - 79s 3s/step - loss: 0.1057 - categorical_accuracy: 0.9747 - val_loss: 0.6146 - val_categorical_accuracy: 0.8100\n",
            "\n",
            "Epoch 00030: saving model to model6_conv3d_2020-08-1613_15_57.168437/model-00030-0.10567-0.97436-0.61462-0.81000.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f295d8c55f8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bc6PMtJxnAGX"
      },
      "source": [
        "**Observation**\n",
        "\n",
        "*   Training Accuracy: 95%\n",
        "*   Validation Accuracy: 82%\n",
        "*   Learning Rate: 0.008\n",
        "*   Filter Size: 5 X 5, 3 X 3\n",
        "*   Batch Size: 24\n",
        "*   Epoch: 30\n",
        "*   Optimizer: Adadelta\n",
        "*   Augmentation: None\n",
        "*   Trainable Parameters: 1,752,901"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lbvVNA4REHYz"
      },
      "source": [
        "**Inference**\n",
        "\n",
        "Accuracy was much higher when we used Adam optimizer. Also the learning rate has increased with Adadelta. So from henceforth we will be using the Adam optimizer for the rest of the models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cGyin5QSne6G"
      },
      "source": [
        "**Model 8 - Data Augmentation and Adam optimizer having image size 120 X 120 with Batch Size 32 and Epoch 10**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jKbYYLeHm1r4",
        "colab": {}
      },
      "source": [
        "#write your model here\n",
        "img_height = 120\n",
        "img_width = 120\n",
        "img_filter = [16,32,64,128]\n",
        "img_dense = [256,128]\n",
        "dropout = [0.5,0.25]\n",
        "\n",
        "# Input\n",
        "input_shape=(img_frames,img_height,img_width,img_channel)\n",
        "\n",
        "# Input Layer\n",
        "model = Sequential()\n",
        "model.add(Conv3D(img_filter[0], kernel_size=(5, 5, 5), padding='same', input_shape=input_shape))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "# Layer 2\n",
        "model.add(Conv3D(img_filter[1], kernel_size=(3, 3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "# Layer 3\n",
        "model.add(Conv3D(img_filter[2], kernel_size=(1, 3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "# Layer 4\n",
        "model.add(Conv3D(img_filter[3], kernel_size=(1, 3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "#Flatten the layers\n",
        "model.add(Flatten())\n",
        "\n",
        "#Fully Connected Layer\n",
        "model.add(Dense(img_dense[0],activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(dropout[0]))\n",
        "\n",
        "model.add(Dense(img_dense[1],activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(dropout[1]))\n",
        "\n",
        "#softmax layer\n",
        "model.add(Dense(nb_classes,activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-XM0lOH9oj2d",
        "colab": {},
        "outputId": "39df106a-9878-4b8a-84b9-118ae83a33f3"
      },
      "source": [
        "learning_rate = 0.001\n",
        "optimiser = Adam(learning_rate)\n",
        "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "print (model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv3d_9 (Conv3D)            (None, 30, 120, 120, 16)  6016      \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 30, 120, 120, 16)  0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 30, 120, 120, 16)  64        \n",
            "_________________________________________________________________\n",
            "max_pooling3d_9 (MaxPooling3 (None, 15, 60, 60, 16)    0         \n",
            "_________________________________________________________________\n",
            "conv3d_10 (Conv3D)           (None, 15, 60, 60, 32)    13856     \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 15, 60, 60, 32)    0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 15, 60, 60, 32)    128       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_10 (MaxPooling (None, 7, 30, 30, 32)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_11 (Conv3D)           (None, 7, 30, 30, 64)     18496     \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 7, 30, 30, 64)     0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 7, 30, 30, 64)     256       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_11 (MaxPooling (None, 3, 15, 15, 64)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_12 (Conv3D)           (None, 3, 15, 15, 128)    73856     \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 3, 15, 15, 128)    0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 3, 15, 15, 128)    512       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_12 (MaxPooling (None, 1, 7, 7, 128)      0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 256)               1605888   \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 5)                 645       \n",
            "=================================================================\n",
            "Total params: 1,754,149\n",
            "Trainable params: 1,752,901\n",
            "Non-trainable params: 1,248\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "waJobsSIoyTC"
      },
      "source": [
        "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IvbgukEFoztr",
        "colab": {}
      },
      "source": [
        "train_generator = augmented_generator(train_path, train_doc, batch_size)\n",
        "val_generator = augmented_generator(val_path, val_doc, batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "T5HUmLPNo-Jn",
        "colab": {}
      },
      "source": [
        "model_name = 'model8_conv3d' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
        "    \n",
        "if not os.path.exists(model_name):\n",
        "    os.mkdir(model_name)\n",
        "        \n",
        "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
        "\n",
        "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1) # write the REducelronplateau code here\n",
        "callbacks_list = [checkpoint, LR]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "j2JohHv_pAAF",
        "colab": {}
      },
      "source": [
        "if (num_train_sequences%batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "if (num_val_sequences%batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences/batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences//batch_size) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B1t8MTkepE5D",
        "colab": {},
        "outputId": "a95e309c-687e-4fab-cfdf-2c9363d75980"
      },
      "source": [
        "batch_size = 32\n",
        "num_epochs = 10\n",
        "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
        "                    callbacks=callbacks_list, validation_data=val_generator, \n",
        "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source path =  Project_data/val ; batch size = 32\n",
            "Source path =  Project_data/train ; batch size = 32\n",
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: DeprecationWarning: `imread` is deprecated!\n",
            "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imread`` instead.\n",
            "  app.launch_new_instance()\n",
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:34: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n",
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:57: DeprecationWarning: `imread` is deprecated!\n",
            "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imread`` instead.\n",
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:75: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21/21 [==============================] - 212s 10s/step - loss: 1.8413 - categorical_accuracy: 0.3479 - val_loss: 3.0295 - val_categorical_accuracy: 0.2800\n",
            "\n",
            "Epoch 00001: saving model to model8_conv3d_2020-08-1616_56_15.829707/model-00001-1.84137-0.34842-3.02946-0.28000.h5\n",
            "Epoch 2/10\n",
            "21/21 [==============================] - 86s 4s/step - loss: 1.3918 - categorical_accuracy: 0.4582 - val_loss: 3.5258 - val_categorical_accuracy: 0.2900\n",
            "\n",
            "Epoch 00002: saving model to model8_conv3d_2020-08-1616_56_15.829707/model-00002-1.39263-0.45852-3.52581-0.29000.h5\n",
            "Epoch 3/10\n",
            "21/21 [==============================] - 90s 4s/step - loss: 1.2381 - categorical_accuracy: 0.5343 - val_loss: 2.0619 - val_categorical_accuracy: 0.4400\n",
            "\n",
            "Epoch 00003: saving model to model8_conv3d_2020-08-1616_56_15.829707/model-00003-1.24554-0.52941-2.06190-0.44000.h5\n",
            "Epoch 4/10\n",
            "21/21 [==============================] - 89s 4s/step - loss: 1.0863 - categorical_accuracy: 0.5799 - val_loss: 1.4501 - val_categorical_accuracy: 0.5300\n",
            "\n",
            "Epoch 00004: saving model to model8_conv3d_2020-08-1616_56_15.829707/model-00004-1.08367-0.58069-1.45012-0.53000.h5\n",
            "Epoch 5/10\n",
            "21/21 [==============================] - 91s 4s/step - loss: 0.9403 - categorical_accuracy: 0.6328 - val_loss: 1.1415 - val_categorical_accuracy: 0.6000\n",
            "\n",
            "Epoch 00005: saving model to model8_conv3d_2020-08-1616_56_15.829707/model-00005-0.94162-0.63198-1.14150-0.60000.h5\n",
            "Epoch 6/10\n",
            "21/21 [==============================] - 92s 4s/step - loss: 0.8264 - categorical_accuracy: 0.6723 - val_loss: 1.5625 - val_categorical_accuracy: 0.5400\n",
            "\n",
            "Epoch 00006: saving model to model8_conv3d_2020-08-1616_56_15.829707/model-00006-0.82462-0.67421-1.56246-0.54000.h5\n",
            "Epoch 7/10\n",
            "21/21 [==============================] - 92s 4s/step - loss: 0.8818 - categorical_accuracy: 0.6590 - val_loss: 0.9242 - val_categorical_accuracy: 0.6300\n",
            "\n",
            "Epoch 00007: saving model to model8_conv3d_2020-08-1616_56_15.829707/model-00007-0.88268-0.65913-0.92420-0.63000.h5\n",
            "Epoch 8/10\n",
            "21/21 [==============================] - 91s 4s/step - loss: 0.7773 - categorical_accuracy: 0.7004 - val_loss: 1.2218 - val_categorical_accuracy: 0.5900\n",
            "\n",
            "Epoch 00008: saving model to model8_conv3d_2020-08-1616_56_15.829707/model-00008-0.77897-0.69985-1.22179-0.59000.h5\n",
            "Epoch 9/10\n",
            "21/21 [==============================] - 91s 4s/step - loss: 0.6955 - categorical_accuracy: 0.7474 - val_loss: 0.9187 - val_categorical_accuracy: 0.6800\n",
            "\n",
            "Epoch 00009: saving model to model8_conv3d_2020-08-1616_56_15.829707/model-00009-0.69337-0.74811-0.91866-0.68000.h5\n",
            "Epoch 10/10\n",
            "21/21 [==============================] - 91s 4s/step - loss: 0.6653 - categorical_accuracy: 0.7432 - val_loss: 1.2383 - val_categorical_accuracy: 0.5700\n",
            "\n",
            "Epoch 00010: saving model to model8_conv3d_2020-08-1616_56_15.829707/model-00010-0.66530-0.74208-1.23832-0.57000.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb718621e80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CWdGDF1-FFml"
      },
      "source": [
        "**Observation**\n",
        "\n",
        "*   Training Accuracy: 74.7%\n",
        "*   Validation Accuracy: 68%\n",
        "*   Learning Rate: 0.001\n",
        "*   Filter Size: 5 X 5, 3 X 3\n",
        "*   Batch Size: 24\n",
        "*   Epoch: 10\n",
        "*   Optimizer: Adam\n",
        "*   Augmentation: Yes\n",
        "*   Trainable Parameters: 1,752,901"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HTgHJpa1xCMf"
      },
      "source": [
        "**Model 9 - Data Augmentation and Adam optimizer having image size 100 X 100 with Batch Size 32 and Epoch 10**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rzEI3DOEwztu",
        "colab": {}
      },
      "source": [
        "#write your model here\n",
        "img_height = 100\n",
        "img_width = 100\n",
        "img_filter = [16,32,64,128]\n",
        "img_dense = [256,128]\n",
        "dropout = [0.5,0.25]\n",
        "\n",
        "# Input\n",
        "input_shape=(img_frames,img_height,img_width,img_channel)\n",
        "\n",
        "# Input Layer\n",
        "model = Sequential()\n",
        "model.add(Conv3D(img_filter[0], kernel_size=(5, 5, 5), padding='same', input_shape=input_shape))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "# Layer 2\n",
        "model.add(Conv3D(img_filter[1], kernel_size=(3, 3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "# Layer 3\n",
        "model.add(Conv3D(img_filter[2], kernel_size=(1, 3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "# Layer 4\n",
        "model.add(Conv3D(img_filter[3], kernel_size=(1, 3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "#Flatten the layers\n",
        "model.add(Flatten())\n",
        "\n",
        "#Fully Connected Layer\n",
        "model.add(Dense(img_dense[0],activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(dropout[0]))\n",
        "\n",
        "model.add(Dense(img_dense[1],activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(dropout[1]))\n",
        "\n",
        "#softmax layer\n",
        "model.add(Dense(nb_classes,activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ov2r_U6LyGyN",
        "colab": {},
        "outputId": "4fb5a613-31b7-49ad-f2b5-3ca9d0d2989b"
      },
      "source": [
        "learning_rate = 0.001\n",
        "optimiser = Adam(learning_rate)\n",
        "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "print (model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv3d_13 (Conv3D)           (None, 30, 100, 100, 16)  6016      \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 30, 100, 100, 16)  0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_19 (Batc (None, 30, 100, 100, 16)  64        \n",
            "_________________________________________________________________\n",
            "max_pooling3d_13 (MaxPooling (None, 15, 50, 50, 16)    0         \n",
            "_________________________________________________________________\n",
            "conv3d_14 (Conv3D)           (None, 15, 50, 50, 32)    13856     \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 15, 50, 50, 32)    0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_20 (Batc (None, 15, 50, 50, 32)    128       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_14 (MaxPooling (None, 7, 25, 25, 32)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_15 (Conv3D)           (None, 7, 25, 25, 64)     18496     \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 7, 25, 25, 64)     0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_21 (Batc (None, 7, 25, 25, 64)     256       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_15 (MaxPooling (None, 3, 12, 12, 64)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_16 (Conv3D)           (None, 3, 12, 12, 128)    73856     \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 3, 12, 12, 128)    0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_22 (Batc (None, 3, 12, 12, 128)    512       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_16 (MaxPooling (None, 1, 6, 6, 128)      0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 256)               1179904   \n",
            "_________________________________________________________________\n",
            "batch_normalization_23 (Batc (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "batch_normalization_24 (Batc (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 5)                 645       \n",
            "=================================================================\n",
            "Total params: 1,328,165\n",
            "Trainable params: 1,326,917\n",
            "Non-trainable params: 1,248\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rQG4EJpyyHyj",
        "colab": {}
      },
      "source": [
        "train_generator = augmented_generator(train_path, train_doc, batch_size)\n",
        "val_generator = augmented_generator(val_path, val_doc, batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Tj2Y_5d6yNIf",
        "colab": {}
      },
      "source": [
        "model_name = 'model9_conv3D' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
        "    \n",
        "if not os.path.exists(model_name):\n",
        "    os.mkdir(model_name)\n",
        "        \n",
        "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
        "\n",
        "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1) # write the REducelronplateau code here\n",
        "callbacks_list = [checkpoint, LR]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "41eU9U1LyS6r",
        "colab": {}
      },
      "source": [
        "if (num_train_sequences%batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "if (num_val_sequences%batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences/batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences//batch_size) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0yZXUN_IyYcE",
        "colab": {},
        "outputId": "0ad49ba3-0b30-42cd-c56a-e541f4e3df4f"
      },
      "source": [
        "batch_size = 32\n",
        "num_epochs = 10\n",
        "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
        "                    callbacks=callbacks_list, validation_data=val_generator, \n",
        "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source path =  Project_data/val ; batch size = 32\n",
            "Source path =  Project_data/train ; batch size = 32\n",
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: DeprecationWarning: `imread` is deprecated!\n",
            "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imread`` instead.\n",
            "  app.launch_new_instance()\n",
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:34: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " 2/21 [=>............................] - ETA: 2:19 - loss: 2.1513 - categorical_accuracy: 0.2969"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:57: DeprecationWarning: `imread` is deprecated!\n",
            "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imread`` instead.\n",
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:75: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21/21 [==============================] - 101s 5s/step - loss: 1.9180 - categorical_accuracy: 0.3555 - val_loss: 3.5765 - val_categorical_accuracy: 0.2800\n",
            "\n",
            "Epoch 00001: saving model to model9_conv3D_2020-08-1616_56_15.829707/model-00001-1.92350-0.35445-3.57652-0.28000.h5\n",
            "Epoch 2/10\n",
            "21/21 [==============================] - 83s 4s/step - loss: 1.3725 - categorical_accuracy: 0.4582 - val_loss: 3.3277 - val_categorical_accuracy: 0.2500\n",
            "\n",
            "Epoch 00002: saving model to model9_conv3D_2020-08-1616_56_15.829707/model-00002-1.37615-0.45852-3.32771-0.25000.h5\n",
            "Epoch 3/10\n",
            "21/21 [==============================] - 88s 4s/step - loss: 1.2613 - categorical_accuracy: 0.5109 - val_loss: 1.8199 - val_categorical_accuracy: 0.4500\n",
            "\n",
            "Epoch 00003: saving model to model9_conv3D_2020-08-1616_56_15.829707/model-00003-1.25664-0.51131-1.81986-0.45000.h5\n",
            "Epoch 4/10\n",
            "21/21 [==============================] - 89s 4s/step - loss: 1.2030 - categorical_accuracy: 0.5554 - val_loss: 2.1319 - val_categorical_accuracy: 0.4200\n",
            "\n",
            "Epoch 00004: saving model to model9_conv3D_2020-08-1616_56_15.829707/model-00004-1.20920-0.55354-2.13188-0.42000.h5\n",
            "Epoch 5/10\n",
            "21/21 [==============================] - 88s 4s/step - loss: 1.0402 - categorical_accuracy: 0.5920 - val_loss: 2.6263 - val_categorical_accuracy: 0.3300\n",
            "\n",
            "Epoch 00005: saving model to model9_conv3D_2020-08-1616_56_15.829707/model-00005-1.04152-0.59125-2.62634-0.33000.h5\n",
            "\n",
            "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "Epoch 6/10\n",
            "21/21 [==============================] - 87s 4s/step - loss: 0.9706 - categorical_accuracy: 0.6242 - val_loss: 1.3770 - val_categorical_accuracy: 0.4500\n",
            "\n",
            "Epoch 00006: saving model to model9_conv3D_2020-08-1616_56_15.829707/model-00006-0.96850-0.62443-1.37697-0.45000.h5\n",
            "Epoch 7/10\n",
            "21/21 [==============================] - 88s 4s/step - loss: 0.9262 - categorical_accuracy: 0.6585 - val_loss: 1.1493 - val_categorical_accuracy: 0.5400\n",
            "\n",
            "Epoch 00007: saving model to model9_conv3D_2020-08-1616_56_15.829707/model-00007-0.92505-0.65913-1.14925-0.54000.h5\n",
            "Epoch 8/10\n",
            "21/21 [==============================] - 87s 4s/step - loss: 0.8532 - categorical_accuracy: 0.6639 - val_loss: 1.0070 - val_categorical_accuracy: 0.5900\n",
            "\n",
            "Epoch 00008: saving model to model9_conv3D_2020-08-1616_56_15.829707/model-00008-0.84943-0.66516-1.00700-0.59000.h5\n",
            "Epoch 9/10\n",
            "21/21 [==============================] - 87s 4s/step - loss: 0.7650 - categorical_accuracy: 0.7018 - val_loss: 1.7749 - val_categorical_accuracy: 0.4000\n",
            "\n",
            "Epoch 00009: saving model to model9_conv3D_2020-08-1616_56_15.829707/model-00009-0.76726-0.70136-1.77485-0.40000.h5\n",
            "Epoch 10/10\n",
            "21/21 [==============================] - 87s 4s/step - loss: 0.7450 - categorical_accuracy: 0.7138 - val_loss: 1.6821 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00010: saving model to model9_conv3D_2020-08-1616_56_15.829707/model-00010-0.74515-0.71342-1.68209-0.50000.h5\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb67c2067f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ci8m8ZPzFiVC"
      },
      "source": [
        "**Observation**\n",
        "\n",
        "*   Training Accuracy: 66%\n",
        "*   Validation Accuracy: 59%\n",
        "*   Learning Rate: 0.0005\n",
        "*   Filter Size: 5 X 5, 3 X 3\n",
        "*   Batch Size: 32\n",
        "*   Epoch: 10\n",
        "*   Optimizer: Adam\n",
        "*   Augmentation: Yes\n",
        "*   Trainable Parameters: 1,326,917"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fCSGmPdWGakz"
      },
      "source": [
        "**Inference**\n",
        "\n",
        "Both training aand validation acuracy has significantly reduced on reducing the image size from 120 X 120 to 100 X 100. Also the learning rate has become half on reducing the image size. We will be continuing our further experimentation on a image of size 120 X 120."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dVD6l5FtKZqg"
      },
      "source": [
        "**Model 10 - Data Augmentation and Adam optimizer having image size 120 X 120 with Batch Size 32 and Epoch 30**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NWU-GjmUJFUQ",
        "colab": {}
      },
      "source": [
        "#write your model here\n",
        "img_height = 120\n",
        "img_width = 120\n",
        "img_filter = [16,32,64,128]\n",
        "img_dense = [256,128]\n",
        "dropout = [0.5,0.25]\n",
        "\n",
        "# Input\n",
        "input_shape=(img_frames,img_height,img_width,img_channel)\n",
        "\n",
        "# Input Layer\n",
        "model = Sequential()\n",
        "model.add(Conv3D(img_filter[0], kernel_size=(5, 5, 5), padding='same', input_shape=input_shape))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "# Layer 2\n",
        "model.add(Conv3D(img_filter[1], kernel_size=(3, 3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "# Layer 3\n",
        "model.add(Conv3D(img_filter[2], kernel_size=(1, 3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "# Layer 4\n",
        "model.add(Conv3D(img_filter[3], kernel_size=(1, 3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "#Flatten the layers\n",
        "model.add(Flatten())\n",
        "\n",
        "#Fully Connected Layer\n",
        "model.add(Dense(img_dense[0],activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(dropout[0]))\n",
        "\n",
        "model.add(Dense(img_dense[1],activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(dropout[1]))\n",
        "\n",
        "#softmax layer\n",
        "model.add(Dense(nb_classes,activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eYqhIPOr9CHt",
        "colab": {},
        "outputId": "cee2a054-9fb0-421a-9076-ea2ab431663b"
      },
      "source": [
        "learning_rate = 0.001\n",
        "optimiser = Adam(learning_rate)\n",
        "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "print (model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv3d_17 (Conv3D)           (None, 30, 120, 120, 16)  6016      \n",
            "_________________________________________________________________\n",
            "activation_17 (Activation)   (None, 30, 120, 120, 16)  0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_25 (Batc (None, 30, 120, 120, 16)  64        \n",
            "_________________________________________________________________\n",
            "max_pooling3d_17 (MaxPooling (None, 15, 60, 60, 16)    0         \n",
            "_________________________________________________________________\n",
            "conv3d_18 (Conv3D)           (None, 15, 60, 60, 32)    13856     \n",
            "_________________________________________________________________\n",
            "activation_18 (Activation)   (None, 15, 60, 60, 32)    0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_26 (Batc (None, 15, 60, 60, 32)    128       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_18 (MaxPooling (None, 7, 30, 30, 32)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_19 (Conv3D)           (None, 7, 30, 30, 64)     18496     \n",
            "_________________________________________________________________\n",
            "activation_19 (Activation)   (None, 7, 30, 30, 64)     0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_27 (Batc (None, 7, 30, 30, 64)     256       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_19 (MaxPooling (None, 3, 15, 15, 64)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_20 (Conv3D)           (None, 3, 15, 15, 128)    73856     \n",
            "_________________________________________________________________\n",
            "activation_20 (Activation)   (None, 3, 15, 15, 128)    0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_28 (Batc (None, 3, 15, 15, 128)    512       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_20 (MaxPooling (None, 1, 7, 7, 128)      0         \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 256)               1605888   \n",
            "_________________________________________________________________\n",
            "batch_normalization_29 (Batc (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "batch_normalization_30 (Batc (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 5)                 645       \n",
            "=================================================================\n",
            "Total params: 1,754,149\n",
            "Trainable params: 1,752,901\n",
            "Non-trainable params: 1,248\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n6Kut8lI9CHy",
        "colab": {}
      },
      "source": [
        "train_generator = augmented_generator(train_path, train_doc, batch_size)\n",
        "val_generator = augmented_generator(val_path, val_doc, batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g1jbuK6K9CH4",
        "colab": {}
      },
      "source": [
        "model_name = 'model_10_conv3D' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
        "    \n",
        "if not os.path.exists(model_name):\n",
        "    os.mkdir(model_name)\n",
        "        \n",
        "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
        "\n",
        "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1) # write the REducelronplateau code here\n",
        "callbacks_list = [checkpoint, LR]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dHgp5x_-9CH7",
        "colab": {}
      },
      "source": [
        "if (num_train_sequences%batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "if (num_val_sequences%batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences/batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences//batch_size) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2N_KZD7Z9CH-",
        "colab": {},
        "outputId": "13f24716-a196-484e-ef78-b631bc24352f"
      },
      "source": [
        "batch_size = 32\n",
        "num_epochs = 30\n",
        "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
        "                    callbacks=callbacks_list, validation_data=val_generator, \n",
        "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source path =  Project_data/val ; batch size = 32\n",
            "Source path =  Project_data/train ; batch size = 32\n",
            "Epoch 1/30\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: DeprecationWarning: `imread` is deprecated!\n",
            "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imread`` instead.\n",
            "  app.launch_new_instance()\n",
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:34: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " 2/21 [=>............................] - ETA: 2:09 - loss: 2.2018 - categorical_accuracy: 0.2031"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:57: DeprecationWarning: `imread` is deprecated!\n",
            "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imread`` instead.\n",
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:75: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21/21 [==============================] - 102s 5s/step - loss: 1.8575 - categorical_accuracy: 0.3579 - val_loss: 4.7349 - val_categorical_accuracy: 0.2800\n",
            "\n",
            "Epoch 00001: saving model to model_10_conv3D_2020-08-1616_56_15.829707/model-00001-1.86456-0.35747-4.73492-0.28000.h5\n",
            "Epoch 2/30\n",
            "21/21 [==============================] - 88s 4s/step - loss: 1.4701 - categorical_accuracy: 0.4582 - val_loss: 4.0973 - val_categorical_accuracy: 0.3100\n",
            "\n",
            "Epoch 00002: saving model to model_10_conv3D_2020-08-1616_56_15.829707/model-00002-1.47221-0.45852-4.09734-0.31000.h5\n",
            "Epoch 3/30\n",
            "21/21 [==============================] - 91s 4s/step - loss: 1.1895 - categorical_accuracy: 0.5410 - val_loss: 1.5951 - val_categorical_accuracy: 0.4600\n",
            "\n",
            "Epoch 00003: saving model to model_10_conv3D_2020-08-1616_56_15.829707/model-00003-1.18858-0.54299-1.59511-0.46000.h5\n",
            "Epoch 4/30\n",
            "21/21 [==============================] - 91s 4s/step - loss: 1.0307 - categorical_accuracy: 0.5969 - val_loss: 1.1387 - val_categorical_accuracy: 0.5500\n",
            "\n",
            "Epoch 00004: saving model to model_10_conv3D_2020-08-1616_56_15.829707/model-00004-1.03292-0.59729-1.13870-0.55000.h5\n",
            "Epoch 5/30\n",
            "21/21 [==============================] - 91s 4s/step - loss: 0.9347 - categorical_accuracy: 0.6581 - val_loss: 1.3059 - val_categorical_accuracy: 0.5500\n",
            "\n",
            "Epoch 00005: saving model to model_10_conv3D_2020-08-1616_56_15.829707/model-00005-0.93786-0.65762-1.30589-0.55000.h5\n",
            "Epoch 6/30\n",
            "21/21 [==============================] - 91s 4s/step - loss: 0.7762 - categorical_accuracy: 0.6788 - val_loss: 1.2946 - val_categorical_accuracy: 0.6600\n",
            "\n",
            "Epoch 00006: saving model to model_10_conv3D_2020-08-1616_56_15.829707/model-00006-0.77443-0.68024-1.29458-0.66000.h5\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "Epoch 7/30\n",
            "21/21 [==============================] - 91s 4s/step - loss: 0.7466 - categorical_accuracy: 0.7193 - val_loss: 1.1796 - val_categorical_accuracy: 0.6100\n",
            "\n",
            "Epoch 00007: saving model to model_10_conv3D_2020-08-1616_56_15.829707/model-00007-0.74865-0.71795-1.17961-0.61000.h5\n",
            "Epoch 8/30\n",
            "21/21 [==============================] - 91s 4s/step - loss: 0.7172 - categorical_accuracy: 0.7404 - val_loss: 1.1997 - val_categorical_accuracy: 0.5500\n",
            "\n",
            "Epoch 00008: saving model to model_10_conv3D_2020-08-1616_56_15.829707/model-00008-0.71554-0.74208-1.19968-0.55000.h5\n",
            "\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "Epoch 9/30\n",
            "21/21 [==============================] - 92s 4s/step - loss: 0.6525 - categorical_accuracy: 0.7740 - val_loss: 1.0801 - val_categorical_accuracy: 0.6500\n",
            "\n",
            "Epoch 00009: saving model to model_10_conv3D_2020-08-1616_56_15.829707/model-00009-0.65356-0.77225-1.08006-0.65000.h5\n",
            "Epoch 10/30\n",
            "21/21 [==============================] - 91s 4s/step - loss: 0.6088 - categorical_accuracy: 0.7536 - val_loss: 1.5125 - val_categorical_accuracy: 0.5700\n",
            "\n",
            "Epoch 00010: saving model to model_10_conv3D_2020-08-1616_56_15.829707/model-00010-0.60984-0.75264-1.51252-0.57000.h5\n",
            "Epoch 11/30\n",
            "21/21 [==============================] - 91s 4s/step - loss: 0.5215 - categorical_accuracy: 0.7953 - val_loss: 0.9138 - val_categorical_accuracy: 0.6800\n",
            "\n",
            "Epoch 00011: saving model to model_10_conv3D_2020-08-1616_56_15.829707/model-00011-0.52244-0.79487-0.91385-0.68000.h5\n",
            "Epoch 12/30\n",
            "21/21 [==============================] - 91s 4s/step - loss: 0.5137 - categorical_accuracy: 0.7882 - val_loss: 1.2540 - val_categorical_accuracy: 0.5600\n",
            "\n",
            "Epoch 00012: saving model to model_10_conv3D_2020-08-1616_56_15.829707/model-00012-0.51323-0.78884-1.25397-0.56000.h5\n",
            "Epoch 13/30\n",
            "21/21 [==============================] - 92s 4s/step - loss: 0.4703 - categorical_accuracy: 0.8191 - val_loss: 0.8733 - val_categorical_accuracy: 0.6900\n",
            "\n",
            "Epoch 00013: saving model to model_10_conv3D_2020-08-1616_56_15.829707/model-00013-0.47150-0.81900-0.87330-0.69000.h5\n",
            "Epoch 14/30\n",
            "21/21 [==============================] - 91s 4s/step - loss: 0.4927 - categorical_accuracy: 0.7997 - val_loss: 1.5005 - val_categorical_accuracy: 0.5100\n",
            "\n",
            "Epoch 00014: saving model to model_10_conv3D_2020-08-1616_56_15.829707/model-00014-0.49405-0.79940-1.50054-0.51000.h5\n",
            "Epoch 15/30\n",
            "21/21 [==============================] - 93s 4s/step - loss: 0.4076 - categorical_accuracy: 0.8459 - val_loss: 1.2579 - val_categorical_accuracy: 0.5500\n",
            "\n",
            "Epoch 00015: saving model to model_10_conv3D_2020-08-1616_56_15.829707/model-00015-0.40705-0.84615-1.25794-0.55000.h5\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "Epoch 16/30\n",
            "21/21 [==============================] - 93s 4s/step - loss: 0.4021 - categorical_accuracy: 0.8453 - val_loss: 0.9563 - val_categorical_accuracy: 0.6500\n",
            "\n",
            "Epoch 00016: saving model to model_10_conv3D_2020-08-1616_56_15.829707/model-00016-0.40076-0.84615-0.95632-0.65000.h5\n",
            "Epoch 17/30\n",
            "21/21 [==============================] - 94s 4s/step - loss: 0.3494 - categorical_accuracy: 0.8723 - val_loss: 0.8301 - val_categorical_accuracy: 0.7100\n",
            "\n",
            "Epoch 00017: saving model to model_10_conv3D_2020-08-1616_56_15.829707/model-00017-0.34980-0.87179-0.83009-0.71000.h5\n",
            "Epoch 18/30\n",
            "21/21 [==============================] - 92s 4s/step - loss: 0.3138 - categorical_accuracy: 0.8867 - val_loss: 1.0720 - val_categorical_accuracy: 0.6600\n",
            "\n",
            "Epoch 00018: saving model to model_10_conv3D_2020-08-1616_56_15.829707/model-00018-0.31416-0.88688-1.07201-0.66000.h5\n",
            "Epoch 19/30\n",
            "21/21 [==============================] - 93s 4s/step - loss: 0.3545 - categorical_accuracy: 0.8686 - val_loss: 0.9089 - val_categorical_accuracy: 0.6800\n",
            "\n",
            "Epoch 00019: saving model to model_10_conv3D_2020-08-1616_56_15.829707/model-00019-0.35102-0.87029-0.90892-0.68000.h5\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "Epoch 20/30\n",
            "21/21 [==============================] - 93s 4s/step - loss: 0.3448 - categorical_accuracy: 0.8842 - val_loss: 0.7450 - val_categorical_accuracy: 0.7100\n",
            "\n",
            "Epoch 00020: saving model to model_10_conv3D_2020-08-1616_56_15.829707/model-00020-0.34514-0.88386-0.74503-0.71000.h5\n",
            "Epoch 21/30\n",
            "21/21 [==============================] - 92s 4s/step - loss: 0.2821 - categorical_accuracy: 0.9056 - val_loss: 0.7570 - val_categorical_accuracy: 0.7500\n",
            "\n",
            "Epoch 00021: saving model to model_10_conv3D_2020-08-1616_56_15.829707/model-00021-0.28204-0.90498-0.75700-0.75000.h5\n",
            "Epoch 22/30\n",
            "21/21 [==============================] - 91s 4s/step - loss: 0.2930 - categorical_accuracy: 0.8986 - val_loss: 0.7568 - val_categorical_accuracy: 0.7200\n",
            "\n",
            "Epoch 00022: saving model to model_10_conv3D_2020-08-1616_56_15.829707/model-00022-0.29312-0.89894-0.75685-0.72000.h5\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "Epoch 23/30\n",
            "21/21 [==============================] - 91s 4s/step - loss: 0.3216 - categorical_accuracy: 0.8935 - val_loss: 0.7050 - val_categorical_accuracy: 0.7700\n",
            "\n",
            "Epoch 00023: saving model to model_10_conv3D_2020-08-1616_56_15.829707/model-00023-0.31754-0.89442-0.70501-0.77000.h5\n",
            "Epoch 24/30\n",
            "21/21 [==============================] - 91s 4s/step - loss: 0.3222 - categorical_accuracy: 0.8801 - val_loss: 0.7728 - val_categorical_accuracy: 0.7900\n",
            "\n",
            "Epoch 00024: saving model to model_10_conv3D_2020-08-1616_56_15.829707/model-00024-0.32199-0.88084-0.77277-0.79000.h5\n",
            "Epoch 25/30\n",
            "21/21 [==============================] - 92s 4s/step - loss: 0.2824 - categorical_accuracy: 0.9056 - val_loss: 0.7400 - val_categorical_accuracy: 0.7900\n",
            "\n",
            "Epoch 00025: saving model to model_10_conv3D_2020-08-1616_56_15.829707/model-00025-0.28315-0.90498-0.74005-0.79000.h5\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "Epoch 26/30\n",
            "21/21 [==============================] - 93s 4s/step - loss: 0.2740 - categorical_accuracy: 0.8997 - val_loss: 0.6633 - val_categorical_accuracy: 0.7700\n",
            "\n",
            "Epoch 00026: saving model to model_10_conv3D_2020-08-1616_56_15.829707/model-00026-0.27562-0.89894-0.66330-0.77000.h5\n",
            "Epoch 27/30\n",
            "21/21 [==============================] - 92s 4s/step - loss: 0.2822 - categorical_accuracy: 0.8867 - val_loss: 0.7496 - val_categorical_accuracy: 0.7200\n",
            "\n",
            "Epoch 00027: saving model to model_10_conv3D_2020-08-1616_56_15.829707/model-00027-0.28115-0.88688-0.74959-0.72000.h5\n",
            "Epoch 28/30\n",
            "21/21 [==============================] - 92s 4s/step - loss: 0.3067 - categorical_accuracy: 0.8870 - val_loss: 0.6655 - val_categorical_accuracy: 0.7600\n",
            "\n",
            "Epoch 00028: saving model to model_10_conv3D_2020-08-1616_56_15.829707/model-00028-0.30560-0.88839-0.66552-0.76000.h5\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "Epoch 29/30\n",
            "21/21 [==============================] - 93s 4s/step - loss: 0.2734 - categorical_accuracy: 0.9010 - val_loss: 0.6796 - val_categorical_accuracy: 0.7600\n",
            "\n",
            "Epoch 00029: saving model to model_10_conv3D_2020-08-1616_56_15.829707/model-00029-0.27321-0.90196-0.67959-0.76000.h5\n",
            "Epoch 30/30\n",
            "21/21 [==============================] - 92s 4s/step - loss: 0.2697 - categorical_accuracy: 0.9120 - val_loss: 0.6614 - val_categorical_accuracy: 0.8200\n",
            "\n",
            "Epoch 00030: saving model to model_10_conv3D_2020-08-1616_56_15.829707/model-00030-0.26988-0.91252-0.66142-0.82000.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb6d8072128>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5nQKPXfRHZrO"
      },
      "source": [
        "**Observation**\n",
        "\n",
        "*   Training Accuracy: 91%\n",
        "*   Validation Accuracy: 82%\n",
        "*   Learning Rate: 3.125000148429535e-05\n",
        "*   Filter Size: 5 X 5, 3 X 3\n",
        "*   Batch Size: 32\n",
        "*   Epoch: 30\n",
        "*   Optimizer: Adam\n",
        "*   Augmentation: None\n",
        "*   Trainable Parameters: 1,752,901"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jqz1HJ-GH0vW"
      },
      "source": [
        "**Inference**\n",
        "\n",
        "\n",
        "*   Training accuracy diminished from ***98% (without data augmentation)*** to ***91% (with data augmentation)***\n",
        "*   Validation accuracy diminished from ***87% (without data augmentation)*** to ***82% (with data augmentation)***\n",
        "\n",
        "Hence we will continue our further experiments without any data augmentations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GVd-RLE_KpDT"
      },
      "source": [
        "**Model 11 - Data Augmentation and Adam optimizer with increased number of layers [double of the earlier ones], having image size 120 X 120 with Batch Size 32 and Epoch 30**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PlsNb8vd9CIC",
        "colab": {}
      },
      "source": [
        "#write your model here\n",
        "img_height = 120\n",
        "img_width = 120\n",
        "img_filter = [16,32,64,128]\n",
        "img_dense = [256,128]\n",
        "dropout = [0.5,0.25]\n",
        "\n",
        "# Input\n",
        "input_shape=(img_frames,img_height,img_width,img_channel)\n",
        "\n",
        "# Input Layer\n",
        "model = Sequential()\n",
        "model.add(Conv3D(img_filter[0], kernel_size=(5, 5, 5), padding='same', input_shape=input_shape))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv3D(img_filter[0], kernel_size=(5, 5, 5), padding='same', input_shape=input_shape))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "# Layer 2\n",
        "model.add(Conv3D(img_filter[1], kernel_size=(3, 3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv3D(img_filter[1], kernel_size=(3, 3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "# Layer 3\n",
        "model.add(Conv3D(img_filter[2], kernel_size=(1, 3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv3D(img_filter[2], kernel_size=(1, 3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "# Layer 4\n",
        "model.add(Conv3D(img_filter[3], kernel_size=(1, 3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv3D(img_filter[3], kernel_size=(1, 3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "#Flatten the layers\n",
        "model.add(Flatten())\n",
        "\n",
        "#Fully Connected Layer\n",
        "model.add(Dense(img_dense[0],activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(dropout[0]))\n",
        "\n",
        "model.add(Dense(img_dense[1],activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(dropout[1]))\n",
        "\n",
        "#softmax layer\n",
        "model.add(Dense(nb_classes,activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YqsgDmVk9CIJ",
        "colab": {},
        "outputId": "6b77c811-edce-432f-fdad-bf5c8344f472"
      },
      "source": [
        "learning_rate = 0.001\n",
        "optimiser = Adam(learning_rate)\n",
        "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "print (model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv3d_21 (Conv3D)           (None, 30, 120, 120, 16)  6016      \n",
            "_________________________________________________________________\n",
            "activation_21 (Activation)   (None, 30, 120, 120, 16)  0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_31 (Batc (None, 30, 120, 120, 16)  64        \n",
            "_________________________________________________________________\n",
            "conv3d_22 (Conv3D)           (None, 30, 120, 120, 16)  32016     \n",
            "_________________________________________________________________\n",
            "activation_22 (Activation)   (None, 30, 120, 120, 16)  0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_32 (Batc (None, 30, 120, 120, 16)  64        \n",
            "_________________________________________________________________\n",
            "max_pooling3d_21 (MaxPooling (None, 15, 60, 60, 16)    0         \n",
            "_________________________________________________________________\n",
            "conv3d_23 (Conv3D)           (None, 15, 60, 60, 32)    13856     \n",
            "_________________________________________________________________\n",
            "activation_23 (Activation)   (None, 15, 60, 60, 32)    0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_33 (Batc (None, 15, 60, 60, 32)    128       \n",
            "_________________________________________________________________\n",
            "conv3d_24 (Conv3D)           (None, 15, 60, 60, 32)    27680     \n",
            "_________________________________________________________________\n",
            "activation_24 (Activation)   (None, 15, 60, 60, 32)    0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_34 (Batc (None, 15, 60, 60, 32)    128       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_22 (MaxPooling (None, 7, 30, 30, 32)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_25 (Conv3D)           (None, 7, 30, 30, 64)     18496     \n",
            "_________________________________________________________________\n",
            "activation_25 (Activation)   (None, 7, 30, 30, 64)     0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_35 (Batc (None, 7, 30, 30, 64)     256       \n",
            "_________________________________________________________________\n",
            "conv3d_26 (Conv3D)           (None, 7, 30, 30, 64)     36928     \n",
            "_________________________________________________________________\n",
            "activation_26 (Activation)   (None, 7, 30, 30, 64)     0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_36 (Batc (None, 7, 30, 30, 64)     256       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_23 (MaxPooling (None, 3, 15, 15, 64)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_27 (Conv3D)           (None, 3, 15, 15, 128)    73856     \n",
            "_________________________________________________________________\n",
            "activation_27 (Activation)   (None, 3, 15, 15, 128)    0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_37 (Batc (None, 3, 15, 15, 128)    512       \n",
            "_________________________________________________________________\n",
            "conv3d_28 (Conv3D)           (None, 3, 15, 15, 128)    147584    \n",
            "_________________________________________________________________\n",
            "activation_28 (Activation)   (None, 3, 15, 15, 128)    0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_38 (Batc (None, 3, 15, 15, 128)    512       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_24 (MaxPooling (None, 1, 7, 7, 128)      0         \n",
            "_________________________________________________________________\n",
            "flatten_6 (Flatten)          (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 256)               1605888   \n",
            "_________________________________________________________________\n",
            "batch_normalization_39 (Batc (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "batch_normalization_40 (Batc (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 5)                 645       \n",
            "=================================================================\n",
            "Total params: 1,999,317\n",
            "Trainable params: 1,997,589\n",
            "Non-trainable params: 1,728\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WD5rkfPF9CIZ",
        "colab": {}
      },
      "source": [
        "train_generator = generator(train_path, train_doc, batch_size)\n",
        "val_generator = generator(val_path, val_doc, batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Wsj0oOez9CId",
        "colab": {}
      },
      "source": [
        "model_name = 'model_11_conv3D' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
        "    \n",
        "if not os.path.exists(model_name):\n",
        "    os.mkdir(model_name)\n",
        "        \n",
        "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
        "\n",
        "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1) # write the REducelronplateau code here\n",
        "callbacks_list = [checkpoint, LR]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zUdCRFHk9CIg",
        "colab": {}
      },
      "source": [
        "if (num_train_sequences%batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "if (num_val_sequences%batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences/batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences//batch_size) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jOUrvEIK9CIk",
        "colab": {},
        "outputId": "6326b041-cd11-49a4-fb9b-1b90e50c1655"
      },
      "source": [
        "batch_size = 32\n",
        "num_epochs = 20\n",
        "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
        "                    callbacks=callbacks_list, validation_data=val_generator, \n",
        "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source path =  Project_data/val ; batch size = 32\n",
            "Source path =  Epoch 1/20\n",
            "Project_data/train ; batch size = 32\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:15: DeprecationWarning: `imread` is deprecated!\n",
            "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imread`` instead.\n",
            "  from ipykernel import kernelapp as app\n",
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n",
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:39: DeprecationWarning: `imread` is deprecated!\n",
            "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imread`` instead.\n",
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:44: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21/21 [==============================] - 187s 9s/step - loss: 1.7814 - categorical_accuracy: 0.3683 - val_loss: 11.2991 - val_categorical_accuracy: 0.2300\n",
            "\n",
            "Epoch 00001: saving model to model_11_conv3D_2020-08-1616_56_15.829707/model-00001-1.77822-0.36802-11.29906-0.23000.h5\n",
            "Epoch 2/20\n",
            "21/21 [==============================] - 132s 6s/step - loss: 1.1074 - categorical_accuracy: 0.5790 - val_loss: 1.3951 - val_categorical_accuracy: 0.4000\n",
            "\n",
            "Epoch 00002: saving model to model_11_conv3D_2020-08-1616_56_15.829707/model-00002-1.10593-0.57919-1.39512-0.40000.h5\n",
            "Epoch 3/20\n",
            "21/21 [==============================] - 131s 6s/step - loss: 0.8603 - categorical_accuracy: 0.6540 - val_loss: 1.1432 - val_categorical_accuracy: 0.5400\n",
            "\n",
            "Epoch 00003: saving model to model_11_conv3D_2020-08-1616_56_15.829707/model-00003-0.86263-0.65460-1.14320-0.54000.h5\n",
            "Epoch 4/20\n",
            "21/21 [==============================] - 132s 6s/step - loss: 0.6877 - categorical_accuracy: 0.7391 - val_loss: 1.1439 - val_categorical_accuracy: 0.5500\n",
            "\n",
            "Epoch 00004: saving model to model_11_conv3D_2020-08-1616_56_15.829707/model-00004-0.68843-0.73906-1.14387-0.55000.h5\n",
            "Epoch 5/20\n",
            "21/21 [==============================] - 132s 6s/step - loss: 0.6184 - categorical_accuracy: 0.7526 - val_loss: 1.1397 - val_categorical_accuracy: 0.6400\n",
            "\n",
            "Epoch 00005: saving model to model_11_conv3D_2020-08-1616_56_15.829707/model-00005-0.62028-0.75113-1.13965-0.64000.h5\n",
            "Epoch 6/20\n",
            "21/21 [==============================] - 132s 6s/step - loss: 0.4530 - categorical_accuracy: 0.8206 - val_loss: 1.1494 - val_categorical_accuracy: 0.5900\n",
            "\n",
            "Epoch 00006: saving model to model_11_conv3D_2020-08-1616_56_15.829707/model-00006-0.45380-0.82051-1.14945-0.59000.h5\n",
            "Epoch 7/20\n",
            "21/21 [==============================] - 132s 6s/step - loss: 0.3605 - categorical_accuracy: 0.8608 - val_loss: 1.0710 - val_categorical_accuracy: 0.6200\n",
            "\n",
            "Epoch 00007: saving model to model_11_conv3D_2020-08-1616_56_15.829707/model-00007-0.36009-0.86124-1.07100-0.62000.h5\n",
            "Epoch 8/20\n",
            "21/21 [==============================] - 132s 6s/step - loss: 0.2695 - categorical_accuracy: 0.9001 - val_loss: 1.4817 - val_categorical_accuracy: 0.5600\n",
            "\n",
            "Epoch 00008: saving model to model_11_conv3D_2020-08-1616_56_15.829707/model-00008-0.26936-0.90045-1.48166-0.56000.h5\n",
            "Epoch 9/20\n",
            "21/21 [==============================] - 131s 6s/step - loss: 0.2118 - categorical_accuracy: 0.9220 - val_loss: 1.6892 - val_categorical_accuracy: 0.5600\n",
            "\n",
            "Epoch 00009: saving model to model_11_conv3D_2020-08-1616_56_15.829707/model-00009-0.21239-0.92157-1.68920-0.56000.h5\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "Epoch 10/20\n",
            "21/21 [==============================] - 132s 6s/step - loss: 0.1487 - categorical_accuracy: 0.9583 - val_loss: 1.0300 - val_categorical_accuracy: 0.6500\n",
            "\n",
            "Epoch 00010: saving model to model_11_conv3D_2020-08-1616_56_15.829707/model-00010-0.14904-0.95777-1.03004-0.65000.h5\n",
            "Epoch 11/20\n",
            "21/21 [==============================] - 132s 6s/step - loss: 0.1199 - categorical_accuracy: 0.9658 - val_loss: 1.2654 - val_categorical_accuracy: 0.6200\n",
            "\n",
            "Epoch 00011: saving model to model_11_conv3D_2020-08-1616_56_15.829707/model-00011-0.11996-0.96531-1.26538-0.62000.h5\n",
            "Epoch 12/20\n",
            "21/21 [==============================] - 132s 6s/step - loss: 0.1006 - categorical_accuracy: 0.9771 - val_loss: 1.0033 - val_categorical_accuracy: 0.6500\n",
            "\n",
            "Epoch 00012: saving model to model_11_conv3D_2020-08-1616_56_15.829707/model-00012-0.09984-0.97738-1.00335-0.65000.h5\n",
            "Epoch 13/20\n",
            "21/21 [==============================] - 132s 6s/step - loss: 0.0701 - categorical_accuracy: 0.9836 - val_loss: 0.8608 - val_categorical_accuracy: 0.7200\n",
            "\n",
            "Epoch 00013: saving model to model_11_conv3D_2020-08-1616_56_15.829707/model-00013-0.07050-0.98341-0.86081-0.72000.h5\n",
            "Epoch 14/20\n",
            "21/21 [==============================] - 131s 6s/step - loss: 0.0514 - categorical_accuracy: 0.9911 - val_loss: 0.8849 - val_categorical_accuracy: 0.7100\n",
            "\n",
            "Epoch 00014: saving model to model_11_conv3D_2020-08-1616_56_15.829707/model-00014-0.05118-0.99095-0.88493-0.71000.h5\n",
            "Epoch 15/20\n",
            "21/21 [==============================] - 131s 6s/step - loss: 0.0654 - categorical_accuracy: 0.9855 - val_loss: 0.9994 - val_categorical_accuracy: 0.6300\n",
            "\n",
            "Epoch 00015: saving model to model_11_conv3D_2020-08-1616_56_15.829707/model-00015-0.06429-0.98643-0.99940-0.63000.h5\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "Epoch 16/20\n",
            "21/21 [==============================] - 131s 6s/step - loss: 0.0562 - categorical_accuracy: 0.9881 - val_loss: 0.7919 - val_categorical_accuracy: 0.7200\n",
            "\n",
            "Epoch 00016: saving model to model_11_conv3D_2020-08-1616_56_15.829707/model-00016-0.05616-0.98793-0.79187-0.72000.h5\n",
            "Epoch 17/20\n",
            "21/21 [==============================] - 131s 6s/step - loss: 0.0578 - categorical_accuracy: 0.9881 - val_loss: 0.8876 - val_categorical_accuracy: 0.6400\n",
            "\n",
            "Epoch 00017: saving model to model_11_conv3D_2020-08-1616_56_15.829707/model-00017-0.05765-0.98793-0.88761-0.64000.h5\n",
            "Epoch 18/20\n",
            "21/21 [==============================] - 131s 6s/step - loss: 0.0307 - categorical_accuracy: 0.9985 - val_loss: 0.7980 - val_categorical_accuracy: 0.7200\n",
            "\n",
            "Epoch 00018: saving model to model_11_conv3D_2020-08-1616_56_15.829707/model-00018-0.03062-0.99849-0.79799-0.72000.h5\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "Epoch 19/20\n",
            "21/21 [==============================] - 131s 6s/step - loss: 0.0320 - categorical_accuracy: 0.9940 - val_loss: 0.9278 - val_categorical_accuracy: 0.7100\n",
            "\n",
            "Epoch 00019: saving model to model_11_conv3D_2020-08-1616_56_15.829707/model-00019-0.03214-0.99397-0.92781-0.71000.h5\n",
            "Epoch 20/20\n",
            "21/21 [==============================] - 131s 6s/step - loss: 0.0326 - categorical_accuracy: 0.9940 - val_loss: 0.9274 - val_categorical_accuracy: 0.7000\n",
            "\n",
            "Epoch 00020: saving model to model_11_conv3D_2020-08-1616_56_15.829707/model-00020-0.03263-0.99397-0.92743-0.70000.h5\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb702772c88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "K9mg648VJHkD"
      },
      "source": [
        "**Observation**\n",
        "\n",
        "*   Training Accuracy: 98%\n",
        "*   Validation Accuracy: 72%\n",
        "*   Learning Rate: 0.0005\n",
        "*   Filter Size: 5 X 5, 3 X 3\n",
        "*   Batch Size: 32\n",
        "*   Epoch: 30\n",
        "*   Optimizer: Adam\n",
        "*   Augmentation: None\n",
        "*   Trainable Parameters: 1,997,589"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V3JXUE-CJp1E"
      },
      "source": [
        "**Inference**\n",
        "\n",
        "By doubling the number of hidden layers, the training accuracy reamained same but validation accuracy has diminished from **82%** to **72%**. Hence there is no need to increase the number of layers. Moreover we can see that the data is overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "H7vwN0zgLC-k"
      },
      "source": [
        "**Model 12 - Data Augmentation and Adam optimizer with reduced parameters, having image size 120 X 120 with Batch Size 32 and Epoch 30**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-I9ym5q59CIo",
        "colab": {}
      },
      "source": [
        "#write your model here\n",
        "img_height = 120\n",
        "img_width = 120\n",
        "img_filter = [16,32,64,128]\n",
        "img_dense = 128\n",
        "dropout = [0.5,0.25]\n",
        "\n",
        "\n",
        "# Input\n",
        "input_shape=(img_frames,img_height,img_width,img_channel)\n",
        "\n",
        "# Input Layer\n",
        "model = Sequential()\n",
        "model.add(Conv3D(img_filter[0], kernel_size=(5, 5, 5), padding='same', input_shape=input_shape))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "# Layer 2\n",
        "model.add(Conv3D(img_filter[1], kernel_size=(3, 3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "# Layer 3\n",
        "model.add(Conv3D(img_filter[2], kernel_size=(1, 3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "# Layer 4\n",
        "model.add(Conv3D(img_filter[3], kernel_size=(1, 3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "#Flatten the layers\n",
        "model.add(Flatten())\n",
        "\n",
        "#Fully Connected Layer\n",
        "model.add(Dense(img_dense,activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(dropout[0]))\n",
        "\n",
        "model.add(Dense(img_dense,activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(dropout[1]))\n",
        "\n",
        "#softmax layer\n",
        "model.add(Dense(nb_classes,activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PBeH94cuMDZ8",
        "colab": {},
        "outputId": "5f8376fe-704f-4d6c-9829-feeec97fb21f"
      },
      "source": [
        "learning_rate = 0.001\n",
        "optimiser = Adam(learning_rate)\n",
        "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "print (model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv3d_29 (Conv3D)           (None, 30, 120, 120, 16)  6016      \n",
            "_________________________________________________________________\n",
            "activation_29 (Activation)   (None, 30, 120, 120, 16)  0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_41 (Batc (None, 30, 120, 120, 16)  64        \n",
            "_________________________________________________________________\n",
            "max_pooling3d_25 (MaxPooling (None, 15, 60, 60, 16)    0         \n",
            "_________________________________________________________________\n",
            "conv3d_30 (Conv3D)           (None, 15, 60, 60, 32)    13856     \n",
            "_________________________________________________________________\n",
            "activation_30 (Activation)   (None, 15, 60, 60, 32)    0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_42 (Batc (None, 15, 60, 60, 32)    128       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_26 (MaxPooling (None, 7, 30, 30, 32)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_31 (Conv3D)           (None, 7, 30, 30, 64)     18496     \n",
            "_________________________________________________________________\n",
            "activation_31 (Activation)   (None, 7, 30, 30, 64)     0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_43 (Batc (None, 7, 30, 30, 64)     256       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_27 (MaxPooling (None, 3, 15, 15, 64)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_32 (Conv3D)           (None, 3, 15, 15, 128)    73856     \n",
            "_________________________________________________________________\n",
            "activation_32 (Activation)   (None, 3, 15, 15, 128)    0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_44 (Batc (None, 3, 15, 15, 128)    512       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_28 (MaxPooling (None, 1, 7, 7, 128)      0         \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 128)               802944    \n",
            "_________________________________________________________________\n",
            "batch_normalization_45 (Batc (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "batch_normalization_46 (Batc (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 5)                 645       \n",
            "=================================================================\n",
            "Total params: 934,309\n",
            "Trainable params: 933,317\n",
            "Non-trainable params: 992\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cytSBOWXMYLz",
        "colab": {}
      },
      "source": [
        "train_generator = generator(train_path, train_doc, batch_size)\n",
        "val_generator = generator(val_path, val_doc, batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "saKlslCCMY99",
        "colab": {}
      },
      "source": [
        "model_name = 'model_11_conv3D' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
        "    \n",
        "if not os.path.exists(model_name):\n",
        "    os.mkdir(model_name)\n",
        "        \n",
        "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
        "\n",
        "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1) # write the REducelronplateau code here\n",
        "callbacks_list = [checkpoint, LR]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rrnDlrB0Mdfm",
        "colab": {}
      },
      "source": [
        "if (num_train_sequences%batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "if (num_val_sequences%batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences/batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences//batch_size) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XI0IQXu-Mh7f",
        "colab": {},
        "outputId": "dcf211a7-6dcc-472a-8587-f67ba51e6a99"
      },
      "source": [
        "batch_size = 32\n",
        "num_epochs = 30\n",
        "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
        "                    callbacks=callbacks_list, validation_data=val_generator, \n",
        "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source path =  Project_data/val ; batch size = 32\n",
            "Source path =  Project_data/train ; batch size = 32\n",
            "Epoch 1/30\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:15: DeprecationWarning: `imread` is deprecated!\n",
            "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imread`` instead.\n",
            "  from ipykernel import kernelapp as app\n",
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " 3/21 [===>..........................] - ETA: 1:35 - loss: 2.3973 - categorical_accuracy: 0.2292"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:39: DeprecationWarning: `imread` is deprecated!\n",
            "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imread`` instead.\n",
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:44: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21/21 [==============================] - 89s 4s/step - loss: 1.8088 - categorical_accuracy: 0.3538 - val_loss: 1.6505 - val_categorical_accuracy: 0.3400\n",
            "\n",
            "Epoch 00001: saving model to model_11_conv3D_2020-08-1616_56_15.829707/model-00001-1.81116-0.35445-1.65050-0.34000.h5\n",
            "Epoch 2/30\n",
            "21/21 [==============================] - 73s 3s/step - loss: 1.2527 - categorical_accuracy: 0.5040 - val_loss: 1.0908 - val_categorical_accuracy: 0.6000\n",
            "\n",
            "Epoch 00002: saving model to model_11_conv3D_2020-08-1616_56_15.829707/model-00002-1.25331-0.50377-1.09079-0.60000.h5\n",
            "Epoch 3/30\n",
            "21/21 [==============================] - 77s 4s/step - loss: 0.9847 - categorical_accuracy: 0.6207 - val_loss: 1.2272 - val_categorical_accuracy: 0.4900\n",
            "\n",
            "Epoch 00003: saving model to model_11_conv3D_2020-08-1616_56_15.829707/model-00003-0.98429-0.62142-1.22723-0.49000.h5\n",
            "Epoch 4/30\n",
            "21/21 [==============================] - 78s 4s/step - loss: 0.9014 - categorical_accuracy: 0.6173 - val_loss: 1.0721 - val_categorical_accuracy: 0.5100\n",
            "\n",
            "Epoch 00004: saving model to model_11_conv3D_2020-08-1616_56_15.829707/model-00004-0.90479-0.61689-1.07210-0.51000.h5\n",
            "Epoch 5/30\n",
            "21/21 [==============================] - 78s 4s/step - loss: 0.8255 - categorical_accuracy: 0.6897 - val_loss: 1.0153 - val_categorical_accuracy: 0.5500\n",
            "\n",
            "Epoch 00005: saving model to model_11_conv3D_2020-08-1616_56_15.829707/model-00005-0.82548-0.69080-1.01534-0.55000.h5\n",
            "Epoch 6/30\n",
            "21/21 [==============================] - 78s 4s/step - loss: 0.6665 - categorical_accuracy: 0.7344 - val_loss: 1.0988 - val_categorical_accuracy: 0.5300\n",
            "\n",
            "Epoch 00006: saving model to model_11_conv3D_2020-08-1616_56_15.829707/model-00006-0.65913-0.73605-1.09879-0.53000.h5\n",
            "Epoch 7/30\n",
            "21/21 [==============================] - 77s 4s/step - loss: 0.5299 - categorical_accuracy: 0.8089 - val_loss: 1.1087 - val_categorical_accuracy: 0.5500\n",
            "\n",
            "Epoch 00007: saving model to model_11_conv3D_2020-08-1616_56_15.829707/model-00007-0.53345-0.80694-1.10865-0.55000.h5\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "Epoch 8/30\n",
            "21/21 [==============================] - 76s 4s/step - loss: 0.4534 - categorical_accuracy: 0.8435 - val_loss: 1.1659 - val_categorical_accuracy: 0.5700\n",
            "\n",
            "Epoch 00008: saving model to model_11_conv3D_2020-08-1616_56_15.829707/model-00008-0.45411-0.84314-1.16588-0.57000.h5\n",
            "Epoch 9/30\n",
            "21/21 [==============================] - 78s 4s/step - loss: 0.3280 - categorical_accuracy: 0.8827 - val_loss: 1.1211 - val_categorical_accuracy: 0.5700\n",
            "\n",
            "Epoch 00009: saving model to model_11_conv3D_2020-08-1616_56_15.829707/model-00009-0.32865-0.88235-1.12113-0.57000.h5\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "Epoch 10/30\n",
            "21/21 [==============================] - 77s 4s/step - loss: 0.3164 - categorical_accuracy: 0.8945 - val_loss: 0.8115 - val_categorical_accuracy: 0.6800\n",
            "\n",
            "Epoch 00010: saving model to model_11_conv3D_2020-08-1616_56_15.829707/model-00010-0.31412-0.89593-0.81153-0.68000.h5\n",
            "Epoch 11/30\n",
            "21/21 [==============================] - 78s 4s/step - loss: 0.2503 - categorical_accuracy: 0.9244 - val_loss: 1.0895 - val_categorical_accuracy: 0.6000\n",
            "\n",
            "Epoch 00011: saving model to model_11_conv3D_2020-08-1616_56_15.829707/model-00011-0.24963-0.92459-1.08952-0.60000.h5\n",
            "Epoch 12/30\n",
            "21/21 [==============================] - 77s 4s/step - loss: 0.2510 - categorical_accuracy: 0.9140 - val_loss: 0.9434 - val_categorical_accuracy: 0.6700\n",
            "\n",
            "Epoch 00012: saving model to model_11_conv3D_2020-08-1616_56_15.829707/model-00012-0.25075-0.91403-0.94339-0.67000.h5\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "Epoch 13/30\n",
            "21/21 [==============================] - 78s 4s/step - loss: 0.2248 - categorical_accuracy: 0.9354 - val_loss: 0.8376 - val_categorical_accuracy: 0.7100\n",
            "\n",
            "Epoch 00013: saving model to model_11_conv3D_2020-08-1616_56_15.829707/model-00013-0.22534-0.93514-0.83756-0.71000.h5\n",
            "Epoch 14/30\n",
            "21/21 [==============================] - 77s 4s/step - loss: 0.2087 - categorical_accuracy: 0.9369 - val_loss: 0.7512 - val_categorical_accuracy: 0.7600\n",
            "\n",
            "Epoch 00014: saving model to model_11_conv3D_2020-08-1616_56_15.829707/model-00014-0.20916-0.93665-0.75118-0.76000.h5\n",
            "Epoch 15/30\n",
            "21/21 [==============================] - 78s 4s/step - loss: 0.2135 - categorical_accuracy: 0.9309 - val_loss: 0.7694 - val_categorical_accuracy: 0.7300\n",
            "\n",
            "Epoch 00015: saving model to model_11_conv3D_2020-08-1616_56_15.829707/model-00015-0.21344-0.93062-0.76938-0.73000.h5\n",
            "Epoch 16/30\n",
            "21/21 [==============================] - 77s 4s/step - loss: 0.1812 - categorical_accuracy: 0.9363 - val_loss: 0.7763 - val_categorical_accuracy: 0.7100\n",
            "\n",
            "Epoch 00016: saving model to model_11_conv3D_2020-08-1616_56_15.829707/model-00016-0.17982-0.93665-0.77630-0.71000.h5\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "Epoch 17/30\n",
            "21/21 [==============================] - 77s 4s/step - loss: 0.1718 - categorical_accuracy: 0.9483 - val_loss: 0.7485 - val_categorical_accuracy: 0.7500\n",
            "\n",
            "Epoch 00017: saving model to model_11_conv3D_2020-08-1616_56_15.829707/model-00017-0.17175-0.94872-0.74854-0.75000.h5\n",
            "Epoch 18/30\n",
            "21/21 [==============================] - 77s 4s/step - loss: 0.1819 - categorical_accuracy: 0.9447 - val_loss: 0.7450 - val_categorical_accuracy: 0.7500\n",
            "\n",
            "Epoch 00018: saving model to model_11_conv3D_2020-08-1616_56_15.829707/model-00018-0.17955-0.94570-0.74504-0.75000.h5\n",
            "Epoch 19/30\n",
            "21/21 [==============================] - 77s 4s/step - loss: 0.1368 - categorical_accuracy: 0.9622 - val_loss: 0.8135 - val_categorical_accuracy: 0.7400\n",
            "\n",
            "Epoch 00019: saving model to model_11_conv3D_2020-08-1616_56_15.829707/model-00019-0.13695-0.96229-0.81349-0.74000.h5\n",
            "Epoch 20/30\n",
            "21/21 [==============================] - 78s 4s/step - loss: 0.1601 - categorical_accuracy: 0.9587 - val_loss: 0.6856 - val_categorical_accuracy: 0.7500\n",
            "\n",
            "Epoch 00020: saving model to model_11_conv3D_2020-08-1616_56_15.829707/model-00020-0.15921-0.95928-0.68564-0.75000.h5\n",
            "Epoch 21/30\n",
            "21/21 [==============================] - 77s 4s/step - loss: 0.1202 - categorical_accuracy: 0.9816 - val_loss: 0.7005 - val_categorical_accuracy: 0.7800\n",
            "\n",
            "Epoch 00021: saving model to model_11_conv3D_2020-08-1616_56_15.829707/model-00021-0.12029-0.98190-0.70049-0.78000.h5\n",
            "Epoch 22/30\n",
            "21/21 [==============================] - 77s 4s/step - loss: 0.1535 - categorical_accuracy: 0.9538 - val_loss: 0.6660 - val_categorical_accuracy: 0.7800\n",
            "\n",
            "Epoch 00022: saving model to model_11_conv3D_2020-08-1616_56_15.829707/model-00022-0.15409-0.95324-0.66605-0.78000.h5\n",
            "Epoch 23/30\n",
            "21/21 [==============================] - 77s 4s/step - loss: 0.1319 - categorical_accuracy: 0.9682 - val_loss: 0.6484 - val_categorical_accuracy: 0.7800\n",
            "\n",
            "Epoch 00023: saving model to model_11_conv3D_2020-08-1616_56_15.829707/model-00023-0.13231-0.96833-0.64837-0.78000.h5\n",
            "Epoch 24/30\n",
            "21/21 [==============================] - 77s 4s/step - loss: 0.1051 - categorical_accuracy: 0.9771 - val_loss: 0.6389 - val_categorical_accuracy: 0.7800\n",
            "\n",
            "Epoch 00024: saving model to model_11_conv3D_2020-08-1616_56_15.829707/model-00024-0.10496-0.97738-0.63887-0.78000.h5\n",
            "Epoch 25/30\n",
            "21/21 [==============================] - 77s 4s/step - loss: 0.1270 - categorical_accuracy: 0.9697 - val_loss: 0.6205 - val_categorical_accuracy: 0.7900\n",
            "\n",
            "Epoch 00025: saving model to model_11_conv3D_2020-08-1616_56_15.829707/model-00025-0.12701-0.96983-0.62047-0.79000.h5\n",
            "Epoch 26/30\n",
            "21/21 [==============================] - 77s 4s/step - loss: 0.1055 - categorical_accuracy: 0.9762 - val_loss: 0.6459 - val_categorical_accuracy: 0.7900\n",
            "\n",
            "Epoch 00026: saving model to model_11_conv3D_2020-08-1616_56_15.829707/model-00026-0.10561-0.97587-0.64585-0.79000.h5\n",
            "Epoch 27/30\n",
            "21/21 [==============================] - 76s 4s/step - loss: 0.1398 - categorical_accuracy: 0.9542 - val_loss: 0.6921 - val_categorical_accuracy: 0.7700\n",
            "\n",
            "Epoch 00027: saving model to model_11_conv3D_2020-08-1616_56_15.829707/model-00027-0.13857-0.95475-0.69211-0.77000.h5\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "Epoch 28/30\n",
            "21/21 [==============================] - 77s 4s/step - loss: 0.1058 - categorical_accuracy: 0.9825 - val_loss: 0.6603 - val_categorical_accuracy: 0.7800\n",
            "\n",
            "Epoch 00028: saving model to model_11_conv3D_2020-08-1616_56_15.829707/model-00028-0.10459-0.98341-0.66031-0.78000.h5\n",
            "Epoch 29/30\n",
            "21/21 [==============================] - 77s 4s/step - loss: 0.0971 - categorical_accuracy: 0.9792 - val_loss: 0.6739 - val_categorical_accuracy: 0.7800\n",
            "\n",
            "Epoch 00029: saving model to model_11_conv3D_2020-08-1616_56_15.829707/model-00029-0.09694-0.97888-0.67386-0.78000.h5\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "Epoch 30/30\n",
            "21/21 [==============================] - 76s 4s/step - loss: 0.1063 - categorical_accuracy: 0.9801 - val_loss: 0.6736 - val_categorical_accuracy: 0.8000\n",
            "\n",
            "Epoch 00030: saving model to model_11_conv3D_2020-08-1616_56_15.829707/model-00030-0.10498-0.98039-0.67360-0.80000.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb702772b00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "K9SLKVjje0kL"
      },
      "source": [
        "**Observation**\n",
        "\n",
        "*   Training Accuracy: 98%\n",
        "*   Validation Accuracy: 80%\n",
        "*   Learning Rate: 1.5625000742147677e-05\n",
        "*   Filter Size: 5 X 5, 3 X 3\n",
        "*   Batch Size: 24\n",
        "*   Epoch: 30\n",
        "*   Optimizer: Adam\n",
        "*   Augmentation: None\n",
        "*   Trainable Parameters: 933,317"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Pm0f-yOle8ps"
      },
      "source": [
        "**Inference**\n",
        "\n",
        "***Reducing the number of trainable parameters*** from **1752901** to **933317** actually ***reduced the accuracy*** from **87%** to **80%**. So let us stick to our original model having **1752901** parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZmWU4wPnhqxE"
      },
      "source": [
        "# CNN + LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0uES8VwVhpmt",
        "colab": {}
      },
      "source": [
        "#write your model here\n",
        "img_height = 120\n",
        "img_width = 120\n",
        "img_filter = [16,32,64,128,256]\n",
        "img_dense = 64\n",
        "lstm_cells = 64\n",
        "dropout = 0.25\n",
        "\n",
        "# Input\n",
        "input_shape=(img_frames,img_height,img_width,img_channel)\n",
        "\n",
        "# Input Layer\n",
        "model = Sequential()\n",
        "model.add(TimeDistributed(Conv2D(img_filter[0], kernel_size=(3,3), padding='same', activation='relu'), input_shape=input_shape))\n",
        "model.add(TimeDistributed(BatchNormalization()))\n",
        "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2), strides=(2, 2))))\n",
        "\n",
        "# Layer 2\n",
        "model.add(TimeDistributed(Conv2D(img_filter[1], (3, 3), padding='same', activation='relu')))\n",
        "model.add(TimeDistributed(BatchNormalization()))\n",
        "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2), strides=(2, 2))))\n",
        "\n",
        "# Layer 3\n",
        "model.add(TimeDistributed(Conv2D(img_filter[2], (2, 2), padding='same', activation='relu')))\n",
        "model.add(TimeDistributed(BatchNormalization()))\n",
        "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2), strides=(2, 2))))\n",
        "\n",
        "# Layer 4\n",
        "model.add(TimeDistributed(Conv2D(img_filter[3], (2, 2), padding='same', activation='relu')))\n",
        "model.add(TimeDistributed(BatchNormalization()))\n",
        "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2), strides=(2, 2))))\n",
        "\n",
        "# Layer 5\n",
        "model.add(TimeDistributed(Conv2D(img_filter[4], (2, 2), padding='same', activation='relu')))\n",
        "model.add(TimeDistributed(BatchNormalization()))\n",
        "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2), strides=(2, 2))))\n",
        "\n",
        "#Flatten the layers\n",
        "model.add(TimeDistributed(Flatten()))\n",
        "\n",
        "# LSTM Layer\n",
        "model.add(LSTM(lstm_cells))\n",
        "model.add(Dropout(dropout))\n",
        "\n",
        "#Fully Connected Layer\n",
        "model.add(Dense(img_dense,activation='relu'))\n",
        "model.add(Dropout(dropout))\n",
        "\n",
        "#softmax layer\n",
        "model.add(Dense(nb_classes,activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vze2k-72jmXM",
        "colab": {},
        "outputId": "24ff0124-5edc-4f55-e0db-6a82f6928658"
      },
      "source": [
        "optimiser = Adam()\n",
        "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "print (model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "time_distributed_1 (TimeDist (None, 30, 120, 120, 16)  448       \n",
            "_________________________________________________________________\n",
            "time_distributed_2 (TimeDist (None, 30, 120, 120, 16)  64        \n",
            "_________________________________________________________________\n",
            "time_distributed_3 (TimeDist (None, 30, 60, 60, 16)    0         \n",
            "_________________________________________________________________\n",
            "time_distributed_4 (TimeDist (None, 30, 60, 60, 32)    4640      \n",
            "_________________________________________________________________\n",
            "time_distributed_5 (TimeDist (None, 30, 60, 60, 32)    128       \n",
            "_________________________________________________________________\n",
            "time_distributed_6 (TimeDist (None, 30, 30, 30, 32)    0         \n",
            "_________________________________________________________________\n",
            "time_distributed_7 (TimeDist (None, 30, 30, 30, 64)    8256      \n",
            "_________________________________________________________________\n",
            "time_distributed_8 (TimeDist (None, 30, 30, 30, 64)    256       \n",
            "_________________________________________________________________\n",
            "time_distributed_9 (TimeDist (None, 30, 15, 15, 64)    0         \n",
            "_________________________________________________________________\n",
            "time_distributed_10 (TimeDis (None, 30, 15, 15, 128)   32896     \n",
            "_________________________________________________________________\n",
            "time_distributed_11 (TimeDis (None, 30, 15, 15, 128)   512       \n",
            "_________________________________________________________________\n",
            "time_distributed_12 (TimeDis (None, 30, 7, 7, 128)     0         \n",
            "_________________________________________________________________\n",
            "time_distributed_13 (TimeDis (None, 30, 7, 7, 256)     131328    \n",
            "_________________________________________________________________\n",
            "time_distributed_14 (TimeDis (None, 30, 7, 7, 256)     1024      \n",
            "_________________________________________________________________\n",
            "time_distributed_15 (TimeDis (None, 30, 3, 3, 256)     0         \n",
            "_________________________________________________________________\n",
            "time_distributed_16 (TimeDis (None, 30, 2304)          0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 64)                606464    \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 5)                 325       \n",
            "=================================================================\n",
            "Total params: 790,501\n",
            "Trainable params: 789,509\n",
            "Non-trainable params: 992\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ta-aZiImjzPQ",
        "colab": {}
      },
      "source": [
        "train_generator = generator(train_path, train_doc, batch_size)\n",
        "val_generator = generator(val_path, val_doc, batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TpJdn6mSj0R2",
        "colab": {}
      },
      "source": [
        "model_name = 'model_LSTM' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
        "    \n",
        "if not os.path.exists(model_name):\n",
        "    os.mkdir(model_name)\n",
        "        \n",
        "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
        "\n",
        "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1) # write the REducelronplateau code here\n",
        "callbacks_list = [checkpoint, LR]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yLI4wIiyj7QT",
        "colab": {}
      },
      "source": [
        "if (num_train_sequences%batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "if (num_val_sequences%batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences/batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences//batch_size) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Xxjc-HdHkB_v",
        "colab": {},
        "outputId": "ef4597bf-f2d6-437c-e0ff-fa60667adcc7"
      },
      "source": [
        "batch_size = 32\n",
        "num_epochs = 20\n",
        "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
        "                    callbacks=callbacks_list, validation_data=val_generator, \n",
        "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source path =  Project_data/val ; batch size = 32\n",
            "Source path =  Project_data/train ; batch size = 32\n",
            "Epoch 1/30\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:15: DeprecationWarning: `imread` is deprecated!\n",
            "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imread`` instead.\n",
            "  from ipykernel import kernelapp as app\n",
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " 3/21 [===>..........................] - ETA: 1:34 - loss: 1.5596 - categorical_accuracy: 0.3750"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:39: DeprecationWarning: `imread` is deprecated!\n",
            "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imread`` instead.\n",
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:44: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21/21 [==============================] - 90s 4s/step - loss: 1.4490 - categorical_accuracy: 0.3603 - val_loss: 1.2505 - val_categorical_accuracy: 0.5400\n",
            "\n",
            "Epoch 00001: saving model to model_LSTM_2020-08-1616_56_15.829707/model-00001-1.44938-0.36048-1.25054-0.54000.h5\n",
            "Epoch 2/30\n",
            "21/21 [==============================] - 71s 3s/step - loss: 1.2463 - categorical_accuracy: 0.4644 - val_loss: 1.1506 - val_categorical_accuracy: 0.5500\n",
            "\n",
            "Epoch 00002: saving model to model_LSTM_2020-08-1616_56_15.829707/model-00002-1.24854-0.46305-1.15064-0.55000.h5\n",
            "Epoch 3/30\n",
            "21/21 [==============================] - 78s 4s/step - loss: 1.1444 - categorical_accuracy: 0.5347 - val_loss: 1.1046 - val_categorical_accuracy: 0.5500\n",
            "\n",
            "Epoch 00003: saving model to model_LSTM_2020-08-1616_56_15.829707/model-00003-1.14498-0.53544-1.10463-0.55000.h5\n",
            "Epoch 4/30\n",
            "21/21 [==============================] - 77s 4s/step - loss: 1.1179 - categorical_accuracy: 0.5494 - val_loss: 1.0803 - val_categorical_accuracy: 0.6200\n",
            "\n",
            "Epoch 00004: saving model to model_LSTM_2020-08-1616_56_15.829707/model-00004-1.12121-0.54751-1.08030-0.62000.h5\n",
            "Epoch 5/30\n",
            "21/21 [==============================] - 78s 4s/step - loss: 0.9533 - categorical_accuracy: 0.6144 - val_loss: 0.9903 - val_categorical_accuracy: 0.6300\n",
            "\n",
            "Epoch 00005: saving model to model_LSTM_2020-08-1616_56_15.829707/model-00005-0.95427-0.61388-0.99026-0.63000.h5\n",
            "Epoch 6/30\n",
            "21/21 [==============================] - 77s 4s/step - loss: 0.8701 - categorical_accuracy: 0.6786 - val_loss: 0.9873 - val_categorical_accuracy: 0.5900\n",
            "\n",
            "Epoch 00006: saving model to model_LSTM_2020-08-1616_56_15.829707/model-00006-0.86991-0.67722-0.98734-0.59000.h5\n",
            "Epoch 7/30\n",
            "21/21 [==============================] - 78s 4s/step - loss: 0.7403 - categorical_accuracy: 0.7138 - val_loss: 0.9618 - val_categorical_accuracy: 0.6500\n",
            "\n",
            "Epoch 00007: saving model to model_LSTM_2020-08-1616_56_15.829707/model-00007-0.74007-0.71342-0.96180-0.65000.h5\n",
            "Epoch 8/30\n",
            "21/21 [==============================] - 77s 4s/step - loss: 0.7255 - categorical_accuracy: 0.7115 - val_loss: 1.0390 - val_categorical_accuracy: 0.6000\n",
            "\n",
            "Epoch 00008: saving model to model_LSTM_2020-08-1616_56_15.829707/model-00008-0.72008-0.71342-1.03905-0.60000.h5\n",
            "Epoch 9/30\n",
            "21/21 [==============================] - 77s 4s/step - loss: 0.7164 - categorical_accuracy: 0.7268 - val_loss: 1.5430 - val_categorical_accuracy: 0.4900\n",
            "\n",
            "Epoch 00009: saving model to model_LSTM_2020-08-1616_56_15.829707/model-00009-0.71993-0.72549-1.54302-0.49000.h5\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "Epoch 10/30\n",
            "21/21 [==============================] - 76s 4s/step - loss: 0.6009 - categorical_accuracy: 0.7685 - val_loss: 0.9772 - val_categorical_accuracy: 0.6000\n",
            "\n",
            "Epoch 00010: saving model to model_LSTM_2020-08-1616_56_15.829707/model-00010-0.60064-0.76772-0.97716-0.60000.h5\n",
            "Epoch 11/30\n",
            "21/21 [==============================] - 77s 4s/step - loss: 0.5535 - categorical_accuracy: 0.8092 - val_loss: 0.9203 - val_categorical_accuracy: 0.6300\n",
            "\n",
            "Epoch 00011: saving model to model_LSTM_2020-08-1616_56_15.829707/model-00011-0.55466-0.80845-0.92031-0.63000.h5\n",
            "Epoch 12/30\n",
            "21/21 [==============================] - 77s 4s/step - loss: 0.4794 - categorical_accuracy: 0.8156 - val_loss: 0.9574 - val_categorical_accuracy: 0.6500\n",
            "\n",
            "Epoch 00012: saving model to model_LSTM_2020-08-1616_56_15.829707/model-00012-0.47776-0.81599-0.95740-0.65000.h5\n",
            "Epoch 13/30\n",
            "21/21 [==============================] - 77s 4s/step - loss: 0.3890 - categorical_accuracy: 0.8593 - val_loss: 0.8553 - val_categorical_accuracy: 0.6600\n",
            "\n",
            "Epoch 00013: saving model to model_LSTM_2020-08-1616_56_15.829707/model-00013-0.38991-0.85973-0.85532-0.66000.h5\n",
            "Epoch 14/30\n",
            "21/21 [==============================] - 76s 4s/step - loss: 0.3259 - categorical_accuracy: 0.9116 - val_loss: 0.9332 - val_categorical_accuracy: 0.6500\n",
            "\n",
            "Epoch 00014: saving model to model_LSTM_2020-08-1616_56_15.829707/model-00014-0.32750-0.91101-0.93318-0.65000.h5\n",
            "Epoch 15/30\n",
            "21/21 [==============================] - 76s 4s/step - loss: 0.2752 - categorical_accuracy: 0.9125 - val_loss: 0.8960 - val_categorical_accuracy: 0.6800\n",
            "\n",
            "Epoch 00015: saving model to model_LSTM_2020-08-1616_56_15.829707/model-00015-0.27558-0.91252-0.89595-0.68000.h5\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "Epoch 16/30\n",
            "21/21 [==============================] - 78s 4s/step - loss: 0.2324 - categorical_accuracy: 0.9360 - val_loss: 0.9066 - val_categorical_accuracy: 0.6900\n",
            "\n",
            "Epoch 00016: saving model to model_LSTM_2020-08-1616_56_15.829707/model-00016-0.23342-0.93514-0.90659-0.69000.h5\n",
            "Epoch 17/30\n",
            "21/21 [==============================] - 78s 4s/step - loss: 0.2138 - categorical_accuracy: 0.9503 - val_loss: 0.8736 - val_categorical_accuracy: 0.6800\n",
            "\n",
            "Epoch 00017: saving model to model_LSTM_2020-08-1616_56_15.829707/model-00017-0.21287-0.95023-0.87357-0.68000.h5\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "Epoch 18/30\n",
            "21/21 [==============================] - 77s 4s/step - loss: 0.1864 - categorical_accuracy: 0.9524 - val_loss: 0.8502 - val_categorical_accuracy: 0.6900\n",
            "\n",
            "Epoch 00018: saving model to model_LSTM_2020-08-1616_56_15.829707/model-00018-0.18753-0.95173-0.85023-0.69000.h5\n",
            "Epoch 19/30\n",
            "21/21 [==============================] - 76s 4s/step - loss: 0.1635 - categorical_accuracy: 0.9702 - val_loss: 0.8326 - val_categorical_accuracy: 0.6900\n",
            "\n",
            "Epoch 00019: saving model to model_LSTM_2020-08-1616_56_15.829707/model-00019-0.16390-0.96983-0.83258-0.69000.h5\n",
            "Epoch 20/30\n",
            "21/21 [==============================] - 76s 4s/step - loss: 0.1528 - categorical_accuracy: 0.9747 - val_loss: 0.8300 - val_categorical_accuracy: 0.6800\n",
            "\n",
            "Epoch 00020: saving model to model_LSTM_2020-08-1616_56_15.829707/model-00020-0.15259-0.97436-0.83000-0.68000.h5\n",
            "Epoch 21/30\n",
            "21/21 [==============================] - 76s 4s/step - loss: 0.1525 - categorical_accuracy: 0.9592 - val_loss: 0.8243 - val_categorical_accuracy: 0.6900\n",
            "\n",
            "Epoch 00021: saving model to model_LSTM_2020-08-1616_56_15.829707/model-00021-0.15246-0.95928-0.82427-0.69000.h5\n",
            "Epoch 22/30\n",
            "21/21 [==============================] - 76s 4s/step - loss: 0.1414 - categorical_accuracy: 0.9656 - val_loss: 0.8369 - val_categorical_accuracy: 0.6800\n",
            "\n",
            "Epoch 00022: saving model to model_LSTM_2020-08-1616_56_15.829707/model-00022-0.14007-0.96682-0.83688-0.68000.h5\n",
            "Epoch 23/30\n",
            "21/21 [==============================] - 76s 4s/step - loss: 0.1478 - categorical_accuracy: 0.9712 - val_loss: 0.8033 - val_categorical_accuracy: 0.7200\n",
            "\n",
            "Epoch 00023: saving model to model_LSTM_2020-08-1616_56_15.829707/model-00023-0.14689-0.97134-0.80333-0.72000.h5\n",
            "Epoch 24/30\n",
            "21/21 [==============================] - 76s 4s/step - loss: 0.1379 - categorical_accuracy: 0.9702 - val_loss: 0.7856 - val_categorical_accuracy: 0.7300\n",
            "\n",
            "Epoch 00024: saving model to model_LSTM_2020-08-1616_56_15.829707/model-00024-0.13911-0.96983-0.78564-0.73000.h5\n",
            "Epoch 25/30\n",
            "21/21 [==============================] - 76s 4s/step - loss: 0.1300 - categorical_accuracy: 0.9756 - val_loss: 0.7788 - val_categorical_accuracy: 0.7200\n",
            "\n",
            "Epoch 00025: saving model to model_LSTM_2020-08-1616_56_15.829707/model-00025-0.12961-0.97587-0.77881-0.72000.h5\n",
            "Epoch 26/30\n",
            "21/21 [==============================] - 76s 4s/step - loss: 0.1288 - categorical_accuracy: 0.9741 - val_loss: 0.7614 - val_categorical_accuracy: 0.7300\n",
            "\n",
            "Epoch 00026: saving model to model_LSTM_2020-08-1616_56_15.829707/model-00026-0.12861-0.97436-0.76144-0.73000.h5\n",
            "Epoch 27/30\n",
            "21/21 [==============================] - 75s 4s/step - loss: 0.1180 - categorical_accuracy: 0.9836 - val_loss: 0.7329 - val_categorical_accuracy: 0.7200\n",
            "\n",
            "Epoch 00027: saving model to model_LSTM_2020-08-1616_56_15.829707/model-00027-0.11768-0.98341-0.73289-0.72000.h5\n",
            "Epoch 28/30\n",
            "21/21 [==============================] - 75s 4s/step - loss: 0.1125 - categorical_accuracy: 0.9706 - val_loss: 0.7224 - val_categorical_accuracy: 0.7100\n",
            "\n",
            "Epoch 00028: saving model to model_LSTM_2020-08-1616_56_15.829707/model-00028-0.11214-0.97134-0.72238-0.71000.h5\n",
            "Epoch 29/30\n",
            "21/21 [==============================] - 76s 4s/step - loss: 0.1065 - categorical_accuracy: 0.9771 - val_loss: 0.7343 - val_categorical_accuracy: 0.7200\n",
            "\n",
            "Epoch 00029: saving model to model_LSTM_2020-08-1616_56_15.829707/model-00029-0.10611-0.97738-0.73431-0.72000.h5\n",
            "Epoch 30/30\n",
            "21/21 [==============================] - 75s 4s/step - loss: 0.1046 - categorical_accuracy: 0.9792 - val_loss: 0.7632 - val_categorical_accuracy: 0.7300\n",
            "\n",
            "Epoch 00030: saving model to model_LSTM_2020-08-1616_56_15.829707/model-00030-0.10507-0.97888-0.76323-0.73000.h5\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb6ff1747f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hiFk9BQ_Iob5"
      },
      "source": [
        "**Observation**\n",
        "\n",
        "*   Training Accuracy: 97%\n",
        "*   Validation Accuracy: 73%\n",
        "*   Learning Rate: 0.000125\n",
        "*   Batch Size: 32\n",
        "*   Epoch: 30\n",
        "*   Optimizer: Adam\n",
        "*   Augmentation: None\n",
        "*   Trainable Parameters: 789,509"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1PafpStvI9Hf"
      },
      "source": [
        "**Inference**\n",
        "\n",
        "It is overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Vvsf--pj3jqX"
      },
      "source": [
        "# Transfer Learning with Mobilenet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZTb5fSFQ7BpJ",
        "colab": {},
        "outputId": "4d7cd458-2ec7-4a88-be97-e68d1c414959"
      },
      "source": [
        "#write your model here\n",
        "img_height = 120\n",
        "img_width = 120\n",
        "lstm_layer = 64\n",
        "img_dense = 64\n",
        "dropout = 0.25\n",
        "img_frames = 16  # Number of image frames per video\n",
        "batch_size = 8\n",
        "\n",
        "# Input\n",
        "input_shape=(img_frames,img_height,img_width,img_channel)\n",
        "\n",
        "#imports the mobilenet model and discards the last 1000 neuron layer.\n",
        "base_model = mobilenet.MobileNet(weights='imagenet', include_top=False)\n",
        "model = Sequential()\n",
        "model.add(TimeDistributed(base_model, input_shape=input_shape))\n",
        "\n",
        "for layer in model.layers:\n",
        "\tlayer.trainable = False\n",
        "\n",
        "model.add(TimeDistributed(BatchNormalization()))\n",
        "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "model.add(TimeDistributed(Flatten()))\n",
        "\n",
        "#LSTM layers\n",
        "model.add(LSTM(lstm_layer))\n",
        "model.add(Dropout(dropout))\n",
        "\n",
        "#Fully Connected Layer\n",
        "model.add(Dense(img_dense,activation='relu'))\n",
        "model.add(Dropout(dropout))\n",
        "\n",
        "#softmax layer\n",
        "model.add(Dense(nb_classes, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/keras_applications/mobilenet.py:206: UserWarning: MobileNet shape is undefined. Weights for input shape (224, 224) will be loaded.\n",
            "  warnings.warn('MobileNet shape is undefined.'\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m4FkFC2R7rvz",
        "colab": {},
        "outputId": "d66f557f-ada4-4ea0-e93a-966a95b1fbc8"
      },
      "source": [
        "optimiser = Adam()\n",
        "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "print (model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "time_distributed_17 (TimeDis (None, 16, 3, 3, 1024)    3228864   \n",
            "_________________________________________________________________\n",
            "time_distributed_18 (TimeDis (None, 16, 3, 3, 1024)    4096      \n",
            "_________________________________________________________________\n",
            "time_distributed_19 (TimeDis (None, 16, 1, 1, 1024)    0         \n",
            "_________________________________________________________________\n",
            "time_distributed_20 (TimeDis (None, 16, 1024)          0         \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, 64)                278784    \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 5)                 325       \n",
            "=================================================================\n",
            "Total params: 3,516,229\n",
            "Trainable params: 285,317\n",
            "Non-trainable params: 3,230,912\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E6kDVzC97x3c",
        "colab": {}
      },
      "source": [
        "train_generator = generator(train_path, train_doc, batch_size)\n",
        "val_generator = generator(val_path, val_doc, batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4IJxNgxe7yyG",
        "colab": {}
      },
      "source": [
        "model_name = 'model_TL_Mobilenet' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
        "    \n",
        "if not os.path.exists(model_name):\n",
        "    os.mkdir(model_name)\n",
        "        \n",
        "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
        "\n",
        "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1) # write the REducelronplateau code here\n",
        "callbacks_list = [checkpoint, LR]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sr0G6tSG73YI",
        "colab": {}
      },
      "source": [
        "if (num_train_sequences%batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "if (num_val_sequences%batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences/batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences//batch_size) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DgAdKKwWI71j",
        "colab": {},
        "outputId": "89356646-4c98-4fa9-92eb-6c94f2444ab2"
      },
      "source": [
        "batch_size = 8\n",
        "num_epochs = 20\n",
        "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
        "                    callbacks=callbacks_list, validation_data=val_generator, \n",
        "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source path =  Project_data/val ; batch size = 8Source path =  Project_data/train ; batch size = 8\n",
            "Epoch 1/20\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:15: DeprecationWarning: `imread` is deprecated!\n",
            "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imread`` instead.\n",
            "  from ipykernel import kernelapp as app\n",
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "82/83 [============================>.] - ETA: 0s - loss: 1.5744 - categorical_accuracy: 0.2927"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:37: DeprecationWarning: `imread` is deprecated!\n",
            "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imread`` instead.\n",
            "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:40: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "83/83 [==============================] - 44s 535ms/step - loss: 1.5702 - categorical_accuracy: 0.2926 - val_loss: 1.7082 - val_categorical_accuracy: 0.1900\n",
            "\n",
            "Epoch 00001: saving model to model_TL_Mobilenet_2020-08-1705_57_31.761419/model-00001-1.57069-0.29261-1.70817-0.19000.h5\n",
            "Epoch 2/20\n",
            "83/83 [==============================] - 39s 465ms/step - loss: 1.3235 - categorical_accuracy: 0.4615 - val_loss: 1.6895 - val_categorical_accuracy: 0.2500\n",
            "\n",
            "Epoch 00002: saving model to model_TL_Mobilenet_2020-08-1705_57_31.761419/model-00002-1.32372-0.46154-1.68949-0.25000.h5\n",
            "Epoch 3/20\n",
            "83/83 [==============================] - 42s 503ms/step - loss: 1.1338 - categorical_accuracy: 0.5538 - val_loss: 1.8454 - val_categorical_accuracy: 0.2200\n",
            "\n",
            "Epoch 00003: saving model to model_TL_Mobilenet_2020-08-1705_57_31.761419/model-00003-1.13434-0.55354-1.84543-0.22000.h5\n",
            "Epoch 4/20\n",
            "83/83 [==============================] - 41s 497ms/step - loss: 0.9861 - categorical_accuracy: 0.6091 - val_loss: 1.7921 - val_categorical_accuracy: 0.2500\n",
            "\n",
            "Epoch 00004: saving model to model_TL_Mobilenet_2020-08-1705_57_31.761419/model-00004-0.98548-0.60935-1.79208-0.25000.h5\n",
            "\n",
            "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "Epoch 5/20\n",
            "83/83 [==============================] - 40s 485ms/step - loss: 0.8352 - categorical_accuracy: 0.6984 - val_loss: 1.8972 - val_categorical_accuracy: 0.2500\n",
            "\n",
            "Epoch 00005: saving model to model_TL_Mobilenet_2020-08-1705_57_31.761419/model-00005-0.83551-0.69834-1.89722-0.25000.h5\n",
            "Epoch 6/20\n",
            "83/83 [==============================] - 42s 500ms/step - loss: 0.7678 - categorical_accuracy: 0.6867 - val_loss: 1.8717 - val_categorical_accuracy: 0.2300\n",
            "\n",
            "Epoch 00006: saving model to model_TL_Mobilenet_2020-08-1705_57_31.761419/model-00006-0.76839-0.68627-1.87166-0.23000.h5\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "Epoch 7/20\n",
            "83/83 [==============================] - 41s 494ms/step - loss: 0.6339 - categorical_accuracy: 0.7691 - val_loss: 1.9729 - val_categorical_accuracy: 0.2200\n",
            "\n",
            "Epoch 00007: saving model to model_TL_Mobilenet_2020-08-1705_57_31.761419/model-00007-0.63339-0.76923-1.97294-0.22000.h5\n",
            "Epoch 8/20\n",
            "83/83 [==============================] - 41s 500ms/step - loss: 0.5455 - categorical_accuracy: 0.8085 - val_loss: 1.9481 - val_categorical_accuracy: 0.2300\n",
            "\n",
            "Epoch 00008: saving model to model_TL_Mobilenet_2020-08-1705_57_31.761419/model-00008-0.54544-0.80845-1.94812-0.23000.h5\n",
            "\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "Epoch 9/20\n",
            "83/83 [==============================] - 41s 493ms/step - loss: 0.5155 - categorical_accuracy: 0.8206 - val_loss: 1.9626 - val_categorical_accuracy: 0.2200\n",
            "\n",
            "Epoch 00009: saving model to model_TL_Mobilenet_2020-08-1705_57_31.761419/model-00009-0.51558-0.82051-1.96263-0.22000.h5\n",
            "Epoch 10/20\n",
            "83/83 [==============================] - 41s 495ms/step - loss: 0.4970 - categorical_accuracy: 0.8311 - val_loss: 2.0003 - val_categorical_accuracy: 0.2400\n",
            "\n",
            "Epoch 00010: saving model to model_TL_Mobilenet_2020-08-1705_57_31.761419/model-00010-0.49716-0.83107-2.00025-0.24000.h5\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "Epoch 11/20\n",
            "83/83 [==============================] - 41s 494ms/step - loss: 0.4572 - categorical_accuracy: 0.8475 - val_loss: 2.0076 - val_categorical_accuracy: 0.2300\n",
            "\n",
            "Epoch 00011: saving model to model_TL_Mobilenet_2020-08-1705_57_31.761419/model-00011-0.45733-0.84766-2.00764-0.23000.h5\n",
            "Epoch 12/20\n",
            "83/83 [==============================] - 41s 499ms/step - loss: 0.4585 - categorical_accuracy: 0.8401 - val_loss: 1.9952 - val_categorical_accuracy: 0.2200\n",
            "\n",
            "Epoch 00012: saving model to model_TL_Mobilenet_2020-08-1705_57_31.761419/model-00012-0.45871-0.84012-1.99520-0.22000.h5\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "Epoch 13/20\n",
            "83/83 [==============================] - 42s 504ms/step - loss: 0.4793 - categorical_accuracy: 0.8279 - val_loss: 2.0075 - val_categorical_accuracy: 0.2400\n",
            "\n",
            "Epoch 00013: saving model to model_TL_Mobilenet_2020-08-1705_57_31.761419/model-00013-0.47923-0.82805-2.00746-0.24000.h5\n",
            "Epoch 14/20\n",
            "83/83 [==============================] - 41s 494ms/step - loss: 0.4526 - categorical_accuracy: 0.8509 - val_loss: 2.0253 - val_categorical_accuracy: 0.2300\n",
            "\n",
            "Epoch 00014: saving model to model_TL_Mobilenet_2020-08-1705_57_31.761419/model-00014-0.45303-0.85068-2.02531-0.23000.h5\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "Epoch 15/20\n",
            "83/83 [==============================] - 42s 504ms/step - loss: 0.4553 - categorical_accuracy: 0.8307 - val_loss: 2.0179 - val_categorical_accuracy: 0.2300\n",
            "\n",
            "Epoch 00015: saving model to model_TL_Mobilenet_2020-08-1705_57_31.761419/model-00015-0.45471-0.83107-2.01794-0.23000.h5\n",
            "Epoch 16/20\n",
            "83/83 [==============================] - 40s 488ms/step - loss: 0.4879 - categorical_accuracy: 0.8264 - val_loss: 2.0231 - val_categorical_accuracy: 0.2300\n",
            "\n",
            "Epoch 00016: saving model to model_TL_Mobilenet_2020-08-1705_57_31.761419/model-00016-0.48747-0.82655-2.02309-0.23000.h5\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "Epoch 17/20\n",
            "83/83 [==============================] - 41s 490ms/step - loss: 0.4273 - categorical_accuracy: 0.8627 - val_loss: 2.0259 - val_categorical_accuracy: 0.2400\n",
            "\n",
            "Epoch 00017: saving model to model_TL_Mobilenet_2020-08-1705_57_31.761419/model-00017-0.42722-0.86275-2.02590-0.24000.h5\n",
            "Epoch 18/20\n",
            "83/83 [==============================] - 42s 511ms/step - loss: 0.4433 - categorical_accuracy: 0.8623 - val_loss: 2.0228 - val_categorical_accuracy: 0.2400\n",
            "\n",
            "Epoch 00018: saving model to model_TL_Mobilenet_2020-08-1705_57_31.761419/model-00018-0.44265-0.86275-2.02280-0.24000.h5\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "Epoch 19/20\n",
            "83/83 [==============================] - 43s 514ms/step - loss: 0.4684 - categorical_accuracy: 0.8328 - val_loss: 2.0231 - val_categorical_accuracy: 0.2300\n",
            "\n",
            "Epoch 00019: saving model to model_TL_Mobilenet_2020-08-1705_57_31.761419/model-00019-0.46862-0.83258-2.02313-0.23000.h5\n",
            "Epoch 20/20\n",
            "83/83 [==============================] - 41s 496ms/step - loss: 0.4208 - categorical_accuracy: 0.8473 - val_loss: 2.0249 - val_categorical_accuracy: 0.2400\n",
            "\n",
            "Epoch 00020: saving model to model_TL_Mobilenet_2020-08-1705_57_31.761419/model-00020-0.42035-0.84766-2.02495-0.24000.h5\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fea0f510390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vLjoNh4h7lMK"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uyWcisd0L3cW"
      },
      "source": [
        ""
      ]
    }
  ]
}